<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 8.2 推理系统的低延迟优化

- [8.2 推理系统的低延迟优化](#82-推理系统的低延迟优化)
  - [8.2.1 推理(inference)延迟(latency)](#821-推理inference延迟latency)
  - [8.2.2 层(layer)间与张量(tensor)融合](#822-层layer间与张量tensor融合)
  - [8.2.3 目标后端自动调优](#823-目标后端自动调优)
  - [8.2.4 内存分配策略调优](#824-内存分配策略调优)
  - [8.2.5 低精度推理与精度校准](#825-低精度推理与精度校准)
  - [8.2.6 模型压缩](#826-模型压缩)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

推理系统类似传统的Web服务，需要响应用户请求，并保证一定的服务等级协议(例如，响应时间低于100ms)，进而需要通过一定的策略和优化，保持一定的低延迟(low latency)，本小节将围绕部署过程中涉及到的延迟问题和相应的优化进行展开。

## 8.2.1 推理(inference)延迟(latency)

延迟(latency)是在客户端给出查询后，推理系统呈现给客户端推理结果所花费的时间。推理服务通常位于关键路径上，因此预测必须既快速同时满足有限的[尾部延迟(tail latency)](https://www.cs.utexas.edu/users/mckinley/talks/ds-strangeloop-2017.pdf)才能满足服务水平协议(service level agreement)。

例如：某互联网公司的在线服务要求Service Level Agreement(SLA): 次秒(Sub-second)级别延迟服务水平协议。

为了达到低延迟，推理系统面临以下的挑战：
- 交互式应用程序的低延迟需求通常与离线批处理训练框架设计的目标(例如，大规模，高吞吐，大批次等)不一致
- 简单的模型速度快，复杂的模型更加准确，但是浮点运算量更大
- 次秒(Sub-second)级别延迟约束制了批尺寸(Batch Size)
- 模型融合或多租容易引起长尾延迟(Long Tail Traffic)现象

推理系统常常可以通过以下几个方向进行模型推理的延迟优化：

- 模型优化，降低访存开销：
  - 层间(layer)融合(fusion)或张量(tensor)融合
  - 目标后端自动调优
  - 内存分配策略调优
- 降低一定的准确度，进而降低计算量，最终降低延迟：
  - 低精度推理与精度校准(Precision Calibration)
  - 模型压缩(Model Compression)
- 自适应批尺寸(Batch Size)：动态调整需要进行推理的输入数据数量
- 缓存(Caching)结果：复用已推理的结果或特征

那么接下来的内容，我们将围绕代表性的优化策略进行展开。

## 8.2.2 层(layer)间与张量(tensor)融合

我们将模型抽象为计算图(computation graph)，其中在设备执行的层(layer)在一些框架内也称作内核(kernel)或算子(operator)。我们可以从下图中看到，设备中执行一个内核，一般需要以下几个步骤和时间开销，启动内核，读取数据到设备，执行内核，结果数据写回主存。

<center> <img src="./img/2/8-2-1-fusion.png" width="400" height="140" /></center>
<center>图8-2-1. 内核执行时间线</center>

我们从上面的实例可以观察到需要对模型的层间和张量融合的原因：
- 相对于内核启动开销和每个层的张量数据读写成本，内核(Kernel)计算通常非常快
- 导致内存带宽瓶颈和可用设备(device)资源的利用不足

我们可以将融合问题抽象为一个优化问题：
- 目标：最小化设备(device)访存和最大化设备利用率
- 策略：搜索计算图的最优融合策略

如下图所示，左图为未进行融合的深度学习模型网络结构，右图为经过TensorRT优化和融合之后的深度学习模型网络结构。优化后将很多小的层融合为大的层，这样减少了大量的数据跨设备读写的开销，提升性能。

<center> <img src="./img/2/8-2-2-fusionopt.png" width="500" height="200" /></center>
<center>图8-2-2. TensorRT使用内核融合进行模型执行优化</center>

代表性的开源内核融合系统还有很多，例如，微软开源的[NNFusion](https://github.com/microsoft/nnfusion)。

## 8.2.3 目标后端自动调优

由于深度学习模型中的层的计算逻辑可以转为矩阵运算，而矩阵计算又可以通过循环进行实现。对于循环的实现，由于在不同的推理CPU和硬件设备中，有不同的缓存和内存大小以及访存带宽，进而如何针对不同的设备进行循环的并行化和考虑数据的局部性降低访存开销，可以抽象为一个搜索空间巨大的优化问题。目前有很多代表性的工作在围绕这个问题展开，通过基于专家经验的规则，构建代价模型，抽象为优化问题或者通过机器学习自动化学习和预测是常用的方式。由于在编译优化章节对此有较为详细的介绍，本章不再展开相应的内容，读者可以参考编译优化章节内容理解。

代表性的开源后端自动调优编译器或工具有：[TVM](https://tvm.apache.org/),[Hadlide](https://dl.acm.org/doi/10.1145/2491956.2462176),[Ansor](https://tvm.apache.org/2021/03/03/intro-auto-scheduler)等。

## 8.2.4 内存分配策略调优

由于设备或服务端内存是紧缺资源，推理系统常常也需要考虑做内存的分配策略的优化，进而能够服务更大的模型。

- 目标：最小化内存占用和内存分配调用开销
- 约束：保证延迟SLA
- 优化策略：
  - 缓存分配器(cached allocator)：推理系统预先申请设备内存，构建推理系统的内存管理器，减少设备内存分配释放等API调用代价(例如，[cudaFree调用可能阻塞它的调用者，直到所有GPU上所有先前排队的工作完成](https://arxiv.org/abs/1912.01703))。
  - [预取(pre-fetching)和卸载(off-loading)](https://arxiv.org/abs/1602.08124)：异步地将设备内存数据在读取前和产生后和主存进行换入换出，减少设备内存的数据压力。
  - [算子融合(fusion)](https://github.com/microsoft/nnfusion)：将中间结果在缓存层给下一阶段的内核使用，减少中间结果回写吞吐和延迟更低的设备内存或者主存的开销。

读者也可以参考相关工作（例如，[DNNMem](https://dl.acm.org/doi/abs/10.1145/3368089.3417050)，[vDNN](https://dl.acm.org/doi/10.5555/3195638.3195660)等）进一步了解深度学习模型内存的分配占用分类和优化策略。

## 8.2.5 低精度推理与精度校准

推理阶段相比训练阶段可以适当降低精度，进而降低推理延迟:
- 大多数深度学习框架都以完整的32位精度（FP32）训练神经网络。
- 使用较低的精度会导致较小的模型大小，较低的内存利用率和延迟以及较高的吞吐量。
- 对模型进行充分训练后，由于不需要进行反向传播求梯度，因此推理计算相比训练常常使用的FP32，推理可以使用半精度FP16甚至INT8张量运算降低计算量和内存占用。

如图所示，不同的精度对应的比特数和取值范围不同，进而产生了不同学习性能和系统性能的权衡。
<center> <img src="./img/2/8-2-3-lowprecision.png" width="400" height="100" /></center>
<center>图8-2-3. 精度取值范围</center>

<!-- 我们可以将低精度推理问题抽象为以下优化问题，进而通过一定策略进行优化：
- 目标：最小化数据精度
- 约束：准确度损失   -->

<!-- - INT8编码FP32同样信息:
  - Tensor Values = FP32 scale factor * int8 array
  - 添加饱和阈值准确度损失更低 
- 目标：最小化信息损失，信息损失通过KL(Kullback-Leibler divergence)散度度量
- 策略步骤：
  - 校准数据集运行FP32推理
  - 对每层数据收集激活输出直方图
  - 生成使用不同饱和阈值产生的量化输出分布
  - 选择最小化激活输出分布与量化后的激活输出分布之间KL散度阈值

<center> <img src="./img/2/8-2-4-precision-calibration.png" width="300" height="400" /></center>
<center>图8-2-4. 精度校准</center> -->

## 8.2.6 模型压缩

模型压缩(model compression)是通过一定的算法和策略，在保证模型预测效果满足一定要求的前提下，尽可能地降低模型权重的大小，进而降低模型的推理计算量，内存开销和模型文件的空间占用，最终降低模型推理延迟。因为其可观的收益和一定的预测效果保证，在模型部署和推理之前，通过模型压缩进行模型精简是常常使用的技术。

我们可以将模型压缩抽象为有约束的优化问题：

优化目标： 在所选择的策略下，最小化模型大小
$$min_{policy_i}\{Model\_Size(Policy_i)\}$$

约束：在所选择的策略进行模型压缩后，保证预测精度满足SLA

$$Accuracy(Policy_i) \geq Accuracy\_SLA$$

常常使用的模型压缩技术有如下几种：

- 参数裁剪和共享（Parameter Pruning and Sharing）
  - 剪枝(Pruning)
  - 量化(Quantization)
  - 编码(Encoding)
- 低秩分解（Low-rank Factorization）
- 知识精炼（Knowledge Distillation）
- …

在后面章节中我们将有更为细节的模型压缩内容叙述，本小节只介绍与推理相关的模型压缩技术和问题。

## 小结与讨论

本小节主要围绕推理系统的延迟展开讨论，我们总结了低延迟的推理需求，以及围绕这个设计目标，推理系统常常使用的优化策略。优化延迟的目标，受到空间与准确度的约束，深度学习的推理过程相比训练过程在适当情况下可以牺牲一定准确度进而提升延迟。

看完本章内容后，我们可以思考以下几点问题：
层间与张量融合受到哪些约束?
推理和训练优化内存分配策略的侧重点是否有不同？

## 参考文献 

- [Park, Jongsoo et al. “Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications.” ArXiv abs/1811.09886 (2018): n. pag.](https://arxiv.org/abs/1811.09886)
- [Crankshaw, Daniel et al. “Clipper: A Low-Latency Online Prediction Serving System.” NSDI (2017).](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17). Association for Computing Machinery, New York, NY, USA, 1387–1395. DOI:https://doi.org/10.1145/3097983.3098021](https://research.google/pubs/pub46484/)
- [Olston, Christopher et al. “TensorFlow-Serving: Flexible, High-Performance ML Serving.” ArXiv abs/1712.06139 (2017): n. pag.](https://arxiv.org/abs/1712.06139)
- [Jeong-Min Yun, Yuxiong He, Sameh Elnikety, and Shaolei Ren. 2015. Optimal Aggregation Policy for Reducing Tail Latency of Web Search. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). Association for Computing Machinery, New York, NY, USA, 63–72. DOI:https://doi.org/10.1145/2766462.2767708](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
- [Cheng, Yu et al. “A Survey of Model Compression and Acceleration for Deep Neural Networks.” ArXiv abs/1710.09282 (2017): n. pag.](https://arxiv.org/abs/1710.09282)
- [CSE 599W: System for ML - Model Serving](https://dlsys.cs.washington.edu/)
- https://developer.nvidia.com/deep-learning-performance-training-inference 
- [Han, Song et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition (2016): n. pag.](https://arxiv.org/abs/1510.00149) 
- [Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both weights and connections for efficient neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 (NIPS'15). MIT Press, Cambridge, MA, USA, 1135–1143.](https://arxiv.org/abs/1506.02626)
- [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)
- [Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. SIGPLAN Not. 48, 6 (June 2013), 519–530. DOI:https://doi.org/10.1145/2499370.2462176](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
- [Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: an automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 579–594.](https://arxiv.org/abs/1802.04799)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)