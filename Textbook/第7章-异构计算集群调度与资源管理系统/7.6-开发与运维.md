<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 7.6 开发与运维

在之前的章节，我们已经介绍面向深度学习的集群管理系统的运行时与调度。本章将围绕工程实践中，关于集群管理系统开发，运维的话题进行展开。集群管理系统，也称作平台，是7x24小时的对公司内部使用或者对外提供服务的平台型服务，其本身的开发越来越敏捷，同时也需要有更加敏捷而高效的运维支持。本章将围绕开发与运维的整体内容展开。


- [7.6 开发与运维](#76-开发与运维)
  - [7.6.1 平台功能模块与敏捷开发](#761-平台功能模块与敏捷开发)
  - [7.6.2 监控体系构建](#762-监控体系构建)
      - [***全局监控***](#全局监控)
      - [***性能监控***](#性能监控)
      - [***服务与硬件稳定性监控***](#服务与硬件稳定性监控)
      - [***报警***](#报警)
  - [7.6.3 测试](#763-测试)
      - [***单元测试***](#单元测试)
      - [***集成测试***](#集成测试)
      - [***压力测试***](#压力测试)
      - [***回归测试***](#回归测试)
      - [***模糊测试(Fuzzing)与混沌工程***](#模糊测试fuzzing与混沌工程)
  - [7.6.4 平台部署与DevOps](#764-平台部署与devops)
  - [7.6.5 平台运维](#765-平台运维)
      - [***DRI机制***](#dri机制)
      - [***事件管理***](#事件管理)
      - [***智能运维***](#智能运维)
  - [7.6.6 部署异构资源集群管理系统实验](#766-部署异构资源集群管理系统实验)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)
  
## 7.6.1 平台功能模块与敏捷开发

平台本身由于功能众多，整体性质属于在线服务，业界一般通过敏捷开发的模式进行整体的开发与设计。一般平台中涉及以下重要服务和功能模块的开发与设计。

- 内核：调度器，运行时，管理GUI，权限管理，文件管理等。
  - 调度器设计可以兼顾深度学习特点例如资源局部性。
  - 运行时需要支持特定设备插件，能够将GPU，IB等设备注册进入运行时。
  - 由于深度学习作业主要输入输出为数据和模型，一般可使用文件系统存储即可，需要选用高效（NFS等）且容错（云存储）的文件系统提供支持。
- 监控与报警：性能监控，异常监控，报警系统等。
  - 除常见需要平台监控的性能指标，还要特别关注GPU等核心加速器的资源利用率等指标。
  - 报警系统也要除覆盖常见错误，还需要覆盖新服务与新硬件产生的错误与异常。
- 工具链：IDE，作业提交与调试工具，API(例如：Restful)等。
  - IDE可以选用对Python和深度学习生态支持较好的开发环境。例如，基于浏览器的开发工具(Jupyter)或者插件化支持较好的本地客户端环境VS Code等。
  - 作业提交可以通过Web人工提交或者Restful API方便AutoML工具自动化提交。
  - 可以考虑支持远程SSH方便用户远程登录作业现场调试
- 应用市场：模型市场，镜像市场等。
  - 由于目前深度学习新的模型很多基于研究工作和开源模型与代码进行微调适配和扩展，模型训练框架主流框架也都是开源的，这就让平台方可以通过提供常用和优化版本的模型与框架镜像市场进而优化，管理和提升开发生产力。

不同的功能模块可以分配不同的工程师进行开发，通过[Scrum](https://en.wikipedia.org/wiki/Scrum_(software_development))敏捷开发机制，指定Sprint计划，并通过每日的Scrum进行规划反思与推进。

## 7.6.2 监控体系构建

通过构建完整的指标体系，能够让平台管理员和开发者对平台健康状况和系统负载运行状况一目了然，同时有助于对异常与错误进行诊断与归因分析。围绕异构资源管理系统，可以从以下方面设计指标， 进而通过数据驱动的方法提升运维效率。

#### ***全局监控***

可以通过围绕作业，服务，节点几个维度设计需要整体宏观监控的指标，并围绕团队的核心KPI设计效率，稳定性等全局指标，方便回查与评估。
<style>table{margin: auto;}</style>
<center> <img src="./img/6/7-6-1-monitorpage.jpg" ch="500" width="800" height="600" /></center>
<center>图7-6-1. 平台全局监控(<a href="https://github.com/microsoft/pai">图片来源</a>)</center>

如图所示，平台可以通过全局监控观测硬件资源的利用率，健康状况，也可以拓展到作业，用户等其他指标。通过宏观监控让管理员和运维工程师能观测重要问题，对整体有宏观认识。

#### ***性能监控***

可以围绕作业和硬件的性能进行监控设计，其中硬件的各种指标在作业执行时间的连接可以推导出作业的性能指标。对深度学习系统，尤其需要关注GPU的利用率等指标，这可以通过[NVML](https://developer.nvidia.com/nvidia-management-library-nvml)，[DCGM](https://developer.nvidia.com/dcgm)等工具构建指标导出器进行监控与获取。

例如，我们可以通过nvidia-smi命令获取GPU的利用率，显存使用状况，温度和功耗，以及是否有ECC error等信息，进而将这些信息暴露给监控系统收集，进而形成对系统性能和健康的持续监控。

```shell
$ nvidia-smi
...
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla K80           On   | 00004F2F:00:00.0 Off |                    0 |
| N/A   51C    P0    56W / 149W |   2513MiB / 11441MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           On   | 00007C52:00:00.0 Off |                    0 |
| N/A   31C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

```
#### ***服务与硬件稳定性监控***

可以观测和监控平台的各个模块的健康，通过被动汇报或者主动探测的机制，监测服务的存活状况。对硬件可以周期性运行测试，或者通过监控指标的异常进行判断。服务模块在设计之初就需要考虑暴露接口或者心跳机制设计，可以被监控系统更好的遥测。

很多平台和服务提供了服务存活检测的机制，让服务开发者在服务设计之初就考虑好系统健康状况的可观测性。

例如，以Kubernetes为例，许多长时间运行的应用程序最终会转变为损坏状态，除非重新启动，否则无法恢复。 Kubernetes 提供了活性探针来检测和补救这种情况。

在以下Kubernetes官方提供的实例中，创建一个运行基于 k8s.gcr.io/busybox 映像的容器的Pod。在容器生命的前 30 秒，有一个 /tmp/healthy 文件。因此，在前 30 秒内，命令 cat /tmp/healthy 返回一个成功代码。30 秒后， cat /tmp/healthy由于执行失败，返回失败代码。此实例可以推广开来，用户可以替换要执行的反映状态的命令，也可以替换不同的监测状态的命令。

这是 Pod 的配置文件([文件来源](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/))：

```yaml

apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/busybox
    # 服务启动时执行以下命令
    args: 
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
```

#### ***报警***

监控系统可以在节点部署监控脚本(exporter)以及核心的时序数据库（例如，[Promethus](https://prometheus.io/), [Ganglia](http://ganglia.sourceforge.net/)等）进行监控数据收集，并通过可视化的系统(例如，[Grafana](https://grafana.com/)等)进行监控报表和可视化的展示。
报警系统（例如，[Alert Manager](https://prometheus.io/docs/alerting/latest/alertmanager/)）可以提前规划好报警规则与阈值，不断监控和分析监控系统中的指标是否违规，如果违规及时发送报警信息或者邮件。尤其对用户提交的作业，深度学习作业容易发生OOM，GPU容易出现阻塞或者ECC错误等问题，对于高频和影响较大的异常需要及时修复与处理。

例如，我们可以设计以下的一些报警规则，并通过系统触发报警让运维人员介入：

1. 负载和性能问题规则：例如，例如磁盘使用率超过一定阈值，网络性能低于一定阈值等
2. 服务健康规则：例如，服务失败报警等
3. 硬件健康规则：例如，节点出现健康问题的报警等

<style>table{margin: auto;}</style>
<center> <img src="./img/6/7-6-2-monitor.png" ch="500" width="900" height="600" /></center>
<center>图7-6-2. 监控系统(<a href="https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-monitoring.html">图片来源</a>)</center>

如图所示，一个完整的监控系统如图所示，节点部署监控脚本(例如node-exporter)收集各种性能指标和健康指标，监控指标被存储到时序数据库（例如Promethus）中，报警管理系统周期性查询时序数据库判断是否触发报警，如果触发可以通知接受者（例如通过邮件），同时可视化报表系统也可以查询数据库绘制可视化仪表盘方便管理员监测当前情况。整体的各个系统组件可以通过集群管理系统（例如Kuberenetes）进行部署和运维。

## 7.6.3 测试

可以通过 以下几个方面进行系统测试保证平台的常见问题，早发现，早规避，主动出击保证平台软件代码和服务本身的可靠性。

#### ***单元测试***

平台的新增功能模块需要进行单元测试，保证逻辑的正确性。可以选用所使用的模块的开发语言的单元测试库进行单元测试用例的开发。保证函数和模块的正确性。并在回归测试中触发单元测试进行回测。

#### ***集成测试***

由于开源系统的应用越来越广泛，目前很多平台系统或多或少的模块会采用开源系统进行实现和部署，这就造成平台整体的集成测试变得尤为重要。可以在测试集群或者环境中进行集成测试。

#### ***压力测试***

对于平台，可以不间断的自动提交一些测试作业进行巡检，也可以定期进行压力测试检测服务功能的负载能力或硬件的稳定性。

#### ***回归测试***

一旦完成平台新功能开发或者在线热修复，需要进行回归测试保证符合之前系统假设。
回归测试是指修改了已有代码后，重新进行测试以确认修改没有引入新的错误或导致其他代码产生错误。由于当前平台属于在线服务，同时常常使用敏捷开发模式，这样十分容易产生软件缺陷，设计好回归测试的机制，对保证平台的正确性显得尤为重要。

#### ***模糊测试(Fuzzing)与混沌工程*** 

同时也可以考虑使用混沌测试这种缺陷注入机制，主动注入异常，提前修复和增强平台的稳定性。


<style>table{margin: auto;}</style>
<center> <img src="./img/6/7-6-1-chaos.png" ch="500" width="1000" height="500" /></center>
<center>图7-6-3. 混沌工程通过模糊测试(Fuzzing)为系统主动注入故障(Failure)，增强系统稳定性。(<a href="">图片来源</a>)</center>


## 7.6.4 平台部署与DevOps

可以通过CI/CD机制，持续集成，持续部署，也就是我们通常所说的[DevOps](https://en.wikipedia.org/wiki/DevOps)。平台本身的各个服务组件也可以打包为Docker镜像，通过Kubernetes等工具部署和管理相应服务。可以通过相关的工具（例如：jenkins），构建CI/CD的流水线，进而保证平台工程时开发新功能或者热修复(hotfix)后能够自动化走完完整的测试与部署流程，大幅提升开发与部署效率。


## 7.6.5 平台运维

#### ***DRI机制***

平台可以通过设计[DRI](https://about.gitlab.com/handbook/people-group/directly-responsible-individuals/)机制，让开发工程师轮岗进行值班与平台问题修复运维，这样不会产生开发与运维职责脱节，责任到位，热修复也会执行的更快。

#### ***事件管理***

对平台产生的系统报警和用户提交的问题事件(Incident)，可以通过一定的事件管理进行维护，并制定一定的SLA，保证指定时间内能够介入，缓解与修复，对高优先级的问题需要有相应策略引入更多人员与团队介入。

<style>table{margin: auto;}</style>
<center> <img src="./img/6/7-6-3-operation.png" ch="500" width="800" height="300" /></center>
<center>图7-6-4. 平台事件(Incident)管理(<a href="">图片来源</a>)</center>

如上图所示，平台会通过指定事件(Incident)监测，响应，处理与事后分析的机制，进行事件管理，维护平台的稳定性，应对突发事件，同事通过事件本身不断改进系统这样让平台不断演进。

#### ***智能运维***

目前对于过于复杂问题，非结构化运维数据，以及希望数据驱动解决的运维问题，也可以考虑AIOps技术进行智能运维，主动介入与预测，互补于常规运维方法。

## 7.6.6 部署异构资源集群管理系统实验

请大家参考[AI-System Lab6](https://github.com/microsoft/AI-System/tree/main/Labs/AdvancedLabs/Lab6)进行集群管理系统OpenPAI的部署练习。

大家通过当前实例可以练习和感受以下任务：
- 部署平台
- 监控平台
- 提交作业与监控作业
- 调整调度器配置


## 小结与讨论

本章我们主要介绍异构计算集群管理系统的中的开发与运维，当前软件工程逐渐由客户端开发到服务开发，平台本身即服务，我们会面临开发以外，部署，运维，诊断等更为棘手的问题，希望读者在读完本章后意识到基础架构的挑战与复杂性。

## 参考文献
- https://en.wikipedia.org/wiki/Scrum_(software_development)
- https://about.gitlab.com/handbook/people-group/directly-responsible-individuals/
- https://en.wikipedia.org/wiki/DevOps
- https://prometheus.io/docs/alerting/latest/alertmanager/
- https://grafana.com/
- https://prometheus.io/
- http://ganglia.sourceforge.net/
- [Microsoft Online Services incident management](https://www.youtube.com/watch?v=jpTRdV5eZYE)