
## 2.7 循环神经网络

### 19.0.1 前馈神经网络的不足

通过学习前面的章节，读者可以发现所有的神经网络的输入都是一个或者一批静态的数据，比如一个人的身高、体重、年龄、性别等组成的特征值用于表示一个人当前的属性，这些属性是采样时获得的，并且会保持相对稳定，可以用这些属性通过前馈神经网络来预测一个人的健康状况。再次输入的下一个数据会是另外一个人的特征值，与前一个人丝毫不相关。

或者输入的是一张青蛙的图片，通过卷积神经网络来判断图片中的物体的类别。而下一张图片可能会是另外一只青蛙的图片，或者干脆变成了一张汽车的图片。

而在自然界中，还有很多随着时间而变化的数据需要处理，比如，对一个人来说，在不同的年龄会有不同的身高、体重、健康状况，只有性别是固定的。如果需要根据年龄来预测此人的健康状况，则需要每年对此人的情况进行一次采样，按时间排序后记录到数据库中。

另外一个例子是如果想从一只青蛙的跳跃动作中分析出其跳跃的高度和距离，则需要获得一段视频，然后从视频的每一帧图片中获得青蛙的当前位置和动作。

从上面两个例子中可以看到，对于与时间相关的数据，到目前为止并没有一个很好的解决方案，这就是循环神经网络存在的意义。

### 19.0.2 循环神经网络的发展简史

循环神经网络（RNN，Recurrent Neural Network）的历史可以简单概括如下：

- 1933年，西班牙神经生物学家Rafael Lorente de Nó发现大脑皮层（cerebral cortex）的解剖结构允许刺激在神经回路中循环传递，并由此提出反响回路假设（reverberating circuit hypothesis）。
- 1982年，美国学者John Hopfield使用二元节点建立了具有结合存储（content-addressable memory）能力的神经网络，即Hopfield神经网络。
- 1986年，Michael I. Jordan基于Hopfield网络的结合存储概念，在分布式并行处理（parallel distributed processing）理论下建立了新的循环神经网络，即Jordan网络。
- 1990年，Jeffrey Elman提出了第一个全连接的循环神经网络，Elman网络。Jordan网络和Elman网络是最早出现的面向序列数据的循环神经网络，由于二者都从单层前馈神经网络出发构建递归连接，因此也被称为简单循环网络（Simple Recurrent Network, SRN）。
- 1990年，Paul Werbos提出了循环神经网络的随时间反向传播（BP Through Time，BPTT），BPTT被沿用至今，是循环神经网络进行学习的主要方法。
- 1991年，Sepp Hochreiter发现了循环神经网络的长期依赖问题（long-term dependencies problem），大量优化理论得到引入并衍生出许多改进算法，包括神经历史压缩器（Neural History Compressor, NHC）、长短期记忆网络（Long Short-Term Memory networks, LSTM）、门控循环单元网络（Gated Recurrent Unit networks, GRU）、回声状态网络（echo state network）、独立循环神经网络（Independent RNN）等。

图19-1简单描述了从前馈神经网络到循环神经网络的演化过程。

<img src="./img/19/Forward2Rnn.png">

图19-1 从前馈神经网络到循环神经网络的演化

1. 最左侧的是前馈神经网络的概括图，即，根据一个静态的输入数据x，经过隐层h的计算，最终得到结果a。这里的h是全连接神经网络或者卷积神经网络，a是回归或分类的结果。
2. 当遇到序列数据的问题后（假设时间步数为3），可以建立三个前馈神经网络来分别处理t=1、t=2、t=3的数据，即x1、x2、x3
3. 但是两个时间步之间是有联系的，于是在隐层h1、h2、h3之间建立了一条连接线，实际上是一个矩阵W
4. 根据序列数据的特性，可以扩充时间步的数量，在每个相邻的时间步之间都会有联系

如果仅此而已的话，还不能称之为循环神经网络，只能说是多个前馈神经网络的堆叠而已。在循环神经网络中，以图19-1最右侧的图为例，只有三个参数：

- U：是x到隐层h的权重矩阵
- V：是隐层h到输出a的权重矩阵
- W：是相邻隐层h之间的权重矩阵

请注意这三个参数在不同的时间步是共享的，以图19-1最右侧的图为例，三个U其实是同一个矩阵，三个V是同一个矩阵，两个W是同一个矩阵。这样的话，无论有多少个时间步，都可以像折扇一样“折叠”起来，用一个“循环”来计算各个时间步的输出，这才是“循环神经网络”的真正含义。

### 19.0.3 循环神经网络的结构和典型用途

#### 一对多的结构

在国外，用户可以指定一个风格，或者一段旋律，让机器自动生成一段具有巴赫风格的乐曲。在中国，有藏头诗的娱乐形式，比如以“春”字开头的一句五言绝句可以是“春眠不觉晓”、“春草细还生”等等。这两个例子都是只给出一个输入，生成多个输出的情况，如图19-2所示。

<img src="./img/19/One2Many.png">

图19-2 一对多的结构示意图

这种情况的特殊性在于，第一个时间步生成的结果要做为第二个时间步的输入，使得前后有连贯性。图中只画出了4个时间步，在实际的应用中，如果是五言绝句，则有5个时间步；如果是音乐，则要指定小节数，比如40个小节，则时间步为40。

#### 多对一的结构

在阅读一段影评后，会判断出该观众对所评价的电影的基本印象如何，比如是积极的评价还是消极的评价，反映在数值上就是给5颗星还是只给1颗星。在这个例子中，输入是一段话，可以拆成很多句或者很多词组，输出则是一个分类结果。这是一个多个输入单个输出的形式，如图19-3所示。

<img src="./img/19/Many2One.png">

图19-3 多对一的示意图

图中x可以看作很多连续的词组，依次输入到网络中，只在最后一个时间步才有一个统一的输出。另外一种典型的应用就是视频动作识别，输入连续的视频帧（图片形式），输出是分类结果，比如“跑步”、“骑车”等等动作。

还有一个很吸引人的应用就是股票价格的预测，输入是前10天的股票基本数据，如每天的开盘价、收盘价、交易量等，而输出是明天的股票的收盘价，这也是典型的多对一的应用。但是由于很多其它因素的干扰，股票价格预测具有很大的不确定性。

#### 多对多（输入输出等量）

这种结构要求输入的数据时间步的数量和输出的数据的时间步的数量相同，如图19-4所示。

<img src="./img/19/Many2Many.png">

图19-4 多对多结构

比如想分析视频中每一帧的分类，则输入100帧输入，输出是对应的100个分类结果。另外一个典型应用就是基于字符的语言模型，比如对于英文单词“hello”来说，当第一个字母是h时，计算第二个字母是e的概率，以此类推，则输入是“hell”四个字母，输出是“ello”四个字母的概率。

在中文中，对联的生成问题也是使用了这种结构，如果上联是“风吹水面层层浪”七个字，则下联也一定是七个字，如“雨打沙滩点点坑”。

#### 多对多（输入输出不等量）

这是循环神经网络最重要的一个变种，又叫做编码解码（Encoder-Decoder）模型，或者序列到序列（seqence to seqence）模型，如图19-5所示。

<img src="./img/19/Seq2Seq.png">

图19-5 编码解码模型

以机器翻译任务举例，源语言和目标语言的句子通常不会是相同的长度，为此，此种结构会先把输入数据编码成一个上下文向量，在h2后生成，做为h3的输入。此时，h1和h2可以看做是一个编码网络，h3和h4看做是一个解码网络。解码网络拿到编码网络的输出后，进行解码，得到目标语言的句子。

由于这种结构不限制输入和输出的序列长度，所以应用范围广泛，类似的应用还有：

- 文本摘要：输入是一段文本，输出是摘要，摘要的字数要比正文少很多。
- 阅读理解：输入是文章和问题，输出是问题答案，答案一般都很简短。
- 语音识别，输入是语音信号序列，输出是文字序列，输入的语音信号按时间计算长度，而输出按字数计算长度，根本不是一个量纲。




<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 19.6 深度循环神经网络

### 19.6.1 深度循环神经网络的结构图

前面的几个例子中，单独看每一时刻的网络结构，其实都是由“输入层->隐层->输出层”所组成的，这与我们在前馈神经网络中学到的单隐层的知识一样，由于输入层不算做网络的一层，输出层是必须具备的，所以网络只有一个隐层。我们知道单隐层的能力是有限的，所以人们会使用更深（更多隐层）的网络来解决复杂的问题。

在循环神经网络中，会有同样的需求，要求每一时刻的网络是由多个隐层组成。比如图19-20为两个隐层的循环神经网络，用于解决和19.4节中的同样的问题。

<img src="./img/19/deep_rnn_net.png"/>

图19-20 两个隐层的循环神经网络

注意图19-20中最左侧的两个隐藏状态s1和s2是同时展开为右侧的图的，
这样的循环神经网络称为深度循环神经网络，它可以具备比单隐层的循环神经网络更强大的能力。

### 19.6.2 前向计算

#### 公式推导

对于第一个时间步：
$$
h1 = x \cdot U \tag{1}
$$
$$
h2 = s1 \cdot Q \tag{2}
$$

对于后面的时间步：
$$
h1 = x \cdot U + s1_{t-1} \cdot W1 \tag{3}
$$

$$
h2 = s1 \cdot Q + s2_{t-1} \cdot W2 \tag{4}
$$

对于所有的时间步：

$$
s1 = \tanh(h1) \tag{5}
$$

$$
s2 = \tanh(h2) \tag{6}
$$

对于最后一个时间步：
$$
z = s2 \cdot V \tag{7}
$$
$$
a = Identity(z) \tag{8}
$$
$$
Loss = loss_{\tau} = \frac{1}{2} (a-y)^2 \tag{9}
$$

由于是拟合任务，所以公式8的Identity()函数只是简单地令a=z，以便统一编程接口，最后用均方差做为损失函数。

注意并不是所有的循环神经网络都只在最后一个时间步有监督学习信号，而只是我们这个问题需要这样。在19.2节中的例子就是需要在每一个时间步都要有输出并计算损失函数值的。所以，公式9中只计算了最后一个时间步的损失函数值，做为整体的损失函数值。

#### 代码实现

注意前向计算时需要把prev_s1和prev_s2传入，即上一个时间步的两个隐层的节点值（矩阵）。

```Python
class timestep(object):
    def forward(self, x, U, V, Q, W1, W2, prev_s1, prev_s2, isFirst, isLast):
        ...
```

### 19.6.3 反向传播

#### 公式推导

反向传播部分和前面章节的内容大致相似，我们只把几个关键步骤直接列出来，不做具体推导：

对于最后一个时间步：
$$
\frac{\partial Loss}{\partial z} = a-y \rightarrow dz \tag{10}
$$

$$
\frac{\partial Loss}{\partial V}=\frac{\partial Loss}{\partial z}\frac{\partial z}{\partial V}=s2^{\top} \cdot dz \rightarrow dV \tag{11}
$$

$$
\begin{aligned}
\frac{\partial Loss}{\partial h2} &= \frac{\partial Loss}{\partial z}\frac{\partial z}{\partial s2}\frac{\partial s2}{\partial h2}
\\\\
&=(dz \cdot V^{\top}) \odot \sigma'(s2) \rightarrow dh2 
\end{aligned}
\tag{12}
$$

$$
\begin{aligned}
\frac{\partial Loss}{\partial h1} &= \frac{\partial Loss}{\partial h2}\frac{\partial h2}{\partial s1}\frac{\partial s1}{\partial h1} \\\\
&=(dh2 \cdot Q^{\top}) \odot \sigma'(s1) \rightarrow dh1 
\end{aligned}
\tag{13}
$$

对于其他时间步：

$$
dz = 0 \tag{14}
$$

$$
\begin{aligned}
\frac{\partial Loss}{\partial h2_t} &= \frac{\partial Loss}{\partial h2_{t+1}}\frac{\partial h2_{t+1}}{\partial s2_t}\frac{\partial s2_t}{\partial h2_t}
\\\\
&=(dh2_{t+1} \cdot W2^{\top}) \odot \sigma'(s2_t) \rightarrow dh2_t
\end{aligned}
\tag{15}
$$

$$
dV = 0 \tag{16}
$$

$$
\begin{aligned}
\frac{\partial Loss}{\partial h1_t} &= \frac{\partial Loss}{\partial h1_{t+1}}\frac{\partial h1_{t+1}}{\partial s1_t}\frac{\partial s1_t}{\partial h1_t}+\frac{\partial loss_t}{\partial h2_t}\frac{\partial h2_t}{\partial s1_t}\frac{\partial s1_t}{\partial h1_t}
\\\\
&=(dh1_{t+1} \cdot W1^{\top} + dh2_t\cdot Q^{\top}) \odot \sigma'(s1_t) \rightarrow dh1_t
\end{aligned}
\tag{17}
$$

对于第一个时间步：

$$
dW1 = 0, dW2 = 0 \tag{18}
$$

对于其他时间步：

$$
\frac{\partial Loss}{\partial W1}=s1^{\top}_ {t-1} \cdot dh_1 \rightarrow dW1 \tag{19}
$$

$$
\frac{\partial Loss}{\partial W2}=s2^{\top}_ {t-1} \cdot dh2 \rightarrow dW2 \tag{20}
$$

对于所有时间步：

$$
\frac{\partial Loss}{\partial Q}=\frac{\partial Loss}{\partial h2}\frac{\partial h2}{\partial Q}=s1^{\top} \cdot dh2 \rightarrow dQ \tag{21}
$$

$$
\frac{\partial Loss}{\partial U}=\frac{\partial Loss}{\partial h1}\frac{\partial h1}{\partial U}=x^{\top} \cdot dh1 \rightarrow dU \tag{22}
$$

#### 代码实现

```Python
class timestep(object):
    def backward(self, y, prev_s1, prev_s2, next_dh1, next_dh2, isFirst, isLast):
        ...
```

### 19.6.4 运行结果

#### 超参设置

我们搭建一个双隐层的循环神经网络，隐层1的神经元数为2，隐层2的神经元数也为2，其它参数保持与单隐层的循环神经网络一致：

- 网络类型：回归
- 时间步数：24
- 学习率：0.05
- 最大迭代数：100
- 批大小：64
- 输入特征数：6
- 输出维度：1

#### 训练结果

训练过程如图19-21所示，训练结果如表19-10所示。

<img src="./img/19/deep_rnn_loss.png"/>

图19-21 训练过程中的损失函数值和准确度的变化

表19-10 预测时长与准确度的关系

|预测时长|结果|预测结果|
|---|---|---|
|8|损失函数值：<br/>0.001157<br/>准确度：<br/>0.740684|<img src="./img/19/deeprnn_pm25_fitting_result_24_8.png" height="240"/>
|4|损失函数值：<br/>0.000644<br/>准确度：<br/>0.855700|<img src="./img/19/deeprnn_pm25_fitting_result_24_4.png" height="240"/>
|2|损失函数值：<br/>0.000377<br/>准确度：<br/>0.915486|<img src="./img/19/deeprnn_pm25_fitting_result_24_2.png" height="240"/>
|1|损失函数值：<br/>0.000239<br/>准确度：<br/>0.946411|<img src="./img/19/deeprnn_pm25_fitting_result_24_1.png" height="240"/>

#### 与单层循环神经网络的比较

对于19.3节中的单层循环神经网络，参数配置如下：
```
U: 6x4+4=28
V: 4x1+1= 5
W: 4x4  =16
-----------
Total:   49
```

对于两层的循环神经网络来说，参数配置如下：

```
U: 6x2=12
Q: 2x2= 4
V: 2x1= 2
W1:2x2= 4
W2:2x2= 4
---------
Total: 26
```

表19-11 预测结果比较

||单隐层循环神经网络|深度（双层）循环神经网络|
|---|---|---|
|参数个数|49|26|
|损失函数值（8小时）|0.001171|0.001157|
|损失函数值（4小时）|0.000686|0.000644|
|损失函数值（2小时）|0.000414|0.000377|
|损失函数值（1小时）|0.000268|0.000239|
|准确率值（8小时）|0.737769|0.740684|
|准确率值（4小时）|0.846447|0.855700|
|准确率值（2小时）|0.907291|0.915486|
|准确率值（1小时）|0.940090|0.946411|

从表19-11可以看到，双层的循环神经网络在参数少的情况下，取得了比单层循环神经网络好的效果。

### 代码位置

ch19, Level6





<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 19.7 双向循环神经网络

### 19.7.1 深度循环神经网络的结构图

前面学习的内容，都是因为“过去”的时间步的状态对“未来”的时间步的状态有影响，在本节中，我们将学习一种双向影响的结构，即双向循环神经网络。

比如在一个语音识别的模型中，可能前面的一个词听上去比较模糊，会产生多个猜测，但是后面的词都很清晰，于是可以用后面的词来为前面的词提供一个最有把握（概率最大）的猜测。再比如，在手写识别应用中，前面的笔划与后面的笔划是相互影响的，特别是后面的笔划对整个字的识别有较大的影响。

在本节中会出现两组相似的词：前向计算、反向传播、正向循环、逆向循环。区别如下：

- 前向计算：是指神经网络中通常所说的前向计算，包括正向循环的前向计算和逆向循环的前向计算。
- 反向传播：是指神经网络中通常所说的反向传播，包括正向循环的反向传播和逆向循环的反向传播。
- 正向循环：是指双向循环神经网络中的从左到右时间步。在正向过程中，会存在前向计算和反向传播。
- 逆向循环：是指双向循环神经网络中的从右到左时间步。在逆向过程中，也会存在前向计算和反向传播。

很多资料中关于双向循环神经网络的示意如图19-22所示。

<img src="./img/19/bi_rnn_net_wrong.png"/>

图19-22 双向循环神经网络结构图（不正确）

在图19-22中，$h_{tn}$中的n表示时间步，在图中取值为1至4。
- $h_{t1}$至$h_{t4}$是正向循环的四个隐层状态值，$U$、$V$、$W$ 分别是它们的权重矩阵值；
- $h'_ {t1}$至$h'_ {t4}$是逆向循环的四个隐层状态值，$U'$、$V'$、 $W'$ 分别是它们的权重矩阵值；
- $S_{t1}$至$S_{t4}$是正逆两个方向的隐层状态值的和。

但是，请大家记住，图19-22和上面的相关解释是不正确的！主要的问题集中在 $s_t$ 是如何生成的。

$h_t$ 和 $h'_ t$ 到$s_t$之间不是矩阵相乘的关系，所以没有 $V$ 和 $V'$ 这两个权重矩阵。

正向循环的最后一个时间步$h_{t4}$和逆向循环的第一个时间步$h_{t4}'$共同生成$s_{t4}$，这也是不对的。因为对于正向循环来说，用 $h_{t4}$ 没问题。但是对于逆向循环来说，$h'_ {t4}$ 只是第一个时间步的结果，后面的计算还未发生，所以 $h'_{t4}$ 非常不准确。

正确的双向循环神经网络图应该如图19-23所示。

<img src="./img/19/bi_rnn_net_right.png"/>

图19-23 双向循环神经网络结构图

用$h1/s1$表示正向循环的隐层状态，$U1$、$W1$表示权重矩阵；用$h2/s2$表示逆向循环的隐层状态，$U2$、$W2$表示权重矩阵。$s$ 是 $h$ 的激活函数结果。

请注意上下两组$x_{t1}$至$x_{t4}$的顺序是相反的：

- 对于正向循环的最后一个时间步来说，$x_{t4}$ 作为输入，$s1_{t4}$是最后一个时间步的隐层值；
- 对于逆向循环的最后一个时间步来说，$x_{t1}$ 作为输入，$s2_{t4}$是最后一个时间步的隐层值；
- 然后 $s1_{t4}$ 和 $s2_{t4}$ 拼接得到 $s_{t4}$，再通过与权重矩阵 $V$ 相乘得出 $Z$。

这就解决了图19-22中的逆向循环在第一个时间步的输出不准确的问题，对于两个方向的循环，都是用最后一个时间步的输出。

图19-23中的 $s$ 节点有两种，一种是绿色实心的，表示有实际输出；另一种是绿色空心的，表示没有实际输出，对于没有实际输出的节点，也不需要做反向传播的计算。

如果需要在每个时间步都有输出，那么图19-23也是一种合理的结构，而图19-22就无法解释了。

### 19.7.2 前向计算

我们先假设应用场景只需要在最后一个时间步有输出（比如19.4节和19.5节中的应用就是如此），所以t2所代表的所有中间步都没有a、loss、y三个节点（用空心的圆表示），只有最后一个时间步有输出。

与前面的单向循环网络不同的是，由于有逆向网络的存在，在逆向过程中，t3是第一个时间步，t1是最后一个时间步，所以t1也应该有输出。

#### 公式推导

$$
h1 = x \cdot U1 + s1_{t-1} \cdot W1 \tag{1}
$$

注意公式1在t1时，$s1_{t-1}$是空，所以加法的第二项不存在。

$$
s1 = Tanh(h1) \tag{2}
$$

$$
h2 = x \cdot U2 + s2_{t-1} \cdot W2 \tag{3}
$$

注意公式3在t1时，$s2_{t-1}$是空，所以加法的第二项不存在。而且 $x$ 是颠倒时序后的值。

$$
s2 = Tanh(h2) \tag{4}
$$

$$
s = s1 \oplus s2 \tag{5}
$$

公式5有几种实现方式，比如sum（矩阵求和）、concat（矩阵拼接）、mul（矩阵相乘）、ave（矩阵平均），我们在这里使用矩阵求和，这样在反向传播时的公式比较容易推导。

$$
z = s \cdot V \tag{6}
$$

$$
a = Softmax(z) \tag{7}
$$

公式4、5、6、7只在最后一个时间步发生。

#### 代码实现

由于是双向的，所以在主过程中，存在一正一反两个计算链，1表示正向，2表示逆向，3表示输出时的计算。

```Python
class timestep(object):
    def forward_1(self, x1, U1, bU1, W1, prev_s1, isFirst):
        ...

    def forward_2(self, x2, U2, bU2, W2, prev_s2, isFirst):
        ...

    def forward_3(self, V, bV, isLast):
        ...
```

### 19.7.3 反向传播

#### 正向循环的反向传播

先推导正向循环的反向传播公式，即关于h1、s1节点的计算。

对于最后一个时间步（即$\tau$）：

$$
\frac{\partial Loss}{\partial z_\tau} = \frac{\partial loss_\tau}{\partial z_\tau}=a_\tau-y_\tau \rightarrow dz_\tau \tag{8}
$$

对于其它时间步来说$dz_t=0$，因为不需要输出。

因为$s=s1 + s2$，所以$\frac{\partial s}{\partial s1}=1$，代入下面的公式中：

$$
\begin{aligned}  
\frac{\partial Loss}{\partial h1_\tau}&=\frac{\partial loss_\tau}{\partial h1_\tau}=\frac{\partial loss_\tau}{\partial z_\tau}\frac{\partial z_\tau}{\partial s_\tau}\frac{\partial s_\tau}{\partial s1_\tau}\frac{\partial s1_\tau}{\partial h1_\tau} \\\\
&=dz_\tau \cdot V^T \odot \sigma'(s1_\tau) \rightarrow dh1_\tau 
\end{aligned}
\tag{9}
$$

其中，下标$\tau$表示最后一个时间步，$\sigma'(s1)$表示激活函数的导数，$s1$是激活函数的数值。下同。

比较公式9和19.3节通用循环神经网络模型中的公式9，形式上是完全相同的，原因是$\frac{\partial s}{\partial s1}=1$，并没有给我们带来任何额外的计算，所以关于其他时间步的推导也应该相同。

对于中间的所有时间步，除了本时间步的$loss_t$回传误差外，后一个时间步的$h1_{t+1}$也会回传误差：

$$
\begin{aligned}
\frac{\partial Loss}{\partial h1_t} &= \frac{\partial loss_t}{\partial z_t}\frac{\partial z_t}{\partial s_t}\frac{\partial s_t}{\partial s1_t}\frac{\partial s1_t}{\partial h1_t} + \frac{\partial Loss}{\partial h1_{t+1}}\frac{\partial h1_{t+1}}{\partial s1_{t}}\frac{\partial s1_t}{\partial h1_t}
\\\\
&=dz_t \cdot V^{\top} \odot \sigma'(s1_t) + \frac{\partial Loss}{\partial h1_{t+1}} \cdot W1^{\top} \odot \sigma'(s1_t)
\\\\
&=(dz_t \cdot V^{\top} + dh1_{t+1} \cdot W1^{\top}) \odot \sigma'(s1_t) \rightarrow dh1_t
\end{aligned} \tag{10}
$$

公式10中的$dh1_{t+1}$，就是上一步中计算得到的$dh1_t$，如果追溯到最开始，即公式9中的$dh1_\tau$。因此，先有最后一个时间步的$dh1_\tau$，然后依次向前推，就可以得到所有时间步的$dh1_t$。

对于$V$来说，只有当前时间步的损失函数会给它反向传播的误差，与别的时间步没有关系，所以有：

$$
\frac{\partial loss_t}{\partial V_t} = \frac{\partial loss_t}{\partial z_t}\frac{\partial z_t}{\partial V_t}= s_t^{\top} \cdot dz_t \rightarrow dV_t \tag{11}
$$

对于$U1$，后面的时间步都会给它反向传播误差，但是我们只从$h1$节点考虑：

$$
\frac{\partial Loss}{\partial U1_t} = \frac{\partial Loss}{\partial h1_t}\frac{\partial h1_t}{\partial U1_t}= x^{\top}_ t \cdot dh1_t \rightarrow dU1_t \tag{12}
$$

对于$W1$，和$U1$的考虑是一样的，只从当前时间步的$h1$节点考虑：

$$
\frac{\partial Loss}{\partial W1_t} = \frac{\partial Loss}{\partial h1_t}\frac{\partial h1_t}{\partial W1_t}= s1_{t-1}^{\top} \cdot dh1_t \rightarrow dW1_t \tag{13}
$$

对于第一个时间步，$s1_{t-1}$不存在，所以没有$dW1$：

$$
dW1 = 0 \tag{14}
$$

#### 逆向循环的反向传播

逆向循环的反向传播和正向循环一模一样，只是把 $1$ 变成 $2$ 即可，比如公式13变成：

$$
\frac{\partial Loss}{\partial W2_t} = \frac{\partial Loss}{\partial h2_t}\frac{\partial h2_t}{\partial W2_t}= s2_{t-1}^{\top} \cdot dh2_t \rightarrow dW2_t
$$

### 19.7.4 代码实现

#### 单向循环神经网络的效果

为了与单向的循环神经网络比较，笔者在Level3_Base的基础上实现了一个MNIST分类，超参如下：

```Python
    net_type = NetType.MultipleClassifier # 多分类
    output_type = OutputType.LastStep     # 只在最后一个时间步输出
    num_step = 28
    eta = 0.005                           # 学习率
    max_epoch = 100
    batch_size = 128
    num_input = 28
    num_hidden = 32                       # 隐层神经元32个
    num_output = 10
```

得到的训练结果如下：

```
...
99:42784:0.005000 loss=0.212298, acc=0.943200
99:42999:0.005000 loss=0.200447, acc=0.947200
save last parameters...
testing...
loss=0.186573, acc=0.948800
load best parameters...
testing...
loss=0.176821, acc=0.951700
```

最好的时间点的权重矩阵参数得到的准确率为95.17%，损失函数值为0.176821。

#### 双向循环神经网络的效果
```Python
    eta = 0.01
    max_epoch = 100
    batch_size = 128
    num_step = 28
    num_input = 28
    num_hidden1 = 20          # 正向循环隐层神经元20个
    num_hidden2 = 20          # 逆向循环隐层神经元20个
    num_output = 10
```

得到的结果如图19-23所示。

<img src="./img/19/bi_rnn_loss.png"/>

图19-23 训练过程中损失函数值和准确率的变化

下面是打印输出：

```
...
save best parameters...
98:42569:0.002000 loss=0.163360, acc=0.955200
99:42784:0.002000 loss=0.164529, acc=0.954200
99:42999:0.002000 loss=0.163679, acc=0.955200
save last parameters...
testing...
loss=0.144703, acc=0.958000
load best parameters...
testing...
loss=0.146799, acc=0.958000
```

最好的时间点的权重矩阵参数得到的准确率为95.59%，损失函数值为0.153259。

#### 比较

表19-12 单向和双向循环神经网络的比较

||单向|双向|
|---|---|---|
|参数个数|2281|2060|
|准确率|95.17%|95.8%|
|损失函数值|0.176|0.144|

### 代码位置

ch19, Level7

其中，Level7_Base_MNIST.py是单向的循环神经网络，Level7_BiRnn_MNIST.py是双向的循环神经网络。


<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

# 第20章 高级循环神经网络

## 20.0 高级循环神经网络概述

### 20.0.1 传统循环神经网络的不足

在上一章中，介绍了循环神经网络的由来和发展历史，传统循环神经网络的原理和应用。循环神经网络弥补了前馈神经网络的不足，可以更好的处理时序相关的问题，扩大了神经网络解决问题的范围。

但传统的循环神经网络也有自身的缺陷，由于容易产生梯度爆炸和梯度消失的问题，导致很难处理长距离的依赖。传统神经网络模型，不论是一对多、多对一、多对多，都很难处理不确定序列输出的问题，一般需要输出序列为1，或与输入相同。在机器翻译等问题上产生了局限性。

### 20.0.2 高级循环神经网络简介

针对上述问题，科学家们对普通循环神经网络进行改进，以便处理更复杂场景的数据的模型，提出了如LSTM, GRU, Seq2Seq等模型。此外，注意力（Attention）机制的引入，使得Seq2Seq模型的性能得到提升，关于Attention的内容，目前没有列入章节，以后将会进行补充。

下面简单介绍本章将会讲解的三种网络模型。

#### 长短时记忆网络（LSTM）
长短时记忆网络（LSTM）是最先提出的改进算法，由于门控单元的引入，从根本上解决了梯度爆炸和消失的问题，使网络可以处理长距离依赖。

#### 门控循环单元网络（GRU）
LSTM网络结构中有三个门控单元和两个状态，参数较多，实现复杂。为此，针对LSTM提出了许多变体，其中门控循环单元网络是最流行的一种，它将三个门减少为两个，状态也只保留一个，和普通循环神经网络保持一致。

#### 序列到序列网络（Sequence-to-Sequence）
LSTM与其变体很好地解决了网络中梯度爆炸和消失的问题。但LSTM有一个缺陷，无法处理输入和输出序列不等长的问题，为此提出了序列到序列（Sequence-to-Sequence, 简称Seq2Seq）模型，引入和编码解码机制（Encoder-Decoder），在机器翻译等领域取得了很大的成果，进一步提升了循环神经网络的处理范围。






