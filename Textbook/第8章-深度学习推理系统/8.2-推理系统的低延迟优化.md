<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 8.2 推理系统的低延迟优化

- [8.2 推理系统的低延迟优化](#82-推理系统的低延迟优化)
  - [8.2.1 推理(inference)延迟(latency)](#821-推理inference延迟latency)
  - [8.2.2 层间与张量融合](#822-层间与张量融合)
  - [8.2.3 目标后端自动调优](#823-目标后端自动调优)
  - [8.2.4 内存分配策略调优](#824-内存分配策略调优)
  - [8.2.5 低精度推理与精度校准](#825-低精度推理与精度校准)
  - [8.2.6 模型压缩](#826-模型压缩)
  - [参考文献](#参考文献)

## 8.2.1 推理(inference)延迟(latency)

延迟(latency)是在客户端给出查询后，推理系统呈现推理结果所花费的时间。 推理服务通常位于关键路径上，因此预测必须既快速同时满足有限的尾部延迟(Tail Latency)才能满足服务水平协议(Service Level Agreement)。例如：Service Level Agreement(SLA): 次秒(Sub-second)级别延迟服务水平协议。

为了达到低延迟，推理系统面临以下的挑战：
- 交互式应用程序的低延迟需求通常与离线批处理训练框架设计的目标不一致
- 简单的模型速度快，复杂的模型更加准确，但是浮点运算量更大
- 次秒(Sub-second)级别延迟约束制了批尺寸(Batch Size)
- 模型融合或多租容易引起长尾延迟(Long Tail Traffic)现象

推理系统常常通过以下几个方向进行模型推理的延迟优化：

- 模型优化：降低访存开销
  - 层间融合或张量融合(Layer & Tensor Fusion)
  - 目标后端自动调优
  - 内存分配策略调优
- 允许降低一定的准确度，进而降低计算量，最终降低延迟
  - 低精度推理与精度校准(Precision Calibration)
  - 模型压缩(Model Compression)
- 自适应批尺寸(Batch Size)：动态调整需要进行推理的输入数据数量。
- 缓存(Caching)结果：复用已推理的结果或特征。

## 8.2.2 层间与张量融合

- 融合的原因：
  - 相对于内核启动开销和每个层的张量数据读写成本，内核(Kernel)计算通常非常快
  - 导致内存带宽瓶颈和可用GPU资源的利用不足
- 目标：最小化GPU访存和最大化GPU资源利用率
- 搜索计算图的最优融合策略

<center> <img src="./img/2/8-2-1-fusion.png" width="400" height="140" /></center>
<center>图8-2-1. 内核执行时间线</center>

<center> <img src="./img/2/8-2-2-fusionopt.png" width="500" height="200" /></center>
<center>图8-2-2. TensorRT使用内核融合进行模型执行优化</center>


## 8.2.3 目标后端自动调优

- Halide
- TVM
- Ansor

## 8.2.4 内存分配策略调优

- 设备或服务端内存是紧缺资源
- 目标：最小化内存占用和内存分配调用开销
  - 仅在每个张量为其分配内存
- 约束：保证延迟SLA
- 策略：
  - 缓存分配器(Cached Allocator)
  - 预取(Pre-fetching)和卸载(Off-loading)
  - 算子融合(Operator Fusion)

## 8.2.5 低精度推理与精度校准

- 推理阶段可以适当降低精度
  - 大多数深度学习框架都以完整的32位精度（FP32）训练神经网络
  - 对模型进行充分训练后，由于不需要进行梯度反向传播，因此推理计算可以使用半精度FP16甚至INT8张量运算
  - 使用较低的精度会导致较小的模型大小，较低的内存利用率和延迟以及较高的吞吐量
- 目标：最小化数据精度
- 约束：准确度损失  

<center> <img src="./img/2/8-2-3-lowprecision.png" width="400" height="100" /></center>
<center>图8-2-3. 精度取值范围</center>

- INT8编码FP32同样信息:
  - Tensor Values = FP32 scale factor * int8 array
  - 添加饱和阈值准确度损失更低 
- 目标：最小化信息损失
- KL散度度量
  - P, Q - two discrete probability distributions.
  - KL_divergence(P,Q):= SUM(P[i] * log(P[i] / Q[i] ), i)
- 策略：
  - 校准数据集运行FP32推理
  - 对每层数据收集激活输出直方图
  - 生成使用不同饱和阈值产生的量化输出分布
  - 选择最小化激活输出分布与量化后的激活输出分布之间KL散度阈值

<center> <img src="./img/2/8-2-4-precision-calibration.png" width="300" height="400" /></center>
<center>图8-2-4. 精度校准</center>

## 8.2.6 模型压缩

- 模型压缩的收益：
  - 减少浮点运算量，降低延迟
  - 减少内存占用，提升利用率
- 定义模型压缩问题
  
$$min_{policy_i}\{Model\_Size(Policy_i)\}$$
- 约束
$$accuracy(Policy_i) \geq accuracy\_SLA$$

- 参数裁剪和共享（Parameter Pruning and Sharing）
  - 剪枝(Pruning)
  - 量化(Quantization)
  - 编码(Encoding)
- 低秩分解（Low-rank Factorization）
- 知识精炼（Knowledge Distillation）
- …

剪枝算法

- Train Connectivity
  - 通过正常的网络训练来学习连通性
  - 但是，与常规训练不同，不是为了学习权重的最- 终值，而是在学习哪些连接重要
- Prune Connections
  - 所有权重低于阈值的连接都将删除，从而将稠密- 网络转换为稀疏网络
- Train Weights
  - 对剩余网络权重进行重新训练
  - 如果使用修剪后的网络而不进行重新训练，则准确性会受到很大影响

优化延迟的目标，受到空间与准确度的约束
层间与张量融合受到哪些约束?
推理和训练优化内存分配策略的侧重点是否有不同？

<center> <img src="./img/2/8-2-5-modelcompress.png" width="400" height="250" /></center>
<center>图8-2-4. 精度校准</center>

<!-- ## 8.2.6 自适应批尺寸

## 8.2.7 缓存结果 -->

## 参考文献 

- [Park, Jongsoo et al. “Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications.” ArXiv abs/1811.09886 (2018): n. pag.](https://arxiv.org/abs/1811.09886)
- [Crankshaw, Daniel et al. “Clipper: A Low-Latency Online Prediction Serving System.” NSDI (2017).](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17). Association for Computing Machinery, New York, NY, USA, 1387–1395. DOI:https://doi.org/10.1145/3097983.3098021](https://research.google/pubs/pub46484/)
- [Olston, Christopher et al. “TensorFlow-Serving: Flexible, High-Performance ML Serving.” ArXiv abs/1712.06139 (2017): n. pag.](https://arxiv.org/abs/1712.06139)
- [Jeong-Min Yun, Yuxiong He, Sameh Elnikety, and Shaolei Ren. 2015. Optimal Aggregation Policy for Reducing Tail Latency of Web Search. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). Association for Computing Machinery, New York, NY, USA, 63–72. DOI:https://doi.org/10.1145/2766462.2767708](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
- [Cheng, Yu et al. “A Survey of Model Compression and Acceleration for Deep Neural Networks.” ArXiv abs/1710.09282 (2017): n. pag.](https://arxiv.org/abs/1710.09282)
- [CSE 599W: System for ML - Model Serving](https://dlsys.cs.washington.edu/)
- https://developer.nvidia.com/deep-learning-performance-training-inference 
- [Han, Song et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition (2016): n. pag.](https://arxiv.org/abs/1510.00149) 
- [Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both weights and connections for efficient neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 (NIPS'15). MIT Press, Cambridge, MA, USA, 1135–1143.](https://arxiv.org/abs/1506.02626)
- [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)
- [Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. SIGPLAN Not. 48, 6 (June 2013), 519–530. DOI:https://doi.org/10.1145/2499370.2462176](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
- [Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: an automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 579–594.](https://arxiv.org/abs/1802.04799)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)