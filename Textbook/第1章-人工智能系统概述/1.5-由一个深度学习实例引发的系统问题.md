<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 1.5 由一个深度学习实例引发的系统问题


- [1.5 由一个深度学习实例引发的系统问题](#15-由一个深度学习实例引发的系统问题)
  - [1.4.1 深度学习训练实例与背后潜在的系统问题](#141-深度学习训练实例与背后潜在的系统问题)
  - [1.4.2 通过for循环实现卷积](#142-通过for循环实现卷积)
  - [1.4.3 更大范围的系统问题](#143-更大范围的系统问题)
  - [参考文献](#参考文献)

## 1.4.1 深度学习训练实例与背后潜在的系统问题

下面的实例是PyTorch在MNIST数据集上训练一个卷积神经网络的实例。

```
# 本实例来源PyTorch官方实例 https://github.com/pytorch/examples/blob/main/mnist/main.py 
...
import torch
...

# 如果模型层数多，权重多到无法在单GPU显存防止，我们需要通过模型并行方式进行训练，读者可以参考第6章进行了解。
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 请参考1.4.2 通过循环实现卷积理解卷积的执行逻辑
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        # 我们能否调整超参数32为64？如何高效的搜索最有的配置？这些内容我们将在第9章展开介绍
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output


def train(args, model, device, train_loader, optimizer, epoch):
    # 如何进行高效的训练，运行时是如何执行的？我们将在第3章进行介绍
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        ...


def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            # 推理系统如何高效进行模型推理？我们将在第8章进行介绍
            output = model(data)
            ...


def main():
    ...
    # 当前语句决定了使用哪种加速器以及体系结构，读者可以通过第4章了解。
    device = torch.device("cuda" if use_cuda else "cpu")
    # 如果batch size过大，造成单GPU内存无法容纳模型及中间激活的张量，读者可以参考第6章进行了解如何分布式训练。
    train_kwargs = {'batch_size': args.batch_size}
    test_kwargs = {'batch_size': args.test_batch_size}
    ...
    # 如何高效的进行数据读取？这些内容我们将在第7章进行介绍。
    # 如果我们训练的数据集和模型是预测系统优化配置，我们想训练的模型是优化系统配置，那么读者可以参考第13章，思考如何将AI应用到系统优化。
    # 如果我们的数据集没有提前准备好，需要实时和环境交互获取，那么读者可以参考第10章进行理解
    dataset1 = datasets.MNIST('../data', train=True, download=True,
                       transform=transform)
    dataset2 = datasets.MNIST('../data', train=False,
                       transform=transform)
    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)
    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)
    model = Net().to(device)
    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)
    ... 
    for epoch in range(1, args.epochs + 1):
        train(args, model, device, train_loader, optimizer, epoch)
        # 模型如果训练完成需要部署，我们如何压缩和量化后再部署？读者可以参考第11章进行了解。
        test(model, device, test_loader)
        ... 

    if args.save_model:
        torch.save(model.state_dict(), "mnist_cnn.pt")

# 如果用户提交多个这样的训练作业，系统如何调度和管理资源？读者可以参考第7章进行了解
if __name__ == '__main__':
    main()
```

## 1.4.2 通过for循环实现卷积

底层算子的具体实现时由其对应的矩阵运算翻译为对应的循环（目前我们简化问题，忽略stride等其他超参数影响）。
```
# 为简化阐述计算过程，我们简化了维度(dimension)的形状推导(shape inference)。

# 批尺寸维度
for n in range(N):
  # 输出张量通道维度
  for k in range(K):
    # 输入张量通道维度
    for c in range(C):
      # 输出张量高度维度
      for h in range(H):
        # 输出张量宽度维度
        for w in range(W):
          # 卷积核高度维度
          for fh in range(FH):
            # 卷积核宽度维度
            for w in range(FW):
              # 乘加运算
              output[h,w,k] += input[h + fw, w + fh, c] * kernel[fw,fh,c,k]  
```

在这其中有很多有趣的问题问题读者可以思考与预估：
- 其中参与计算的输入，权重和输出张量能否完全放入GPU缓存(L1，L2)？如果不能放入则需要通过块(tile)优化进行切片，这些内容将在第5章着重介绍
- 循环执行的主要计算语句是否有局部性可以利用？空间局部性（缓存线内相邻的空间是否会被连续访问）以及时间局部性（同一块内存多久后还会被继续访问），这样我们可以通过预估后，尽可能的通过编译调度循环执行，这些内容将在第5章着重介绍
- 如果有些权重为0是否可以不进行计算？读者可以参考第11章稀疏性部分进行了解
- 读者可以[预估]()各个层的output，input，和kernel张量大小，进而评估是否需要多卡，以及换入换出策略等。

## 1.4.3 更大范围的系统问题

- 更多的超参数组合与模型结构探索
  - 之前我们看到的实例本身是单个模型的样例，但是深度学习模型可以通过变换其中的超参数和模型结构获取和训练更好的结果，这种探索式的过程也叫做自动化机器学习，读者可以参考第9章-自动化机器学习系统了解相关领域内容与挑战。
- 共享的资源与多租的环境
  - 如果我们现在的GPU等训练资源都是被公司或组织机构集中管理，用户需要共享使用资源进而提升资源整体利用率，那么在这种环境下系统如何提供给算法工程师接近单机的使用环境体验让算法工程师更加简便高效的使用资源？读者可以参考第7章-异构计算集群调度与资源管理系统进行了解平台如何应对当前的挑战。
- 假设数据无法离线提前准备好？
  - 如果数据没有提前准备好，需要系统提供更加多样的训练方式，深度学习系统需要不断与环境或者模拟器交互，通过强化学习方式进行训练，读者可以参考第10章-强化学习系统进行了解，强化学习系统如何在更复杂与多样的场景下进行模型训练以及数据获取。
- 数据和人工智能模型的安全与隐私如何保障？
  - 当前深度学习为数据驱动的方法，同时会产生交付的模型文件，模型泄露，篡改以及本身的缺陷会造成潜在的安全风险。如何保障深度学习整体的安全与隐私相比传统安全领域遇到了新的挑战，读者可以参考第12章-人工智能安全与隐私进行了解。
- 之前我们大部分了解的是针对人工智能负载做系统设计也称作System for AI，反过来我们也可以思考如何通过人工智能这种数据驱动的方法反过来指导系统设计与优化，也就是AI for System，读者可以参考第13章-人工智能优化计算机系统进行了解。


## 参考文献
