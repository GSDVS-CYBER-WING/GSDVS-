<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->


# 8.4 部署

- [8.4 部署](#84-部署)
  - [8.4.1 可靠性(Reliability)和可扩展性(Scalability)](#841-可靠性reliability和可扩展性scalability)
  - [8.4.2 部署灵活性](#842-部署灵活性)
  - [8.4.3 模型转换与开放协议](#843-模型转换与开放协议)
  - [8.4.4 MLOps:模型版本管理，线上发布，回滚策略](#844-mlops模型版本管理线上发布回滚策略)
  - [8.4.5 移动端部署](#845-移动端部署)
  - [8.4.6 代表性推理芯片](#846-代表性推理芯片)
  - [8.4.7 推理系统部署实验与练习](#847-推理系统部署实验与练习)
    - [8.4.7.1 运行你的第一个容器 - 内容，步骤，作业](#8471-运行你的第一个容器---内容步骤作业)
    - [8.4.7.2 Docker部署PyTorch推理程序 - 内容，步骤，作业](#8472-docker部署pytorch推理程序---内容步骤作业)
    - [8.4.7.3 延迟和吞吐量实验](#8473-延迟和吞吐量实验)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

推理系统进行模型部署时，需要应对多样的框架，多样的部署硬件，以及持续集成和持续部署的模型上线发布等诸多的软件工程问题，本小节将围绕部署过程中涉及到的可靠性(reliability)，可扩展性(scalability)，灵活性(flexibility)，版本 管理和移动端(mobile)部署的挑战进行展开。

## 8.4.1 可靠性(Reliability)和可扩展性(Scalability)

当推理系统部署到生产环境中，需要7x24小时不间断对用户提供相应的在线推理服务。在服务用户的过程中需要对不一致的数据，软件、用户配置和底层执行环境故障等造成的中断有弹性(resilience)，能够快速恢复服务，达到一定的可靠性，保证服务等级协议。同时推理系统也需要优雅地扩展，进而应对生产环境中流量的增加的场景。
综上，推理系统在设计之初就需要考虑提供更好的扩展性。推理系统随着请求负载增加而自动和动态的部署更多的实例，进而才可以应对更大负载，提供更高的推理吞吐和让推理系统更加可靠。

如图所示，通过底层的部署平台（例如，Kubernetes）的支持，用户可以通过配置方便地描述和自动部署多个推理服务的副本，并通过部署前端负载均衡服务达到负载均衡，进而达到高扩展性，同时更多的副本也使得推理服务有了更高的可靠性。

<center> <img src="./img/4/8-4-2-scalability.png" width="500" height="260" /></center>
<center>图8-4-1. Kubernetes部署训练和推理服务</center>

## 8.4.2 部署灵活性

由于在模型训练的过程中，研究员和工程师不断尝试业界领先模型或不断尝试新的超参数和模型结构。由于框架开源，很多新的模型使用的框架类型和版本多样。推理系统需要支持多样的深度学习框架所保存的模型文件，并和其他系统服务进行交互。同时由于框架开源，社区活跃，不断的更新版本，对推理系统对不同版本的支持也提出了挑战。从性能角度考虑，大多数深度学习框架是为训练优化，有些框架甚至不支持在线推理。最后，在部署模型后，整个推理的流水线需要做一定的数据处理或者多模型融合(ensemble)，推理系统也需要支持与不同语言接口和不同逻辑的应用结合。这些因素为推理系统提出了灵活性的挑战。通常有以下解决方法：

- 深度学习模型开放协议：通过ONNX等模型开放协议和工具，将不同框架的模型进行转换，优化和部署。
  - 跨框架模型转换。
- 接口抽象：将模型文件封装并提供特定语言的调用接口。
  - 提供构建不同应用逻辑的灵活性。
  - 提供不同框架的通用抽象。
- 跨进程通信(RPC)：可以将不同的模型或数据处理模块封装为微服务，通过跨进程调用进行推理流水线构建。
  - 跨语言，跨进程通信。
- 镜像和容器技术：通过镜像技术解决多版本问题
  - 运行时环境依赖与资源隔离。

## 8.4.3 模型转换与开放协议

由于目前存在很多深度学习框架已经开源，并可以被开发者选用，同时很多公司自研深度学习框架，并通过相应的框架开源预训练模型。这样一种生态造成有人工智能业务的公司切换，微调和部署模型的工程成本较高，频繁切换模型和需要自研模型转换工具。为了缓解这个痛点，业界有相应的两大类工作来缓解当前问题。

- 模型中间表达标准([ONNX](https://onnx.ai/))：让框架，工具和运行时有一套通用的模型标准，使得优化和工具能够被复用。
  - ONNX是一种用于表示机器学习模型的开放格式。ONNX定义了一组通用运算符(机器学习和深度学习模型的构建块)，以及一种通用文件格式，使AI开发人员能够使用被各种框架、工具、运行时和编译器所支持的深度学习模型。
  - 同时，对ONNX支持较好的还有模型优化与部署的运行时框架[ONNX Runtime](https://onnxruntime.ai/)
- 模型转换工具([MMdnn](https://github.com/microsoft/MMdnn))：让模型可以打通不同框架已有工具链，实现更加方便的部署或迁移学习。
  - 模型通过中间表达(IR)和相应的对应框架的模型解析器(parser)和对应框架的模型发生器(emitter)实现跨框架转换。

如下图所示，ONNX标准成为衔接不同框架与部署环境(服务端和移动端)的桥梁，通过规范的中间表达，模型解析器，优化和后端代码生成的工具链得以复用，减少了开发与维护代价。

<center> <img src="./img/4/8-4-3-onnx.png" width="500" height="230" /></center>
<center>图8-4-2. 模型构建与部署</center>

## 8.4.4 MLOps:模型版本管理，线上发布，回滚策略

推理系统本身就像传统Web服务发布代码包一样，需要定期发布模型，提供新功能或更好的效果。
这些挑战类似于应用程序开发团队在创建和管理应用程序时面临的挑战。借鉴传统的软件工程最佳实践，业界使用了DevOps，这是管理应用程序开发周期操作的行业标准。为了应对深度学习全生命周期的挑战，组织需要一种将 DevOps 的敏捷性带入ML 生命周期的方法，业界称这种方法为[MLOps](https://docs.microsoft.com/en-us/learn/modules/start-ml-lifecycle-mlops/2-mlops-introduction)

<center> <img src="./img/4/8-4-5-mlops.png" width="380" height="300" /></center>
<center>图8-4-2. 模型构建与部署 </center>

在MLOps中，其中较为关键的是模型的版本管理：线上发布，回滚等策略。因为近些年，软件逐渐由客户端软件演化为在线部署的服务，而模型最终也是部署于一定的软件系统中，所以越来越多的模型部署于在线服务中(online service)。在在线服务中， 每隔一段时间训练出的新版本模型替换线上模型，但是可能存在缺陷，另外如果新版本模型发现缺陷需要回滚。

模型生命周期管理的策略实例有：金丝雀(canary)策略，回滚(rollback)策略。
- 金丝雀策略
  - 当获得一个新训练的模型版本时，当前服务的模型成为第二新版本时，用户可以选择同时保持这两个版本
  - 将所有推理请求流量发送到当前两个版本，比较它们的效果
  - 一旦对最新版本达标，用户就可以切换到仅使用最新版本
  - 该策略需要更多的高峰资源，避免将用户暴露于缺陷模型
- 回滚策略
  - 如果在当前的主要服务版本上检测到缺陷，则用户可以请求切换到特定的较旧版本
  - 卸载和装载的顺序应该是可配置的
  - 当问题解决并且获取到新的安全版本模型时，从而结束回滚

## 8.4.5 移动端部署

除了服务端的部署，深度学习模型的另一大场景就是移动端部署，随着越来越多的物联网设备智能化，越来越多的移动端系统中开始部署深度学习模型。移动端部署应用常常有以下场景：智能设备(smart devices)，智慧城市(smart city)，互联工厂(connected factory) ，智慧办公室(smart office)，智能人体监测等。

在展开移动端部署内容前，我们先总结一下，云端部署模型特点与优势，这样才能对比出移动端部署的特点：
- 对功耗(energy)、温升、模型大小(model size)没有严格限制 
- 有用于训练和推理的强大硬件支持
- 集中的数据管理有利于模型训练
- 模型更容易在云端得到保护 
- 深度学习模型的执行平台和框架统一 

虽然云端部署深度学习模型有很多的好处，但同时我们也应该看到，云端部署推理服务也存在一定的问题：
- 云上提供所有AI服务成本高昂
- 部署严格的数据隐私问题(data privacy)
- 数据传输成本(data transfer cost)
- 推理服务依赖于网络的依赖
- 很难定制化(model customization)模型

所以很多场景下模型推理也会考虑在端和云混合情况下提供AI服务。
那么在移动端部署存在哪些挑战呢？
- 严格约束功耗(Power consumption)、热量、模型大小（<10MB） 
- 硬件算力对推理服务来说不足
- 数据分散且难以训练 
- 模型在边缘更容易受到攻击 
- DNN平台多样，无通用解决方案 

在下图中，我们总结了移动端部署全景图：
- 应用层算法优化：很多模型在考虑到移动端部署的苛刻资源约束条件下，都纷纷提供tiny版本供移动端部署和使用。
- 高效率模型设计：通过模型压缩，量化，神经网络结构搜索(NAS)等技术，提升移动端的模型效率。
- 移动端代表性框架：[TensorFlow Lite](https://www.tensorflow.org/lite)，[MACE](https://github.com/XiaoMi/mace)，[ONNX Runtime](https://onnxruntime.ai/)等框架更好的支持模型转换，模型优化与后端生成。
- 移动端芯片：针对推理负载相比训练负载的不同，提供更加高效的低功耗芯片支持。

<center> <img src="./img/4/8-4-1-mobilestack.png" width="500" height="200" /></center>
<center>图8-4-1. 并行启动执行作业可能产生的问题</center>

## 8.4.6 代表性推理芯片

许多架构师认为，成本-能源-性能的重大改进现在必须来自特定领域的硬件。通过针对深度学习设计的芯片，可以加速神经网络 (NN) 的推理。相比CPU的时变([time-varying](https://www.ibm.com/docs/en/zos/2.2.0?topic=time-cpu-variation))优化，神经网络芯片提供更加确定性的模型，有助于保证低延迟，在保证延迟的同时超越之前基准的平均吞吐量，同时精简不必要的功能，让其有较小的功耗。

推理系统最终底层还是通过编译器将深度学习模型翻译成矩阵运算，并在芯片中执行相应的[乘加运算](https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation)，我们可以通过以下一些代表性的芯片了解从硬件角度是如何针对推理任务的特点进行推理芯片端的计算与优化支持。

以下是一些代表性芯片的研究工作和产品：

- [Google TPU](https://dl.acm.org/doi/10.1145/3079856.3080246) 
- NVIDIA 
  - 数据中心芯片：[A100](https://www.nvidia.com/en-us/data-center/a100/)
  - 移动端嵌入式系统：[Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)
- [寒武纪Diannao系列芯片](https://dl.acm.org/doi/10.1145/2654822.2541967#:~:text=We%20show%20that%20it%20is,accelerator%20is%20117.87x%20faster)
- [FPGA](https://www.intel.com/content/www/us/en/developer/learn/course-deep-learning-inference-fpga.html)：FPGA 提供了一种极低延迟、灵活的架构，可在节能解决方案中实现深度学习加速。

## 8.4.7 推理系统部署实验与练习

配置Container进行云上训练或推理。实验目的：配置使用容器，进行自定义深度学习推理。并进行性能和压力测试。

### 8.4.7.1 [运行你的第一个容器 - 内容，步骤，作业](https://github.com/microsoft/AI-System/blob/main/Labs/BasicLabs/Lab5/alpine.md)
读者可以参考Lab中的相应[实例](https://github.com/microsoft/AI-System/blob/main/Labs/BasicLabs/Lab5/alpine.md)，学会部署Docker容器。
### 8.4.7.2 [Docker部署PyTorch推理程序 - 内容，步骤，作业](https://github.com/microsoft/AI-System/blob/main/Labs/BasicLabs/Lab5/inference.md)
读者可以根据Lab中的[实例](https://github.com/microsoft/AI-System/blob/main/Labs/BasicLabs/Lab5/inference.md)，进行推理服务的容器部署。

### 8.4.7.3 延迟和吞吐量实验

读者可以通过相关工具（例如，[JMeter](https://jmeter.apache.org/usermanual/curl.html)）对推理服务进行性能测试。关注响应延迟和吞吐量等性能指标。

## 小结与讨论

本小节主要围绕推理系统的部署展开讨论，推理系统在部署模型时，需要考虑部署的扩展性，灵活性，版本管理，移动端部署等多样的问题，我们在本章针对这些问题总结了业界相关代表性的系统和方法。未来期望读者能以全生命周期的视角看待人工智能的训练与部署，这样才能真正的做好人工智能的工程化实践。

## 参考文献

- [Park, Jongsoo et al. “Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications.” ArXiv abs/1811.09886 (2018): n. pag.](https://arxiv.org/abs/1811.09886)
- [Crankshaw, Daniel et al. “Clipper: A Low-Latency Online Prediction Serving System.” NSDI (2017).](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17). Association for Computing Machinery, New York, NY, USA, 1387–1395. DOI:https://doi.org/10.1145/3097983.3098021](https://research.google/pubs/pub46484/)
- [Olston, Christopher et al. “TensorFlow-Serving: Flexible, High-Performance ML Serving.” ArXiv abs/1712.06139 (2017): n. pag.](https://arxiv.org/abs/1712.06139)
- [Jeong-Min Yun, Yuxiong He, Sameh Elnikety, and Shaolei Ren. 2015. Optimal Aggregation Policy for Reducing Tail Latency of Web Search. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). Association for Computing Machinery, New York, NY, USA, 63–72. DOI:https://doi.org/10.1145/2766462.2767708](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
- [Cheng, Yu et al. “A Survey of Model Compression and Acceleration for Deep Neural Networks.” ArXiv abs/1710.09282 (2017): n. pag.](https://arxiv.org/abs/1710.09282)
- [CSE 599W: System for ML - Model Serving](https://dlsys.cs.washington.edu/)
- https://developer.nvidia.com/deep-learning-performance-training-inference 
- [Han, Song et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition (2016): n. pag.](https://arxiv.org/abs/1510.00149) 
- [Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both weights and connections for efficient neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 (NIPS'15). MIT Press, Cambridge, MA, USA, 1135–1143.](https://arxiv.org/abs/1506.02626)
- [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)
- [Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. SIGPLAN Not. 48, 6 (June 2013), 519–530. DOI:https://doi.org/10.1145/2499370.2462176](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
- [Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: an automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 579–594.](https://arxiv.org/abs/1802.04799)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)
- [ADVANCED AI EMBEDDED SYSTEMS](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)
- [Hot Chips 33](https://hc33.hotchips.org/)
- [The 19th ACM International Conference on Mobile Systems, Applications, and Services](https://www.sigmobile.org/mobisys/2021/program.html#ml)
- [NVIDIA AI INFERENCE TECHNICAL OVERVIEW](https://www.nvidia.com/en-us/data-center/resources/inference-technical-overview/)
- [Tutorial: Deep Learning Inference Optimizations for CPU](https://hc33.hotchips.org/assets/program/tutorials/HC2021.Intel.Guokai%20Ma.V2.pdf)
- [Reuther, A. et al. “Survey of Machine Learning Accelerators.” 2020 IEEE High Performance Extreme Computing Conference (HPEC) (2020): 1-12.](https://arxiv.org/pdf/2009.00993.pdf)
- https://www.nvidia.com/en-us/data-center/a100/