<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->


# 8.4 部署


推理系统进行模型部署时，需要应对多样的框架，多样的部署硬件，以及持续集成和持续部署的模型上线发布等诸多的软件工程问题，本小节将围绕部署过程中涉及到的可靠性(Reliability)，可扩展性(Scalability)，灵活性(Flexibility)，版本 管理和移动端(Mobile)部署的挑战进行展开。

- [8.4 部署](#84-部署)
  - [8.4.1 可靠性(Reliability)和可扩展性(Scalability)](#841-可靠性reliability和可扩展性scalability)
  - [8.4.2 部署灵活性](#842-部署灵活性)
  - [8.4.3 模型转换与开放协议](#843-模型转换与开放协议)
  - [8.4.4 移动端部署](#844-移动端部署)
  - [8.4.5 推理芯片](#845-推理芯片)
  - [8.4.6 推理系统部署与压测实验练习](#846-推理系统部署与压测实验练习)
    - [8.4.6.1 运行你的第一个容器 - 内容，步骤，作业](#8461-运行你的第一个容器---内容步骤作业)
    - [8.4.6.2 Docker部署PyTorch推理程序 - 内容，步骤，作业](#8462-docker部署pytorch推理程序---内容步骤作业)
    - [8.4.6.3 延迟和吞吐量实验](#8463-延迟和吞吐量实验)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

## 8.4.1 可靠性(Reliability)和可扩展性(Scalability)

当推理系统部署到生产环境中，需要7x24小时不间断对用户提供相应的在线推理服务。在服务用户的过程中需要对不一致的数据，软件、用户配置和底层执行环境故障等造成的中断有弹性(Resilience)，能够快速恢复服务，达到一定的可靠性，保证服务等级协议。同时推理系统也需要优雅地扩展，进而应对生产环境中流量的增加的场景。
综上，推理系统在设计之初就需要考虑提供更好的扩展性。推理系统随着请求负载增加而自动和动态的部署更多的实例，进而才可以应对更大负载，提供更高的推理吞吐和让推理系统更加可靠。

如图所示，通过底层的部署平台（例如，Kubernetes）的支持，用户可以通过配置方便地描述和自动部署多个推理服务的副本，并通过部署前端负载均衡服务达到负载均衡，进而达到高扩展性，同时更多的副本也使得推理服务有了更高的可靠性。
<style>table{margin: auto;}</style>
<center> <img src="./img/4/8-4-2-scalability.png" width="500" height="300" /></center>
<center>图8-4-1. Kubernetes部署训练和推理服务</center>

## 8.4.2 部署灵活性

由于在模型训练的过程中，研究员和工程师不断尝试业界领先模型或不断尝试新的超参数和模型结构。由于框架开源，很多新的模型使用的框架类型和版本多样。推理系统需要支持多样的深度学习框架所保存的模型文件，并和其他系统服务进行交互。同时由于框架开源，社区活跃，不断的更新版本，对推理系统对不同版本的支持也提出了挑战。从性能角度考虑，大多数深度学习框架是为训练优化，有些框架甚至不支持在线推理。最后，在部署模型后，整个推理的流水线需要做一定的数据处理或者多模型融合(Ensemble)，推理系统也需要支持与不同语言接口和不同逻辑的应用结合。这些因素为推理系统提出了灵活性的挑战。通常有以下解决方法：

- 深度学习模型开放协议：通过ONNX等模型开放协议和工具，将不同框架的模型进行转换，优化和部署。
  - 跨框架模型转换。
- 接口抽象：将模型文件封装并提供特定语言的调用接口。
  - 提供构建不同应用逻辑的灵活性。
  - 提供不同框架的通用抽象。
- 远程过程调用(Remote Procedure Call)：可以将不同的模型或数据处理模块封装为微服务，通过远程过程调用(RPC)进行推理流水线构建。
  - 跨语言，远程过程调用。
- 镜像和容器技术：通过镜像技术解决多版本问题
  - 运行时环境依赖与资源隔离。

## 8.4.3 模型转换与开放协议

由于目前存在很多深度学习框架已经开源，并可以被开发者选用，同时很多公司自研深度学习框架，并通过相应的框架开源预训练模型。这样一种生态造成有人工智能业务的公司切换，微调和部署模型的工程成本较高，频繁切换模型和需要自研模型转换工具。为了缓解这个痛点，业界有相应的两大类工作来缓解当前问题。

- 模型中间表达标准([ONNX](https://onnx.ai/))：让框架，工具和运行时有一套通用的模型标准，使得优化和工具能够被复用。
  - ONNX是一种用于表示机器学习模型的开放格式。ONNX定义了一组通用运算符(机器学习和深度学习模型的构建块)，以及一种通用文件格式，使AI开发人员能够使用被各种框架、工具、运行时和编译器所支持的深度学习模型。
  - 同时，对ONNX支持较好的还有模型优化与部署的运行时框架[ONNX Runtime](https://onnxruntime.ai/)。
 
  如下图所示，ONNX标准成为衔接不同框架与部署环境(服务端和移动端)的桥梁，通过规范的中间表达，模型解析器，优化和后端代码生成的工具链得以复用，减少了开发与维护代价。
<style>table{margin: auto;}</style>
<center> <img src="./img/4/8-4-3-onnx.png" width="500" height="230" /></center>
<center>图8-4-2. 模型构建与部署</center>

- 模型转换工具([MMdnn](https://github.com/microsoft/MMdnn))：让模型可以打通不同框架已有工具链，实现更加方便的部署或迁移学习(Transfer Learning)。

如下图所示，模型可以通过中间表达(IR)和相应的对应框架的模型解析器(Parser)和对应框架的模型发射器(Emitter)实现跨框架转换。这样，例如某家机构如果开源了模型文件但是是通过TensorFlow训练的，而我想通过PyTorch对其进行迁移学习微调，则可以通过当前的方式进行模型的转换。
<style>table{margin: auto;}</style>
<center> <img src="./img/4/8-4-1-mmdnn.jpg" width="500" height="430" /></center>
<center>图8-4-3. 通过MMdnn模型转换(Model Convert) <a href="https://github.com/microsoft/MMdnn">图片引用</a></center>

## 8.4.4 移动端部署

除了服务端的部署，深度学习模型的另一大场景就是移动端部署，随着越来越多的物联网设备智能化，越来越多的移动端系统中开始部署深度学习模型。移动端部署应用常常有以下场景：智能设备，智慧城市，智能工业互联网，智慧办公室等。

在展开移动端部署内容前，我们先总结一下，云端部署模型特点与优势，这样才能对比出移动端部署的特点：
- 对功耗(Power Consumption)、温度、模型尺寸(Model Size)没有严格限制 
- 有用于训练和推理的强大硬件支持
- 集中的数据管理有利于模型训练
- 模型更容易在云端得到保护 
- 深度学习模型的执行平台和框架统一 

虽然云端部署深度学习模型有很多的好处，但同时我们也应该看到，云端部署推理服务也存在一定的问题：
- 云上提供所有人工智能服务成本高昂
- 部署严格的数据隐私(Data Privacy)问题
- 数据传输成本
- 推理服务依赖于网络的依赖
- 很难定制化模型

所以很多场景下模型推理也会考虑在端和云混合情况下提供AI服务。
那么在移动端部署存在哪些挑战呢？
- 严格约束功耗(Power Consumption)、热量、模型尺寸(Model Size)小于设备内存 
- 硬件算力对推理服务来说不足
- 数据分散且难以训练 
- 模型在边缘更容易受到攻击 
- DNN平台多样，无通用解决方案 

移动端部署各层的工作与挑战：
- 应用层算法优化：很多模型在考虑到移动端部署的苛刻资源约束条件下，都纷纷提供tiny版本供移动端部署和使用。
- 高效率模型设计：通过模型压缩，量化，神经网络结构搜索(NAS)等技术，提升移动端的模型效率。
- 移动端代表性框架：[TensorFlow Lite](https://www.tensorflow.org/lite)，[MACE](https://github.com/XiaoMi/mace)，[ONNX Runtime](https://onnxruntime.ai/)等框架更好的支持模型转换，模型优化与后端生成。
- 移动端芯片：针对推理负载相比训练负载的不同，提供更加高效的低功耗芯片支持，例如[Google Edge TPU](https://cloud.google.com/edge-tpu), [NVIDIA Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)等。

一般我们部署移动端模型的过程中，涉及以下步骤，在每个步骤中一般有相应的工具或者优化算法所支持。

移动端模型部署实践与常见步骤：

1. 设计与选择模型
   - 模型设计之初，针对模型结构和超参数的设计，就可以考虑移动端的约束（内存，浮点运算量，功耗等），这样就可以对[移动端资源指导的搜索空间进行剪枝](https://ieeexplore.ieee.org/document/9402095)，在模型设计与训练阶段就提前规避和减少返工流程和节省训练资源。
   - 在选择预训练好的模型时，用户可以通过模型动物园(Model Zoo)挑选合适的模型。用户一般可以选择针对移动端所训练或者压缩出的经典模型，一般以-tiny,-mobile和-lite等后缀进行命名说明
2. 模型精简
   - 可以通过[模型压缩(Model Compression)](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)减少权重，[模型量化(Model Quantization)](https://pytorch.org/docs/stable/quantization.html)减少数据精度。
3. 模型文件格式转换
   - 当前大多数框架都是为训练设计，在性能和对移动端优化的支持上没有特殊的设计，同时移动端部署的系统框架和训练侧不一致，需要有相应的工具（MMdnn，ONNX等）进行格式转换。
4. 移动端代码生成与优化
   - 同时移动端部署环境，语言与接口多样，所以需要有特定的工具，或者编译器（例如，TVM，NNfusion等）进行模型文件格式转换和后端优化代码生成。
5. 部署
   - 运行时可以通过硬件或软件的稀疏性等机制，在运行时进行加速。
   - 运行时也可以通过针对移动端设计的内存管理策略优化内存消耗。

## 8.4.5 推理芯片

在学术界, 神经网络芯片在2010年左右开始萌芽。The International Symposium on Computer Architecture(ISCA) 2010上，来自法国国立计算机及自动化研究院(INRIA Saclay)的Olivier Temam教授做了["The Rebirth of Neural Networks"](https://pages.saclay.inria.fr/olivier.temam/homepage/ISCA2010web.pdf)的报告。在此次报告中，Olivier Temam指出了1990s像英特尔(Intel)等公司构建硬件神经网络商用系统的应用场景局限性，提出人工神经网网络(Artificial Neural Network)的缺陷容忍(Defect Tolerance)特点，和深度神经网络(Deep Neural Network)的趋势，提出神经网络加速器设计的方向。在12年的ISCA上，Olivier Temam教授提出人工智能(AI)加速器的设计["A defect-tolerant accelerator for emerging high-performance applications"](https://dl.acm.org/doi/10.1145/2366231.2337200)，利用人工神经网网络(Artificial Neural Network)的缺陷容忍特性，提出空间扩展网络(Spatially Expanded Network)相比时分复用(Time-Multiplexed)架构提升能效的优势，并评估了缺陷容忍(Defect Tolerance)效果。在前瞻性的预判之后，Olivier Temam与中科院计算所陈天石，陈云霁的DianNao系列工作。

在工业界，以Google为代表性的工作也在较早的初期在生产环境遇到和对未来神经网络芯片较早进行布局和研发。例如，Google许多架构师认为，成本-能源-性能的重大改进现在必须来自特定领域的硬件。通过针对深度学习设计的芯片，可以加速神经网络 (NN) 的推理。相比CPU的时变([time-varying](https://www.ibm.com/docs/en/zos/2.2.0?topic=time-cpu-variation))优化，神经网络芯片提供更加确定性的模型，有助于保证低延迟，在保证延迟的同时超越之前基准的平均吞吐量，同时精简不必要的功能，让其有较小的功耗。

推理系统最终底层还是通过编译器将深度学习模型翻译成矩阵运算，并在芯片中执行相应的[乘积累加运算(MAC)](https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation)，我们可以通过以下一些代表性的芯片了解从硬件角度是如何针对推理任务的特点进行推理芯片端的计算与优化支持。

以下是一些代表性芯片的研究工作和产品：

- [Google TPU (Tensor Processing Unit)系列ASIC(Application-specific integrated circuit)芯片](https://dl.acm.org/doi/10.1145/3079856.3080246) 
  - Google在2015年推出了针对推理场景的TPUv1，之后再2017你那针对训练场景的TPUv2，TPUv3是在TPUv2基础上做了进一步的性能提升。目前TPU在[Google Cloud](https://cloud.google.com/tpu)中作为一种定制设计的机器学习专用集成电路(Application-Specific Integrated Circuit)，并已经广泛应用于Google的产品，如翻译，照片，搜索和Gmail等。
  - [TPUv1](https://arxiv.org/ftp/arxiv/papers/1704/1704.04760.pdf)的很多特点适合推理场景：
    - 缘起于2013年Google数据中心中的工作负载需求。语音识别服务希望数据中心能提供两倍的算力满足用户需求，但是传统的CPU如果实现这个需求非常昂贵。设计初衷是提供相比GPU的10倍性价比(Cost-Performance)。
    - 确定性(Deterministic)的执行模型，有助于保持推理场景请求的P99th延迟满足SLA。因为其精简了CPU和GPU的很多影响确定性执行的优化（缓存，乱序执行，多线程，多进程，预取等）
    - 因为精简了以上优化，也同时让即使拥有大量[乘积累加运算(MAC)](https://zh.wikipedia.org/wiki/%E4%B9%98%E7%A9%8D%E7%B4%AF%E5%8A%A0%E9%81%8B%E7%AE%97)和更大片上存储器(On-Chip Memory)，TPU也能拥有较小的功耗。
    - 仅支持前向传播用于推理，矩阵乘，卷积和特定的激活函数算子。不需要考虑求导，存储中间结果用于反向传播和对多样的损失函数支持。这样使得硬件更为精简高效。
    - TPUv1作为加速器(Accelerator)通过PCIe总线(Bus)与服务器连接，同时主机可以发送指令给TPU。
- [中科院计算所 DianNao系列ASIC芯片](https://dl.acm.org/doi/10.1145/2654822.2541967#:~:text=We%20show%20that%20it%20is,accelerator%20is%20117.87x%20faster)
  - DianNao系列工作提出了一系列定制的神经网络加速器的设计方案。首先从加速器[DianNao](https://dl.acm.org/doi/10.1145/2541940.2541967)开始，其提出之前的机器学习加速器没有关注到当前卷积神经网络和深度神经网络体积大占用内存高的特点，重点关注内存对加速器设计，内存和功耗的影响。之后展开[DaDiannao](https://ieeexplore.ieee.org/document/7011421)提出多片（Multi-Chip）设计，通过多片设计，将卷积神经网网络和深度神经网络模型能够放置在片上存储(On Chip Storage)。之后的加速器[ShiDiannao](https://ieeexplore.ieee.org/document/7284058)工作将卷积神经网络加速器与传感器(CMOS或CCD传感器)相连，从而减少访存(DRAM access)开销。第四个加速器[PuDiannao](https://dl.acm.org/doi/10.1145/2775054.2694358)工作将加速器从只支持特定神经网络扩宽到支持7种常规机器学习算法。
- NVIDIA系列GPU(Graphics Processing Unit)
  - 数据中心芯片：[A100](https://www.nvidia.com/en-us/data-center/a100/)引入了适合推理的功能来优化工作负载。它加速了从FP32到INT4的全方位精度。 例如，[多实例GPU (MIG)](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)技术提供了GPU虚拟化支持，对数据中心负载混合部署，提升资源利用率有很大帮助。同时提供稀疏性优化支持以提升性能。
  - 移动端嵌入式系统：例如，[Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)是NVIDIA推出的边缘AI硬件与软件栈。Jetson包括Jetson 模组（小巧的高性能计算机）、可加速软件的开发套间JetPack SDK，以及包含传感器、SDK、服务和产品的完整生态系统。通过Jetson进行嵌入式AI的开发能提升开发速度。Jetson具有和NVIDIA其他AI软件兼容的特点，能为提供在边缘端AI所需的性能和功耗。
- [现场可编程门阵列(Field-Programmable Gate Array)](https://www.intel.com/content/www/us/en/developer/learn/course-deep-learning-inference-fpga.html)：FPGA提供了一种极低延迟、灵活的架构，可在节能解决方案中实现深度学习加速。FPGA 包含一组可编程逻辑块和可重构互连的层次结构。与其他芯片相比，FPGA 提供了可编程性和性能的结合。FPGA可以实现实时推理请求的低延迟。不需要异步请求（批处理）。批处理可能会导致更高的延迟，因为需要处理更多数据。因此，与CPU和GPU处理器相比，延迟可以低很多倍。例如，微软Azure就提供了[FPGA推理服务](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-fpga-web-service)。Azure 上的FPGA基于英特尔的 FPGA 设备，数据科学家和开发人员使用这些设备来加速实时AI推理。

<style>table{margin: auto;}</style>
<center> <img src="./img/4/8-4-1-cpu-gpu-fpga.png" width="500" height="400" /></center>
<center>图8-4-4. 对比CPU, GPU和FPGA/ASIC</center>

通过图中我们可以看到，相比通用计算CPU，由于在深度学习专用计算场景，指令更加确定，更多的片上空间可以节省用于放置计算，同时可以通过硬件逻辑减少指令流水线的加载代价，提升数据流处理的吞吐量。

## 8.4.6 推理系统部署与压测实验练习

配置容器(Container)进行云上训练或推理。实验目的：配置使用容器，进行自定义深度学习推理。并进行性能和压力测试。

### 8.4.6.1 [运行你的第一个容器 - 内容，步骤，作业](https://github.com/microsoft/AI-System/blob/main/Labs/BasicLabs/Lab5/alpine.md)
读者可以参考Lab中的相应[实例](https://github.com/microsoft/AI-System/blob/main/Labs/BasicLabs/Lab5/alpine.md)，学会部署Docker容器。
### 8.4.6.2 [Docker部署PyTorch推理程序 - 内容，步骤，作业](https://github.com/microsoft/AI-System/blob/main/Labs/BasicLabs/Lab5/inference.md)
读者可以根据Lab中的[实例](https://github.com/microsoft/AI-System/blob/main/Labs/BasicLabs/Lab5/inference.md)，进行推理服务的容器部署。

### 8.4.6.3 延迟和吞吐量实验

读者可以通过相关工具（例如，[JMeter](https://jmeter.apache.org/usermanual/curl.html)）对推理服务进行性能测试。关注响应延迟和吞吐量等性能指标。

## 小结与讨论

本小节主要围绕推理系统的部署展开讨论，推理系统在部署模型时，需要考虑部署的扩展性，灵活性，版本管理，移动端部署等多样的问题，我们在本章针对这些问题总结了业界相关代表性的系统和方法。未来期望读者能以全生命周期的视角看待人工智能的训练与部署，这样才能真正的做好人工智能的工程化实践。

## 参考文献

- [Park, Jongsoo et al. “Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications.” ArXiv abs/1811.09886 (2018): n. pag.](https://arxiv.org/abs/1811.09886)
- [Crankshaw, Daniel et al. “Clipper: A Low-Latency Online Prediction Serving System.” NSDI (2017).](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17). Association for Computing Machinery, New York, NY, USA, 1387–1395. DOI:https://doi.org/10.1145/3097983.3098021](https://research.google/pubs/pub46484/)
- [Olston, Christopher et al. “TensorFlow-Serving: Flexible, High-Performance ML Serving.” ArXiv abs/1712.06139 (2017): n. pag.](https://arxiv.org/abs/1712.06139)
- [Jeong-Min Yun, Yuxiong He, Sameh Elnikety, and Shaolei Ren. 2015. Optimal Aggregation Policy for Reducing Tail Latency of Web Search. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). Association for Computing Machinery, New York, NY, USA, 63–72. DOI:https://doi.org/10.1145/2766462.2767708](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
- [Cheng, Yu et al. “A Survey of Model Compression and Acceleration for Deep Neural Networks.” ArXiv abs/1710.09282 (2017): n. pag.](https://arxiv.org/abs/1710.09282)
- [CSE 599W: System for ML - Model Serving](https://dlsys.cs.washington.edu/)
- https://developer.nvidia.com/deep-learning-performance-training-inference 
- [Han, Song et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition (2016): n. pag.](https://arxiv.org/abs/1510.00149) 
- [Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both weights and connections for efficient neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 (NIPS'15). MIT Press, Cambridge, MA, USA, 1135–1143.](https://arxiv.org/abs/1506.02626)
- [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)
- [Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. SIGPLAN Not. 48, 6 (June 2013), 519–530. DOI:https://doi.org/10.1145/2499370.2462176](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
- [Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: an automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 579–594.](https://arxiv.org/abs/1802.04799)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)
- [ADVANCED AI EMBEDDED SYSTEMS](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)
- [Hot Chips 33](https://hc33.hotchips.org/)
- [The 19th ACM International Conference on Mobile Systems, Applications, and Services](https://www.sigmobile.org/mobisys/2021/program.html#ml)
- [NVIDIA AI INFERENCE TECHNICAL OVERVIEW](https://www.nvidia.com/en-us/data-center/resources/inference-technical-overview/)
- [Tutorial: Deep Learning Inference Optimizations for CPU](https://hc33.hotchips.org/assets/program/tutorials/HC2021.Intel.Guokai%20Ma.V2.pdf)
- [Reuther, A. et al. “Survey of Machine Learning Accelerators.” 2020 IEEE High Performance Extreme Computing Conference (HPEC) (2020): 1-12.](https://arxiv.org/pdf/2009.00993.pdf)
- https://www.nvidia.com/en-us/data-center/a100/
- [The Rebirth of Neural Networks](https://pages.saclay.inria.fr/olivier.temam/homepage/ISCA2010web.pdf)
- https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-fpga-web-service
- [Inside the Microsoft FPGA based configurable cloud](https://www.youtube.com/watch?v=v_4Ap1bjwgs)