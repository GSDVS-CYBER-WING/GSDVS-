<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

- [3.1 基于数据流图的深度学习框架](#31-基于数据流图的深度学习框架)
	- [3.1.1 深度学习框架发展概述](#311-深度学习框架发展概述)
	- [3.1.2 编程范式：声明式和命令式](#312-编程范式声明式和命令式)
	- [3.1.3 自动微分基础](#313-自动微分基础)
		- [前向微分](#前向微分)
		- [反向微分](#反向微分)
	- [3.1.4 基于数据流图的深度学习框架](#314-基于数据流图的深度学习框架)
		- [计算图](#计算图)
		- [张量和张量操作](#张量和张量操作)
		- [计算图上的自动微分](#计算图上的自动微分)
	- [3.1.5 计算图调度与执行](#315-计算图调度与执行)
		- [单设备算子间调度](#单设备算子间调度)
		- [计算图切分与多设备执行](#计算图切分与多设备执行)
	- [3.1.6 小结与讨论](#316-小结与讨论)
- [参考文献](#参考文献)

# 3.1 基于数据流图的深度学习框架

## 3.1.1 深度学习框架发展概述

神经网络是机器学习技术中一类具体算法分枝。通过堆叠基本处理单元形成宽度和深度构建出一个高度复杂的带参数非凸函数，对蕴含在各类数据分布中的统计规律进行拟合。神经网络方法能够通过调节（1）构成网络使用的处理单元,（2）处理单元之间的组合的方式,（3）网络的学习算法，用一种较为统一的算法设计视角解决各类应用任务，很大程度上减轻了传统机器学习方法在面对不同应用时，往往需要重新选择不同的函数空间，设计新的学习目标以达到所需的学习效果的设计困难。深度学习方法在图像分类，语音识别以及自然语言处理任务中取得的突破性进展，揭示了一些共同的特点：构建更大规模的神经网络对大规模数据进行学习。神经网络应用的开发需要对软件栈的各个抽象层进行编程，这对新算法的开发效率和算力都提出了很高的要求，进而催生了深度学习框架的发展。

深度学习框架是为了在加速器和集群上高效训练深度神经网络而设计的可编程系统，需要同时兼顾以下三大互相制约设计目标：
1. **可编程性**：使用易用的编程接口，用高层次语义描述出各类主流深度学习模型和训练算法。
2. **性能**：为可复用的处理单元提供高效实现；支持多设备、分布式计算。
3. **可扩展性**：降低新模型的开发成本。在添加新硬件支持时，降低增加计算原语和进行计算优化的开发成本。

以神经网络描述方式和编程范式的改变为区分，深度学习框架的发展经历了三个不同时期。不同框架的不同选择也影响了每个框架采用的优化手段以及所能达到的性能：

- **第一个时期**，主要驱动力是支持实验室中神经网络算法研究，支持在GPU或是集群上运行神经网络训练程序，加速复杂神经网络训练的同时提高验证新算法的实验效率。出现了以Theano[<sup>[1]</sup>](#theano-1)，Distbelief[<sup>[2]</sup>](#distbelief-2)为代表的深度学习框架先驱，这些早期工作定义了深度学习框架需要支持的基本功能，如：神经网络基本计算单元，自动微分，甚至是编译期优化。背后的设计理念对今天主流深度学习框架，特别是TensorFlow，产生了深远的影响。
- **第二个时期**，以一组连续堆叠的层表示深度神经网络模型，一层同时注册前向计算和梯度计算。这时的神经网络模型结构还较为简单，主要以支持卷积神经网络和全连接网络为主，出现了以Caffe[<sup>[3]</sup>](#caffe-3)，MxNet[<sup>[4]</sup>](#mxnet-4)为代表的开源工具。这些框架提供的开发方式与C/C++编程接口深度绑定，使得更多研究者能够利用框架提供的基础支持，快速添加高性能的新神经层模型和训练算法，方便地利用GPU来提高训练速度。这些工作进一步加强和验证了系统设计对神经网络算法模块化的抽象，推动了新网络处理单元和网络结构的进一步发展。
- **第三个时期**，以计算图描述深度神经网络。前期实践最终催生出了工业级深度学习框架：TensorFlow[<sup>[5]</sup>](#tensorflow-5)和PyTorch[<sup>[8]</sup>](#pytorch-8)，这一时期同时伴随着如Chainer[<sup>[7]</sup>](#chainer-7)，DyNet[<sup>[6]</sup>](#dynet-6)等激发了框架设计灵感的诸多实验项目。TensorFlow和PyTorch代表了今天深度学习框架两种不同的设计路径：系统性能优先改善灵活性和灵活性易用性优先改善系统性能。这两种选择，随着神经网络算法研究和应用的更进一步发展，又逐步造成了解决方案的分裂。

<p align="center">
<img src="img/frameworks-evolution.png" width=90%><br>
图1. 深度学习框架发展历程。
</p>

到目前阶段，神经网络模型结构越发多变，涌现出了大量如：TensorFlow Eager[<sup>[9]</sup>](#tf-eager-9)，TensorFlow Auto-graph[<sup>[11]</sup>](#auto-graph-11)，PyTorch JIT，JAX[<sup>[10]</sup>](#jax-10)这类呈现出设计选择融合的项目。这些项目纷纷采用设计特定领域语言(Domain-Specific Language，DSL）的思路，在提高描述神经网络算法表达能力和编程灵活性的同时，通过编译期优化技术来改善运行时性能。

## 3.1.2 编程范式：声明式和命令式

主流深度学习框架的使用都以Python这样的高层次语言为前端，提供脚本式的编程体验，后端用更低层次的编程模型和编程语言开发。后端高性能可复用模块与前端深度绑定，通过前端驱动后端方式执行。深度学习框架为前端用户提供声明式（declarative programming）和命令式（imperative programming）两种编程范式。

在声明式编程模型下，前端语言中的表达式不直接执行，而是构建起一个完整前向计算过程表示，对计算图经过优化然后再执行，又被称作define-and-run或是静态图。在命令式编程模型下，前端语言直接驱动后端算子执行，用户表达式会立即被求值，又被称作define-by-run或者动态图。命令式编程的优点是方便调试，灵活性高，但由于在执行前缺少对算法的统一描述，也失去了编译期优化的机会。相比之下，声明式编程对数据和控制流的静态性限制更强，由于能够在执行之前得到全程序描述，从而有机会进行运行前编译（Ahead-Of-Time）优化。

TensorFlow提供了命令式编程体验，Chainer和PyTroch提供了声明式的编程体验。但两种编程模型之间并不存在绝对的边界，multi-stage 编程和及时编译（Just-in-time, JIT)技术能够实现两种编程模式的混合。随着TensorFlow Eager和PyTorch JIT的加入，主流深度学习框架都选择了通过支持混合式编程以兼顾两者的优点。

## 3.1.3 自动微分基础

当用户构造完成一个深度神经网络时，在数学上这个网络对应了一个复杂的带参数的高度非凸函数，求解其中的可学习参数依赖于基于一阶梯度的迭代更新法。手工计算复杂函数的一阶梯度非常容易出错，自动微分（Automatic Differentiation，简称AD）系统就正为了解决这一问题而设计的一种自动化方法。自动微分关注给定一个由原子操作构成的复杂前向计算程序，如何自动生成出高效的反向计算程序。自动微分按照工作模式可分为前向自动微分和反向自动微分，按照实现方式，自动微分又可为：基于对偶数（dual number）的前向微分，基于Tape的反向微分系统，和基于源代码变换的反向微分系统。

自动微分是深度学习框架的核心组件之一，在进入深度学习框架之前，我们先通过一个简单的例子进一步理解自动微分的原理。

**例3.1**： $z=x*y+sin(x)$是一个简单的复合函数，图2是这个函数的表达式树。

<p align="center">
<img src="img/expression-tree-for-example.png" width=15%><br>
图2. 求值z的表达式树。
</p>

假设给定复合函数$z$，$x$和$y$均为标量。让我们思考两个问题：1. 计算机程序会如何通过一系列原子操作对$z$进行求值；2. 如何求解$z$对$x$和$y$的梯度？第一个问题十分直接。为了对$z$求值，依照表达式树定义的计算顺序，复合函数$z$可以被分解成一个求值序列（a.1）至（a.5），我们把这样一个给定输入逐步计算输出的求值序列称之为前向计算过程：

$$\begin{align}
x &= ? \tag{a.1}\\
y &= ? \tag{a.2}\\
a &= x * y \tag{a.3}\\
b &= sin(x) \tag{a.4}\\
z &= a+b  \tag{a.5}
\end{align}$$

### 前向微分

引入一个尚未被赋值的变量$t$，依据复合函数求导的链式法则，按照从（a.1）到（a.5）的顺序，依次令以上5个表达式分别对$t$求导，得到求值序列（b.1）至（b.5）：

$$
\begin{align}
\frac{\partial x}{\partial t} &= ? \tag{b.1}\\
\frac{\partial y}{\partial t} &= ? \tag{b.2}\\
\frac{\partial a}{\partial t} &= y *\frac{\partial x}{\partial t} + x * \frac{\partial y}{\partial t} \tag{b.3}\\
\frac{\partial b}{\partial t} &= cos(x) * \frac{\partial x}{\partial t} \tag{b.4}\\
\frac{\partial z}{\partial t} &= \frac{\partial a}{\partial t} + \frac{\partial b}{\partial t} \tag{b.5}\\
\end{align}
$$

引入导数变量$d\text{xx}$, 表示$\frac{\partial \text{xx}}{\partial t}$。令$t=x$，带入（b.1）至（b.5），会得到（c.1）至（c.5）:

$$
\begin{align}
\text{dx} &= 1 \tag{c.1}\\
\text{dy} &= 0 \tag{c.2}\\
\text{da} &= y \tag{c.3}\\
\text{db} &= cos(x) \tag{c.4}\\
\text{dz} &= y + cos(x) \tag{c.5}\\
\end{align}
$$

同理，令$t=y$ 带入（b.1）至（b.5），得到（d.1）至（d.5）:

$$
\begin{align}
\text{dx} &= 0 \tag{d.1}\\
\text{dy} &= 1 \tag{d.2}\\
\text{da} &= x \tag{d.3}\\
\text{db} &= 0 \tag{d.4}\\
\text{dz} &= x \tag{d.5}\\
\end{align}
$$

在（c.1）至（c.5）和（d.1）至（d.5）这样的两轮计算过程中，我们可以观察到：给定输入变量计算输出变量（前向计算）和给定输出变量计算输出变量对输入变量的导数，能够以完全一致的顺序进行，也就是导数的计算顺序和前向求值过程完全一致。运行（c.1）至（c.5）和（d.1）至（d.5）的过程称之为前向微分。

导数的计算往往依赖于前向计算的结果，由于前向微分导数的计算顺序和前向求值顺序完全一致。于是前向微分可以不用存储前向计算的中间结果，在前向计算的同时完成导数计算，从而节省大量内存空间。这是前向微分的巨大优点。利用这一事实前向微分存在一种基于对偶数（dual number）的简单且高效实现方式。同时可以观察到：前向微分的时间复杂度为$O(n)$，$n$是输入变量的个数。在上面的例子中，输入变量的个数为2，因此前向微分需要运行2次来计算输出变量对输入变量的导数。在神经网络学习中，输入参数个数$n$往往大于1，如果基于前向微分计算中间结果和输入的导数，需要多次运行程序，这也是前向微分应用于神经网络训练的一个局限性。

### 反向微分

链式求导法则是对称的，在计算导数$\frac{\partial \text{xx}}{\partial \text{x}}$时，链式求导法则并不关心哪个变量作为分母，哪个变量作为分子。于是，我们再次引入一个尚为被赋值的变量$s$，通过交换表达式（b.1）至（b.5）中分子和分母的顺序重写链式求导法则，于是得到（e.1）至（e.5）：

$$
\begin{align}
\frac{\partial s}{\partial z} &= ? \tag{e.1}\\
\frac{\partial s}{\partial b} &= \frac{\partial s}{\partial z} * \frac{\partial z}{\partial a} = \frac{\partial s}{\partial z} \tag{e.2}\\
\frac{\partial s}{\partial a} &= \frac{\partial s}{\partial z} * \frac{\partial z}{\partial b} = \frac{\partial s}{\partial z} \tag{e.3}\\
\frac{\partial s}{\partial y} &= \frac{\partial s}{\partial a} * \frac{\partial a }{\partial y} = \frac{\partial s}{\partial a} * x \tag{e.4}\\
\frac{\partial s}{\partial x} &= \frac{\partial s}{\partial a} * \frac{\partial a}{\partial x} + \frac{\partial s}{\partial b} * \frac{\partial b}{\partial x} &= \frac{\partial s}{\partial a} * y + \frac{\partial s}{\partial b} * \text{cos}(x) \tag{e.5}
\end{align}
$$

让我们用$g\text{xx}$替换导数变量$\frac{\partial s}{\partial \text{xx}}$带入（e.1）至（e.5），$g\text{xx}$称为$\text{xx}$的伴随变量（adjoint variable），于是有：

$$
\begin{align}
\text{g}z &= ? \tag{f.1}\\
\text{g}b &= \text{g}z \tag{f.2} \\
\text{g}a &= \text{g}z  \tag{f.3}\\
\text{g}y &= \text{g}a * x  \tag{f.4}\\
\text{g}x &= \text{g}a * y + \text{g}b * \text{cos}(x) \tag{f.5} \\
\end{align}
$$

带入$s = z$，得到:

$$
\begin{align}
\text{g}z &= 1 \tag{g.1} \\
\text{g}b &= 1 \tag{g.2} \\
\text{g}a &= 1 \tag{g.3} \\
\text{g}y &= x \tag{g.4} \\
\text{g}x &= y + \text{cos}(x)  \tag{g.5} \\
\end{align}
$$

表达式（g.1）至（g.2）求值的过程称之为反向微分。从中可以观察到：与前向微分的特点正好相反，在反向微分中，变量导数的计算顺序与变量的前向计算顺序正好相反；运行的时间复杂度是$O(m)$，$m$是输出变量的个数。在神经网络以及大量基于一阶导数方法进行训练的机器学习算法中，不论输入变量数目有多少，模型的输出一定是一个标量函数（损失函数）。这决定了保留前向计算的所有中间结果，只需再次运行程序一次，便可以用反向微分算法计算出损失函数对每个中间变量和输入的导数。反向微分的运行过程十分类似于”扫栈“，需要保留神经网络所有中间层前向结算的结果，对越接近输入层的中间层，其计算结果首先被压入栈中，而他们在反向计算时越晚被弹出栈。显然，网络越深，反向微分会消耗越多的内存，形成一个巨大的内存足迹。

至此，我们对两种自动微分模式进行小结：

1. 前向微分的时间复杂度为$O(n)$，$n$是输入变量的个数$n$。反向微分的时间复杂度为$O(m)$，$m$是输出变量的个数。当$n<m$时，前向微分复杂度更低； 当$n>m$时，反向微分复杂度更低。由于在神经网络训练中，总是输出一个标量形式的网络损失，于是$m=1$，反向微分更加适合神经网络的训练。
2. 当$n=m$时，前向微分和反向微分没有时间复杂度上的差异。但在前向微分中，由于导数能够与前向计算混合在一轮计算中完成，因此不需要存储中间计算结果，落实到更具体的代码实现中，也会带来更好的访问存储设备的局部性，因此前向微分更有优势。
3. 尽管在绝大多数情况下，神经网络的整体训练采用反向微分更加合理，但在局部网络使用前向微分依然是可能的。

## 3.1.4 基于数据流图的深度学习框架

第一章介绍了用户编写的神经网络程序通常包括（1）数据加载，（2）神经网络结构定义，（3）训练算法，正则化，梯度剪切等参数更新方法的定制和（4）训练循环这四个部分。这一节首先关注深度学习框架如何运行基于声明式编程模型的深度学习程序。

复杂神经网络往往由多个基本计算单元组合而成。为了高效地训练一个复杂神经网络，框架依然需要解决诸多问题， 例如：如何实现自动求导，如何利用编译期分析对神经网络计算进行化简、合并、变换，如何规划基本计算单元在加速器上的执行，如何将基本处理单元派发（dispatch）到特定的高效后端实现，如何进行内存预分配和管理等。用统一的方式解决这些问题都驱使着框架设计者思考统一神经网络计算的描述，从而使得在运行神经网络训练之前，便能对整个计算过程尽可能进行推断，在编译期自动为用户程序补全反向计算，规划执行，最大程度地降低运行时开销。目前主流的深度学习框架都选择使用计算图来抽象神经网络计算，图3展示了基于深度学习框架的组件划分。

<p align="center">
<img src="img/framework-overview.png" width=50%><br>
图3. 基于数据流图的深度学习框架的基本组件。
</p>

### 计算图

在科学计算中，计算图是一种经典的描述计算的方式。为了避免在调度执行计算图时陷入循环依赖，通常情况下计算图是一个有向无环图。图中的结点是系统后端支持的基本操作（primitive operation），没有副作用，不带状态，结点的行为完全由输入输出决定；结点之间的边显示地表示了操作之间的数据依赖关系。例3.2（图4）左侧是一个数据流图实例，右侧是对应的TensorFlow代码。图4中橘色圆形是数据流图中边上流动的数据，蓝色方形是数据流图中的基本操作。

<p align="center">
<img src="img/forward-computation-graph.png" width=60%><br>
图4. 例3.2前向计算数据流图实例。
</p>

### 张量和张量操作

科学计算任务中，数据常常被组织成一个高维数组，整个计算任务的绝大部分时间都消耗在这些高维数组上的数值计算操作上。高维数组和高维数组之上的数值计算是神经网络关注的核心，构成了计算图中最重要的一类基本算子。在这一节，我们首先考虑最为常用的稠密数组。

张量（Tensor）是一个高维数组，是对标量，向量，矩阵的推广。前端用户看到的张量由以下几个重要属性决定：

1. **元素的基本数据类型**：在一个张量中，所有元素具有相同的数据类型。
2. **形状**：张量是一个高维数组，每个维度具有固定的大小。张量的形状是一个整型数的元组，描述了一个张量具有几个维度以及每个维度的长度。
3. **设备**：决定了张量的存储设备。如CPU，GPU等。

标量，向量，矩阵分别是0维，1维和2维张量。图4左侧是一个声明为：`CPU`上形状为`(5, 3)`的整型张量的示意图，图4右侧是一个声明为：`GPU`上形状为`（8, 4, 4）`的浮点型张量的示意图。

<p align="center">
<img src="img/tensor.png" width=50%><br>
图5. 二维张量和三维张量示意图。
</p>

张量是基本数据类型整型，浮点型，布尔型，字符型的容器类型。高维数组为用户提供了一种逻辑上易于理解的方式来组织有着规则结构的数据，极大地提高了编程的可理解性。例如，图像任务通常将一副图片据组织成一个3维张量，维度对应着图像的长，宽和通道数目；自然语言处理任务中，一个矩阵被组织成一个2维张量维度对应着词向量和句子的长度。一组图像或者多个句子只需要为张量再增加一个批量（batch）维度。另一方面，使用高维数据组织数据，易于让后端自动化完成元素逻辑存储空间向物理存储空间的映射。更重要的是：张量操作将同构的基本运算类型作为一个整体进行批量操作，通常都隐含着很高的数据并行性，因此非常适合在单指令多数据（SIMD）并行后端上进行加速。对高维数组上的数值计算进行专门的代码优化，在科学计算和高性能计算领域有着悠久的研究历史，可以追溯到早期科学计算语言Fortran。

深度学习框架的设计也很自然地沿用了张量和张量操作，将其作为构造复杂神经网络的基本描述单元，前端用户可以在不陷入后端实现细节的情况下，在脚本语言中复用由后端优化过的张量操作。而计算开发者，能够隔离神经网络算法的细节，将张量计算作为一个独立的性能域，使用底层的编程模型和编程语言应用硬件相关优化。

### 计算图上的自动微分

尽管在[自动微分基础](#自动微分基础)一节的例3.1中，我们使用了标量形式的表达式来展示反向微分和前向微分的差异，并不阻碍理解反向微分如何工作于一个真实神经网络的训练过程。在真实的神经网络训练中，我们可以将上例中每一个基本表达式理解为计算图中的一个结点，这个结点对例3.1中标量形式的表达式进行了向量化的推广，对应着一个框架后端支持的张量操作。

假设，$\mathbf{Y} = G(\mathbf{X})$是一个基本求导原语，其中$\mathbf{Y} = [y_1\ \cdots \ y_m]$和$\mathbf{X}=[x_1 \ \cdots \ x_n]$都是向量。这时，$\mathbf{Y}$对$\mathbf{X}$的导数不再是一个标量，而是由偏导数构成的雅克比矩阵$J$（Jacobian matrix）：

$$
J = \left[\frac{\partial \mathbf{Y}}{\partial x_1}, \cdots, \frac{\partial \mathbf{Y}}{\partial x_n} \right] = \begin{bmatrix}
\frac{\partial y_1}{\partial x_1} \quad \cdots \quad \frac{\partial y_1}{\partial x_n} \\
\vdots \quad \ddots \quad \vdots \\
\frac{\partial y_m}{\partial x_1} \quad \cdots \quad \frac{\partial y_m}{\partial x_n}
\end{bmatrix}
$$

反向传播算法的反向过程中（也是反向微分的反向过程），中间层$\mathbf{Y} = G(\mathbf{X})$ 会收到后一层计算出的损失函数对当前层输出的导数：$\mathbf{v} = \frac{\partial l}{\partial \mathbf{Y}} = \left[\frac{\partial l}{\partial y_1} \ \cdots \ \frac{\partial l}{\partial y_m} \right]$，然后将这个导数继续乘以该层输出对输入的雅克比矩阵$J$向更前一层传播，这个乘法就是vector-Jacobian乘积。反向传播过程中如果直接存储雅克比矩阵，会消耗大量存储空间，取而代之，如果只存储vector-Jacobian的乘积，在减少存储的同时并不会阻碍导数的计算。因此，深度学习框架在实现自动微分时，对每个中间层存储的都是vector-Jacobian的乘积，而非雅克比矩阵。

$$
\mathbf{v} \cdot J = \begin{bmatrix}
\frac{\partial l}{\partial y_1} \cdots \frac{\partial l}{\partial y_m}
\end{bmatrix} \begin{bmatrix}
\frac{\partial y_1}{\partial x_1} \quad \cdots \quad \frac{\partial y_1}{\partial x_n} \\
\vdots \quad \ddots \quad \vdots \\
\frac{\partial y_m}{\partial x_1} \quad \cdots \quad \frac{\partial y_m}{\partial x_n}
\end{bmatrix} = \begin{bmatrix}
\frac{\partial l}{\partial x_1} \cdots \frac{\partial l}{\partial x_n}
\end{bmatrix}
$$

我们继续以图4所示的前向计算图为例，图5展示了反向操作计算图。计算图中的每个结点都是一个无状态的张量操作，结点的入边（incoming edge）表示张量操作的输入，出边表示张量操作的输出。计算图中的可导张量操作实现时都会同时注册前向计算结点和导数计算结点。前向结点接受输入计算输出，反向结点接受损失函数对当前张量操作输出的梯度$\mathbf{v}$，当前张量操作的输入和输出，计算当前张量操作每个输入的vector-Jacobian乘积。

<p align="center">
<img src="img/backward-computation-graph.png" width=30%><br>
图6. 反向计算的数据流图实例。
</p>

从图5中我们可以观察到：前向计算图和反向计算图有着完全相同的结构，区别仅仅在于数据流流动的方向相反。同时，由于梯度通常都会依赖前向计算的输入或是计算结果，反向计算图中会多出一些从前向计算图输入和输出张量指向反向计算图中导数计算结点的边。在基于计算图的深度学习框架中，利用反向微分计算梯度通常实现为数据流图上的一个优化pass，给定前向计算图，以损失函数为根节点广度优先遍历前向计算图的时，便能按照对偶结构自动生成出求导计算图。

## 3.1.5 计算图调度与执行

训练神经网络包含如下五个阶段：前向计算，反向计算，梯度截断，应用正则，以及更新可学习参数。其中，梯度截断和应用正则视用户是否配置了这两项，可能会跳过。

```python
for batch in TrainDataset:
	phrase 1: 前向计算
	phrase 2: 反向计算
	phrase 3: 梯度截断
	phrase 4: 应用正则项
	phrase 5: 更新可学习参数
```

在基于计算流图的深度学习框架中，这五个阶段统一表示为由基本算子构成的数据流图，算子是数据流图中的一个节点，由后端进行高效实现。前端用户只需要给定前向计算，框架会根据前向计算图，自动补全其余阶段，生成出完整的计算图。神经网络的训练就对应了这个计算图的执行过程。算子调度是根据计算图描述的数据依赖关系，确定算子的执行顺序，由运行时系统调度计算图中的节点到设备上执行。

### 单设备算子间调度

对单设备执行环境，制约计算图中节点调度执行的关键因素是节点之间的数据流依赖。这种情况下运行时的调度策略十分直接：初始状态下，运行时系统会将计算图中入度为0的节点加入一个FIFO（First-In-First-Out）就绪队列，然后从就绪队列中选择一个节点，分配给线程池中的一个线程执行。当这个节点执行结束后，会将其后继节点加入就绪队列，该节点被弹出。运行时系统继续处理就绪队列中的节点，直到队列为空。以TensorFlow默认调度策略为例，计算图中的节点会被分类为低代价节点（一般是仅在CPU上执行的一些拼接节点）和高代价节点（张量计算节点）。就绪队列中的一个节点被分配给线程池中的线程调度执行时，这个线程会一次执行完计算图中所有低代价节点，或在遇到高代价节点时，将这个节点派发给线程池中空闲的其他线程执行。

<p align="center">
<img src="img/op-schedule.png" width=60%><br>
图7. 简单计算图的串行执行调度。
</p>

图7是按照数据流约束执行例3.2对应的计算图的一个可能调度序列。

### 计算图切分与多设备执行

对例3.2中这样的简单神经网络，在数据流依赖的约束下只存在串行调度方案。对许多更加复杂的神经网络模型存在多分枝，典型代表如GoogLeNet[<sup>[12]</sup>](#GoogLeNet-12)，图8是构成GoogLeNet的最基本的Inception模块，这时如果后端有多个计算设备，运行时系统在调度执行计算图时，会尝试尽可能将可并行算子派发到并行设备上以提高计算资源的利用率。

<p align="center">
<img src="img/inception-parallel-scheduling.png" width=75%><br>
图8. 基本的Inception模块执行策略。
</p>

多计算设备环境下执行计算图，运行时系统需要解如何将计算图中的节点放置到不同设备上以及如何管理跨设备数据传输两个问题：

1. **计算图切分**：给定一个计算图将计算图切分后放置到多个计算设备上，每个设备拥有计算图的一部分。
2. **插入跨设备通信**：经过切分计算图会被分成若干子图，每个子图被放置在一个设备上，这时计算图中会出现一些边，它们的头和尾分别被放置在不同设备上。运行时系统会自动删除这样的边，将它们替换成一对*Send*和*Receive*算子，实现跨设备数据传输。数据传输的所有细节实现可以被*Send*和*Receive*掩盖。

图9是上面两步的示意图。

<p align="center">
<img src="img/op-schedule-multi-devices.png" width=55%><br>
图9. 计算图切分和插入跨设备数据传输。
</p>

实际上做好计算图切分映射到多设备是一个复杂的组合优化问题，需要在代价模型（cost model）的辅助下预估跨设备通信消耗的时间以及每个算子在设备上的运行时间如何随着输入输出张量大小的改变而变化，最终以数据流依赖为约束，均衡并行执行和数据通信这一对相互竞争的因素。第6章会对并行计算中的策略选择进行更详细的介绍。

## 3.1.6 小结与讨论

主流深度学习框架都采用了计算图作为神经网络计算的高层次抽象。计算图是一个有向无环图，图中的结点是一个后端支持的无状态的原子操作，结点之间的边上流动的是张量，显示地表示了结点之间的数据依赖关系。

1. 计算图用统一的方式描述出了复杂神经网络训练的全过程，使得在运行程序之前有机会对整个计算过程的数据依赖关系进行编译期分析，通过计算图化简，内存优化，预先计算算子间的静态调度策略等方式，改善运行时的性能。
1. 基于计算图描述，深度学习框架在设计上切分出了三个解耦的优化层：计算图优化，运行时调度策略，以及算子优化。当遇到新的神经网络模型结构或是训练算法时，通过以下三步进行扩展：（1）添加新的算子；（2）对算子的内核函数在不同设备，不同超参数下进行计算优化；（3）注册算子和内核函数，由运行时系统在在运行时派发到所需的实现上。
1. 在基于数据流图的深度学习框架设计之初，希望通过对三个优化层之间的解耦来加速深度学习软件栈的迭代，然而，随着神经网络模型计算规模的增加，出现了越来越多的定制化算子，多设备支持需求增加，这三个抽象层之间的抽象边界也在被频繁地打破。在后续的章节我们会进一步讨论。

# 参考文献

<div id="theano-1"></div>

1. Al-Rfou, R., Alain, G., Almahairi, A., Angermueller, C., Bahdanau, D., Ballas, N., ... & Zhang, Y. (2016). [Theano: A Python framework for fast computation of mathematical expressions](https://arxiv.org/pdf/1605.02688.pdf). arXiv e-prints, arXiv-1605.

<div id="distbelief-2"></div>

2. Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., ... & Ng, A. (2012). [Large scale distributed deep networks](https://proceedings.neurips.cc/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf). Advances in neural information processing systems, 25.

<div id="caffe-3"></div>

3. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., ... & Darrell, T. (2014, November). [Caffe: Convolutional architecture for fast feature embedding](https://arxiv.org/pdf/1408.5093.pdf?ref=https://codemonkey.link). In Proceedings of the 22nd ACM international conference on Multimedia (pp. 675-678).

<div id="mxnet-4"></div>

4. Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., ... & Zhang, Z. (2015). [Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems](https://arxiv.org/pdf/1512.01274.pdf). arXiv preprint arXiv:1512.01274.

<div id="tensorflow-5"></div>

5. Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., ... & Zheng, X. (2016). [TensorFlow: A System for Large-Scale Machine Learning](https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf). In 12th USENIX symposium on operating systems design and implementation (OSDI 16) (pp. 265-283).

<div id="dynet-6"></div>

6. Neubig, G., Dyer, C., Goldberg, Y., Matthews, A., Ammar, W., Anastasopoulos, A., ... & Yin, P. (2017). [Dynet: The dynamic neural network toolkit](https://arxiv.org/pdf/1701.03980.pdf)). arXiv preprint arXiv:1701.03980.

<div id="chainer-7"></div>

7. Tokui, S., Oono, K., Hido, S., & Clayton, J. (2015, December). [Chainer: a next-generation open source framework for deep learning](http://learningsys.org/papers/LearningSys_2015_paper_33.pdf). In Proceedings of workshop on machine learning systems (LearningSys) in the twenty-ninth annual conference on neural information processing systems (NIPS) (Vol. 5, pp. 1-6).

<div id="pytorch-8"></div>

8. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ... & Chintala, S. (2019). [Pytorch: An imperative style, high-performance deep learning library](https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf). Advances in neural information processing systems, 32.

<div id="tf-eager-9"></div>

9. Agrawal, A., Modi, A., Passos, A., Lavoie, A., Agarwal, A., Shankar, A., ... & Cai, S. (2019). [TensorFlow Eager: A multi-stage, Python-embedded DSL for machine learning](https://proceedings.mlsys.org/paper/2019/file/2a38a4a9316c49e5a833517c45d31070-Paper.pdf). Proceedings of Machine Learning and Systems, 1, 178-189.

<div id="jax-10"></div>

10. Frostig, R., Johnson, M. J., & Leary, C. (2018). [Compiling machine learning programs via high-level tracing](https://mlsys.org/Conferences/doc/2018/146.pdf). Systems for Machine Learning, 23-24.

<div id="auto-graph-11"></div>

11. Moldovan, D., Decker, J. M., Wang, F., Johnson, A. A., Lee, B. K., Nado, Z., ... & Wiltschko, A. B. (2018). [AutoGraph: Imperative-style Coding with Graph-based Performance](https://arxiv.org/pdf/1810.08061.pdf).(oct 2018). arXiv preprint arXiv:1810.08061.

<div id="GoogLeNet-12"></div>

12. Szegedy, Christian, et al. "[Going deeper with convolutions](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.