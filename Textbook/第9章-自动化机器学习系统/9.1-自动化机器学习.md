<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 9.1 自动化机器学习

- [9.1 自动化机器学习](#91-自动化机器学习)
  - [9.1.1 超参数搜索](#911-超参数搜索)
  - [9.1.2 神经网络结构搜索](#912-神经网络结构搜索)
  - [9.1.3 自动特征工程](#913-自动特征工程)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)


## 9.1.1 超参数搜索

（超参数搜索的应用）

超参数搜索是指在每个超参数可行域内寻找最优超参数取值的过程。超参数广泛存在于机器学习模型中，例如，学习率(learning rate)，批大小(batch size)。设计机器学习或者深度学习模型被调侃的称之为“炼丹”，也说明超参数在模型中的重要性，以及超参数搜索在整个模型设计过程中所占较高的比重。传统的方式是领域专家根据自己的经验设置超参数的取值然后验证该组取值的表现(performance)，根据对效果的分析再结合自己的专家经验来设置新的超参数取值，然后验证其表现。重复这样的过程直到找到满足需求的超参数取值。这种手动超参数调优的过程通常是较为枯燥的，而且很多时候即使是领域专家，对于一个超参数具体取什么值效果最好也是拿捏不定，更多的是根据自己的专家经验确定超参数取值的大致范围。因此，自动超参数搜索应运而生，帮助模型开发者减少模型设计过程中繁杂的调优任务，使模型开发者更专注于模型结构和组件的设计。

另外，超参数不仅仅存在于机器学习模型中，它广泛存在于计算机系统的各个部件中，甚至整个工业生产的各个环节。例如，数据库系统中也存在大量超参数（或称之为配置参数），像缓存(cache)的大小，缓存替换算法的选择。例如，食品工程中一个食品的原料添加量。在这些场景中，自动超参数搜索都可以扮演重要的角色，使整个过程更加自动化。

（超参数搜索的形式化搜索目标和架构）

$$\underset{\theta_{1...n}}{argmax} f_{perf}(\theta_1,\theta_2, ...,\theta_n)$$

超参数搜索的形式化搜索目标可以上面的公式统一表达。其中，$\theta_i$指代的不同的超参数。对于不同的场景和任务，$f_{perf}$这一优化的目标函数的定义也不相同。例如，对于机器学习模型，优化的目标函数通常是模型的验证精度(validation accuracy)。

整个超参数搜索的优化过程如下图所示。自动超参数搜索算法是搜索过程的核心，超参数的取值由该算法产生一组或者多组超参数取值。每一组超参数作为一个试验(Trial)，通过合适的方式（如模型训练）获得该组超参数的表现评估。表现评估会返回给超参数搜索算法。超参数搜索算法通常会依据收到的表现评估来决定下一组或多组希望尝试的超参数取值。循环这个过程直到获得满足需求的超参数取值。

<center> <img src="./img/9-1-1-hpo.png"/></center>
<center>图9-1-1. 超参数搜索流程</center>

（超参数搜索算法）

超参数搜索算法有很多种，传统的通常可以分为三类：暴力搜索(brute-force)算法，启发式(heuristic)算法，和基于模型的(model-based)算法.

暴力搜索算法包括随机搜索算法(random search)和网格搜索算法(grid search)。随机搜索算法是指随机地在每个超参数的可行域中采样出一个取值，从而得到的一组超参数取值。有时用户会根据自己的先验知识指定超参数的分布（如均匀分布，高斯分布），超参数取值的随机采样会依据这个分布进行。网格搜索算法是指对于每个超参数，在其可行域范围内依次遍历尝试其取值。通常网格搜索被用于处理具有离散空间的超参数，对网格搜索做合理的调整也可以使其能够处理连续空间的超参数。

常用的启发式算法包括遗传算法(evolutionary algorithm)，模拟退火算法(simulated annealing)。遗传算法是指维护一个种群，其中每个个体是一组超参数取值，根据这些个体的表现进行变异和淘汰，比如改变一个表现较好的个体的一个超参数取值来生成一个新的个体。具体变异和淘汰的方式有很多种，这里不展开介绍。在机器学习和深度学习领域，遗传算法通常会有较好的表现，特别是在搜索空间较大的情况下。模拟退火算法的整个搜索过程和遗传算法类似。它的初始状态可以是一组或者多组超参数取值，由一个产生函数基于当前超参数取值生成新的超参数取值。类似于遗传算法，这个产生函数可以是改变某个超参数的取值。然后使用一种接受标准（常用的是Metropolis标准）来决定是否接受这组新的超参数取值。

基于模型的算法有很多，总结来说通常是在贝叶斯框架下。。。

（超参数搜索过程在算法层面上的加速）

超参数搜索在深度学习场景下通常是一件极为耗时的事情，因为每一个试验需要运行较长时间才能获得该组超参数的表现评估。一些算法会利用试验的运行特性，试验之间的关系，模型之间的关系，来加速超参数的搜索过程。一类是基于中间表现(intermediate performance)的超参数搜索，一类是基于迁移学习(transferrable learning)的超参数搜索。。。

## 9.1.2 神经网络结构搜索

（nas简介）

在深度学习领域，神经网络结构(neural architecture)是影响模型性能的关键因素。一方面在深度学习发展的过程中，神经网络结构在不断迭代，不断带来更高的模型精度。另一方面，针对特定的场景，神经网络结构需要做有针对性的设计和适配，（比如。。。），以达到预期的模型精度和模型的推理延迟。这个过程是对神经网络结构的设计和调优。一个高效的网络结构设计和调优过程可以分为两个相辅相成的阶段。一个阶段是由专家设计或指定一个网络结构的宏观轮廓(sketch)，另一个阶段是由自动化模块细化这个宏观轮廓生成具体可执行的神经网络结构。这种模型设计和调优过程充分发挥了两者各自的优势，专家更了解逻辑层面上哪些操作(operator)，模块(block or cell)和连接(connection)可能对当前task更有优势，而自动化过程更适合调优细节的操作放置，模块大小，连接选择等等。前者也可以称之为专家定义的搜索空间，而后者是自动的搜索算法。

（搜索空间）

神经网络结构搜索空间是专家知识的凝练，其中每一个搜索空间包含了对于某一个或者某一类任务可能表现优异的一系列网络结构。通常一个神经网络结构搜索的搜索空间包含$10^{10}$以上的不同网络结构。

目前，研究领域已经提出了很多网络结构搜索空间，包括针对计算机视觉的和自然语言处理方向的深度学习模型。一个网络结构搜索空间通常是由可选的算子(如Conv2d，DepthwiseConv)，可选的连接（如算子之间的连接方式），和可选的网络结构超参数(如算子的数量)。图9-1-2是一个神经网络搜索空间的一个例子。图中每一个节点是。。。

<center> <img src="./img/9-1-1-hpo.png"/></center>
<center>图9-1-2. 一个神经网络结构搜索空间的例子</center>

（搜索算法）

nas搜索过程

## 9.1.3 自动特征工程

...

## 小结与讨论

（自动化机器学习的局限）

## 参考文献

...