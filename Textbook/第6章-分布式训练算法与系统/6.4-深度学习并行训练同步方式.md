<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->



# 6.4 深度学习并行训练同步方式
------------------------

在多设备进行并行训练时，可以采用不同的一致性模型，对应其间不同的通信协调方式，大致可分为：同步并行、异步并行、半同步并行。

## 6.4.1 同步并行

<center><img src="./img/image22.png" width="600" height="" /></center>
<center>图: 同步并行示意图 </center>
Figure: Joseph E. Gonzalez, AI-Systems Distributed Training

同步并行是采用具有同步障的通信协调并行。例如在下图中，每个worker的在进行了一些本地计算之后需要与其它worker通信协调。在通信协调的过程中，所有的worker都必须等全部worker完成了本次通信之后才能继续下一轮本地计算。阻止worker在全部通信完成之前继续下一轮计算是同步障。这样的同步方式也称BSP，其优点是本地计算和通信同步严格顺序化，能够容易地保证并行的执行逻辑于串行相同。但完成本地计算更早的worker需要等待其它worker处理，造成了计算硬件的浪费。


## 6.4.2 异步并行

采用不含同步障的通信协调并行。相比于

<center><img src="./img/image23.png" width="600" height="" /></center>
<p style="text-align: center;">图: 异步并行示意图 (Joseph E. Gonzalez, AI-Systems Distributed Training)</p>

## 6.4.2 半同步并行

采用具有限定的宽松同步障的通信协调并行。半同步的基本思路是在严格同步和完全不受限制的异步并行之间取一个这种方案——受到限制的宽松同步。例如,
在 Stale Synchronous Parallel (SSP)中，系统跟踪各个worker的进度并维护最慢进度，通过动态限制进度推进的范围，保证最快进度和最慢进度的差距在一个预定的范围内。这个范围就称为“新旧差阈值”staleness threshold如下图所示，在新旧差阈值为3时，最快进度的worker会停下来等待最慢的worker。

<center><img src="./img/image24.png" width="600" height="" /></center>
<p style="text-align: center;"> 图: 半同步SSP示意图 [Dynamic Stale Synchronous Parallel Distributed Training for Deep Learning (ICDCS’19)]<https://ieeexplore.ieee.org/abstract/document/8885215> </p>


半同步并行通过对于更新的不一致程度的限制，以达到收敛性居于同步并行和异步并行之间的效果。除了同步时机的区别，目前并行同步方式的理论也涉及同步对象的选择上。例如相比于全局所有worker参与的同步，亦有研究只与部分worker同步的方式（Ce Zhang et.al.）。


