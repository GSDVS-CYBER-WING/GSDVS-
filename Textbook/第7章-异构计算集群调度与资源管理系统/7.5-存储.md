<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 7.5 存储

在之前的章节，我们已经介绍面向深度学习的集群管理系统的运行时与调度。本章将围绕平台中的存储，文件系统来展开。计算，存储与网络是构成平台的基本组件，在深度学习系统中，我们常常关注计算与网络，却忽视存储的重要性。本章将围绕平台中的存储内容展开。

[TOC]

# 7.5.1 沿用大数据平台存储路线

当前一部分人工智能平台工程师拥有大数据平台开发背景或衍生于大数据平台组，所以有些平台沿用大数据平台的存储策略作为初始的平台存储方案。

- Hadoop分布式文件系统 ([HDFS](https://hadoop.apache.org/hdfs/。)) 是一种分布式文件系统，最初是作为 Apache Nutch 网络搜索引擎项目的基础设施而构建的，是广泛用于大数据系统的文件系统。它与已有的分布式文件系统有很多相似之处。HDFS 具有高度容错性，旨在部署在低成本硬件上。HDFS 提供对应用程序数据的高吞吐量访问，适用于拥有大量数据集的应用程序。HDFS 放宽了一些 POSIX 要求，以支持对文件系统数据的流式访问。HDFS本身适合顺序读写，不适合随机读写，不建议对小文件访问，主要为主存访问磁盘而设计，没有针对GPU显存和主存之间的数据读写进行特定支持和优化，这些劣势会造成深度学习负载下的一些性能问题和瓶颈。
- 云平台存储，例如，微软Azure云平台中的Azure Blob，亚马逊云平台中的S3等。对于基础架构部署于公有云平台的公司，云平台提供的文件系统不失为一个较好的选择，并通过fuse或者HDFS兼容的访问接口和协议进行访问。云平台的存储提供冗余备份保证可靠性，并通过数据中心高速网络的支持，提供近似本地存储的高速存储访问带宽。但是其通常为通用场景所设计，其fuse等接口一般为外围工具而开发设计，没有对深度学习负载和特点提供定制化的优化和支持。
- [Alluxio](https://www.alluxio.io/)基于内存的存储，可以充当分布式缓存服务。业界也有一些平台公司通过Alluxio管理分布式主存并提供数据缓存和备份功能。由于GPU显存和主存最为接近，提供主存层的缓存可以大幅加速I/O。但同时我们也应该看到，平台方也需要注意持久化的支持和设计。

# 7.5.2 沿用高性能计算平台存储路线

由于深度学习平台本身硬件以GPU和InfiniBand网卡为核心硬件，其技术栈和高性能计算或超算集群高度相似，所以很自然也有很多平台团队会选择使用高性能计算平台中常用的存储方案沿用到深度学习平台中使用。以下文件系统也是通常可以选用的方案：

- 网络文件系统(network file system )简称NFS文件系统是由Sun公司研发的网络文件系统，其基本原理是将某个设备本地文件系统通过以太网的方式共享给其它计算节点使用。也就是说，计算机节点通过NFS存储的数据是通过网络存储在另外一个设备，而不是存储在本地磁盘。其比较适合在平台部署早期提供文件系统支持，方便部署，技术成熟，访问接口优化，挂载到计算节点提供给算法工程师友好的体验。不足是随着数据量的增长，难以支持更大规模的空间和访问吞吐，同时权限管理需要平台层协同设计进行管理
- [Lustre](https://www.lustre.org/)文件系统是高性能计算平台部署最为广泛的商用文件系统。Lustre 是一种并行分布式文件系统，一般用于大规模集群计算。 Lustre 这个名字是源自Linux 和集群的组合词。自 2005 年 6 月以来，Lustre 一直被前十名中的至少一半、世界上最快的 100 台超级计算机中的 60 多台使用。Lustre原生支持和利用InfiniBand高速网卡，可以利用深度学习平台中的IB网络，提供更加高效的数据访问。同时支持高性能的mmap() I/O调用，容器化的支持与数据隔离，小文件的支持等，一系列的优化使得Lustre在人工智能场景也取得了不俗的性能和用户体验。在公有云场景，亚马逊AWS也推出了Amazon FSx服务，作为一项完全托管的服务，[Amazon FSx](https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html)让用户可以更轻松地将 Lustre 用于存储速度很重要的工作负载。FSx for Lustre 消除了设置和管理 Lustre 文件系统的传统复杂性，使用户能够在几分钟内启动并运行经过测试的高性能文件系统。

# 7.5.3 面向深度学习的存储

首先我们通过一个PyTorch实例来看，在深度学习作业中是如何读取数据以及和文件系统打交道的。

```python
# 本实例抽象自https://github.com/pytorch/examples/blob/main/mnist/main.py

def train(args, model, device, train_loader, optimizer, epoch):
    ...
    # 每次从数据加载器读取一个批次的样本
    for batch_idx, (data, target) in enumerate(train_loader):
        # 如果当前device是GPU，下面代码将样本由主存传输到GPU显存
        data, target = data.to(device), target.to(device)
        ...
        output = model(data)
        # 训练部分代码
        ...  

def main():
    ... 
    # 从../data文件夹读取, /data可能是存在共享的文件系统，通过fuse挂载到本地，也可能是本地文件夹存储下载到本地磁盘的数据
    dataset1 = datasets.MNIST('../data', train=True, download=True,
                       transform=transform)
    dataset2 = datasets.MNIST('../data', train=False,
                       transform=transform)
    # 框架本身提供的数据加载器，一般可以支持并行读取等优化
    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)
    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)
    ... 
    for epoch in range(1, args.epochs + 1):
        train(args, model, device, train_loader, optimizer, epoch)
        test(model, device, test_loader)
    ... 
```

通过以上实例我们可以观察到，深度学习场景下，首先从硬件来说，内存层级以GPU显存为传统主存的地位，硬盘和GPU显存之间还有主存中转数据，与GPU显存最近的存储并不是像之前和主存交互的块存储设备。从深度学习作业访存特点是，迭代式执行不断读取一个批次(Batch)的数据，并且访存模式受每轮数据洗牌(Shuffle)的影响是随机读取。从数据结构来看，数据大部分场景下为统一格式规整的张量。同时每次读取的数据并没有像数据库或者大数据系统的针对特定列的过滤需求。从用户侧用户习惯与开发水平的现状触发，用户也更倾向于使用单机文件系统一样通过fuse方式进行数据访问。

**数据读取预估练习与思考**：我们通过如下预估实例，思考和启发读者关于针对深度学习存储的优化：
```python
# 1) 基准预估读batchsize = 1024个样本需要多久
# seconds_per_seek: 从开始读取到读到第一个字节的寻找时间
# per_sample_size: 每个训练样本的大小，单位为字节
# bus_bandwidth: 磁盘或者网络存储等读带宽
1024 (seeks) * seconds_per_seek + 1024 * per_sample_size / bus_bandwidth = per_batch_read_time

# 2) 如果我们将1024个样本变成一批次进行读取需要多久?
1 (seek) * seconds_per_seek + 1024 * per_sample_size / bus_bandwidth = per_batch_read_time

# 当前实例启发我们通过1）内存数据打包为一个张量和批处理进行性能提升 2）设计高效的文件格式减少小文件随机读写问题

# 3) 如果我们并行使用32个线程进行读取需要多久?
1 (seek) * seconds_per_seek + (1024 / 32) * per_sample_size / bus_bandwidth = per_batch_read_time

# 当前预估启发我们思考并行的数据加载器设计

# 4) 如果我们有主存缓存需要读多久？假设PCIe带宽为磁盘或者云存储带宽的K倍。
pcie_bandwidth = bus_bandwidth / k
1 (seek) * seconds_per_seek + (1024 / 32) * per_sample_size / pcie_bandwidth = per_batch_read_time

# 当前预估启发我们思考利用主存作为磁盘缓存尽可能将数据放在内存预取以及流水线机制进行性能优化

# 5) 如果我们知道需要读取的数据位置，访问主存的缓存失效率(cache miss rate)为P，那么当前的读取时间是多少？

P * (1 (seek) * seconds_per_seek + (1024 / 32) * per_sample_size / pcie_bandwidth) + (1 - P) * (1 (seek) * seconds_per_seek + (1024 / 32) * per_sample_size / bus_bandwidth) = per_batch_read_time

# 当前预估启发我们思考利用深度学习作业访存局部性进行性能优化

# 6) 其他潜在优化

6.1) per_sample_size部分，读者可以思考是否可以通过压缩，量化等技术降低数据大小进而提升？
6.2) seconds_per_seek部分，由于磁盘对顺序读取和随机读取性能不同，读者可以思考是否有更好的文件格式设计最大化顺序读最小化随机读？ 

```
以上的特点造成看似对存储优化机会不像传统的数据库或者大数据系统机会多，但是如果不设计好面向深度学习的存储，也会造成和产生系统瓶颈。

所以我们可以朝着以下几个方向，并利用已有成熟的文件系统进行设计。

- 局部性：已知访问顺序的情况下的预取(pre-fetch)策略的支持。
- 流水线：和计算形成流水线协同配合，减少I/O成为瓶颈的几率。框架原生支持跨多种数据源并能异步与并行数据加载的高性能数据加载器模块。
- 缓存：利用数据中心主存不断增长的趋势，在主存做好数据缓存和备份，例如，业界有公司利用Alluxio提供缓存功能。
- 文件格式的设计：减少小文件读写与序列化开销，例如，[TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord)等针对深度学习负载设计的文件格式。


## 参考文献
- https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html
- https://www.alluxio.io/
- https://en.wikipedia.org/wiki/Network_File_System
- https://azure.microsoft.com/en-us/services/storage/blobs/
- https://aws.amazon.com/s3/
- https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/lustre-performance-superior-to-hdfs-white-paper.pdf
- https://wiki.lustre.org/Infiniband_Configuration_Howto
- https://www.hpcadvisorycouncil.com/pdf/Lustre_Best_Practice.pdf
- https://www.sc-asia.org/2018/wp-content/uploads/2018/03/1_1700_Carlos-Thomaz.pdf