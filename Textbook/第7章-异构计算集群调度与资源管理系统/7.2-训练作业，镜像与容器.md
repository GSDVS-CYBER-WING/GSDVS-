<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->
# 7.2训练作业，镜像与容器

- [7.2训练作业，镜像与容器](#72训练作业镜像与容器)
  - [7.2.1 深度学习作业](#721-深度学习作业)
  - [7.2.2 环境依赖：镜像(Image)](#722-环境依赖镜像image)
  - [7.2.3 运行时资源隔离：容器](#723-运行时资源隔离容器)
    - [小结与讨论](#小结与讨论)
    - [GPU虚拟化拓展](#gpu虚拟化拓展)
  - [参考文献](#参考文献)
 
## 7.2.1 深度学习作业

当深度学习开发者在本地机器或者独占服务器进行模型的开发与训练时，环境问题较少，还没有暴露更多问题和挑战。例如请大家思考下面列出的几点因素：

- 独占环境，无需考虑环境，资源隔离问题。
- 环境依赖路径: 本地/anaconda3。
  - 用户通过Python层的包管理软件或环境变量配置路径，即可完成Python库这层的依赖软件的隔离。
- GPU环境依赖: 本地/usr/local/cuda。
  - 用户本地NVIDIA CUDA等底层库较为固定，也可以通过环境变量较为方便的进行切换。
- 数据路径: 本地/data
  - 用户数据直接上传到服务器本地磁盘，带宽较高。
- 直接执行启动脚本: 存储在服务器磁盘，修改，调试监控等较为方便。

```
python train.py --batch_size=256  --model_name=resnet50
```


<img src="img/2/7-2-2-submit.png" ch="300" />
<center>图7-2-1. 4块GPU的服务器</center>



当深度学习作业准备提交到平台，用户需要提供什么信息呢？我们可以参考以下实例为例，实例来源于[OpenPAI](https://github.com/microsoft/pai)的提交作业样本模板。

```
{
    "jobName": "restnet",
    "image": "example.tensorflow:stable",
    "dataDir": "/tmp/data",
    "outputDir": "/tmp/output",
    ...
    "taskRoles": [
        {
            ...
            "taskNumber": 1,
            "cpuNumber": 8,
            "memoryMB": 32768,
            "gpuNumber": 1,
            "command": "python train.py --batch_size=256 \ 
		--model_name=resnet50"
        }
    ]
}
```

从作业提交模板中我们主要关注几个方面，并思考平台需要提供怎样的支持。

- 环境依赖：
  - 问题：平台集群中的机器都是相同的操作系统与环境，如何支持用户使用不同的深度学习框架（例如，TensorFlow和PyTorch）和版本？
  - 解决方法：通过"image"填写的Docker镜像名，解决环境依赖问题。用户需要提前将打包好的依赖构建为Docker镜像，并提交到指定的镜像中心，供作业下载。 
- 数据与代码：
  - 问题：平台上一般运行的是深度学习训练作业，每个作业都需要一定的数据作为输入，同时执行相应的代码，如果将数据和代码直接上传会造成接受用户请求的服务器负载过大，同时不能复用已经上传的数据和代码。
  - 解决方法：通过"dataDir"和"outputDir"填写作业依赖的数据和输出路径。用户上传数据和代码到平台指定的文件系统中的相应路径下，未来平台将数据和代码挂载到相应的作业进程。
- 资源申请量：
  - 问题：用户可能会提交运行使用单GPU的作业，多块GPU的作业和分布式的作业，面对多样的用户需求，平台需要用户明确告知，否则容易提供过多资源浪费，或过少资源造成作业无法启动。
  - 解决方法：用户明确声明需要使用的计算资源（例如，GPU和CPU）和内存，这样让平台根据指定调度策略将匹配的空闲资源进行分配。
- 资源隔离：
  - 问题：用户作业被分配指定资源后，可能多个作业在一台服务器执行，如何保证作业之间尽量不互相干扰？
  - 解决方法：平台可以通过[容器](https://www.docker.com/resources/what-container)/[cgroup](https://en.wikipedia.org/wiki/Cgroups)技术，将进程进行资源限定和隔离。
- 任务部署模式：
  - 问题：对于分布式的作业，如果用户不说明，平台无法知道并行化的策略，进而无法确认需要启动的任务数量。需要用户显式的声明。
  - 解决方法：对于分布式的作业，用户要明确告知需要启动的任务数量，进而平台能够启动多个任务副本进行训练。
- 作业启动命令：
  - 问题：当平台给作业分配资源，需要用户告知作业的启动命令，进而执行相应的代码。
  - 解决方法：用户需要明确在作业中描述相应的代码启动入口命令，进而让作业代码能够启动并执行。

通过以上问题描述和解决方法，相信您已经了解在平台中执行的作业和本地执行的作业差异和需要考虑的问题，我们将在后面章节的内容中，逐步展开其中的重要技术点和原理。

当用户在平台上执行作业，第一个比较大的问题就是本地开发环境和平台集群环境差异：
- 服务器上没有预装好所需要的个性化环境?
- 不同作业需要的框架，依赖和版本不同，安装繁琐且重复？
- 部署服务器上可能会有大量重复安装的库，占用空间？
- 深度学习特有问题：深度学习作业需要安装CUDA依赖和深度学习框架等
  
所以平台朝着以下目标去选用相应的技术方案解决问题：
复用整体安装环境并创建新环境，层级构建依赖，复用每一层级的依赖。这样既保证能有个性化环境的同时，也能保证性能和资源消耗更小。

当用户在平台上执行作业，第二个比较大的问题就是用户想像独占资源那样在执行过程中不受其他作业对资源争用产生的干扰：
- 集群资源被共享，如何保证作业互相之间不干扰和多占用资源？
- 如何能够让不同作业可以运行在不同的命名空间防止冲突？
- 如何保证隔离的同时，作业启动的越快越好？
- 深度学习特有问题：GPU的core和memory如何隔离？

所以平台朝着以下目标去选用相应的技术方案解决问题：
能够尽可能细粒度的进行资源隔离，同时减少由于资源隔离(虚拟化)技术造成的新的开销。

## 7.2.2 环境依赖：镜像(Image)

目前在主流的平台系统中，通过[Docker镜像](https://docs.docker.com/get-started/overview/#docker-objects)来解决这个问题。

而Docker镜像的本质是底层通过[Union文件系统(Unionfs)](https://en.wikipedia.org/wiki/UnionFS)等机制而实现，它使得Docker能够高效地存储镜像层。
Unionfs是Linux、FreeBSD 和 NetBSD 的文件系统服务，其实现了[联合挂载(Union mount)](https://en.wikipedia.org/wiki/Union_mount)。它允许独立文件系统的文件和目录（称为分支）透明覆盖，形成一个单一的连贯文件系统。在合并的分支中具有相同路径的目录的内容将在新的虚拟文件系统内的单个合并目录中一起看到。在计算机操作系统中，联合挂载(Union mount)是一种将多个目录组合成一个似乎包含其组合内容的方法。Linux、BSD 及其几个后续版本和 Plan 9 支持联合挂载，具有相似但略有不同的行为。这样的机制使得Docker能够更加高效的存储文件和包。
Docker支持多种Unionfs。例如, AUFS, OverlayFS等。

接下来我们看一个实例去理解镜像。

首先，用户需要书写Dockerfile，然后通过Dockerfile中的命令，将构建镜像文件。未来用户可以将文件上传到指定的镜像中心(Docker Hub)，未来平台启动作业后，会下载相应镜像到指定服务器，为作业配置好相应的依赖。

下面就是一个Dockfile实例，我们可以通过其中的注释看到，其实就是在执行在服务器中的脚本命令，进行相关库和依赖的安装。

```dockerfile
# 设置镜像使用的基础镜像
FROM nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04
# 设置构建镜像时加入的参数
ARG PYTHON_VERSION=3.6
…
# 构建镜像时所执行的脚本
RUN apt-get update && apt-get install -y --no-install-recommends \
…
RUN curl -o ~/miniconda.sh 
…
 /opt/conda/bin/conda install -y -c pytorch magma-cuda100 && \
…
# 设置指令的工作目录
WORKDIR /opt/pytorch
# 复制文件到镜像中
COPY . .
…
WORKDIR /workspace
RUN chmod -R a+w .
```

经过上面的Dockfile构建后，会生成如下的镜像。我们从镜像中可以看到其中的文件就是Dockerfile中所下载和安装的文件。

<img src="img/2/7-2-4-pytorchimage.png" ch="300" />
<center>图7-2-2. PyTorch镜像</center>

## 7.2.3 运行时资源隔离：容器

通过镜像解决了环境依赖问题，那么运行时的资源隔离问题如何解决呢？目前平台一般通过容器解决，而容器解决资源隔离问题时主要通过cgroups解决资源隔离，通过namespace解决命名空间隔离。

- 容器的定义: 
  - Linux容器（LXC）是一组 1 个或多个与系统其余部分隔离的进程。Linux Containers是一种操作系统级别的虚拟化方法，用于使用单个Linux内核在控制主机上运行多个隔离的 Linux 系统（容器）。 
- 支撑技术：	
  - 控制组[cgroups](https://man7.org/linux/man-pages/man7/cgroups.7.html)(缩写自control group)：是一种Linux内核特性，它能够限制(control)，计数（accounting）和隔离(isolation)一组进程的资源（例如，CPU，内存，磁盘I/O，网络等）及使用。
  - 命名空间[namespaces](https://man7.org/linux/man-pages/man7/namespaces.7.html): 命名空间将全局系统资源包装在一个抽象中，使命名空间内的进程看起来他们拥有自己独立的全局资源实例。命名空间可以实现如，pid，net，mnt，ipc，user等的包装和抽象

如图所示，用户可以通过创建cgroups进行资源的限制，之后启动进程时，通过cgroups约束资源使用量，并在运行时进行隔离。

<img src="img/2/7-2-8-cgroups.png" ch="300" width="300" height="200"/>
<center>图7-2-3. 控制组实例</center>

如图所示，用户可以通过创建namespaces进行资源的包装，之后启动进程时，进程内只能看到命名空间内的资源，无法感知其进程外主机其他的资源。



<img src="img/2/7-2-7-namespace.png" ch="300" width="300" height="300"/>
<center>图7-2-4. 命名空间实例</center>

由于深度学习目前依赖GPU进行训练，为了让容器能支持挂载GPU，一般GPU的厂商都会提供对Docker的特定支持来提供相应的功能。例如，NVIDIA提供了针对NVDIA GPU的支持，用户可以参考官方[nvidia-docker文档](https://github.com/NVIDIA/nvidia-docker)进行环境配置。但是由于软硬件的虚拟化支持不像传统CPU充分，所以目前主流方式还是GPU为粒度的挂载和隔离，无法像传统操作系统对CPU进行时分复用隔离黑内存隔离。
当完成环境配置之后，用户可以使用挂载NVIDIA GPU的Docker容器实例。我们可以可以通过以下命令启动和实用挂载GPU的容器。

```
# 通过官方CUDA镜像测试nvidia-smi命令，并挂载所有GPU
$ docker run --gpus all nvidia/cuda:9.0-base nvidia-smi

# 启动一个可以访问GPU的容器，并挂载2块GPU
$ docker run --gpus 2 nvidia/cuda:9.0-base nvidia-smi

# 启动一个可以访问GPU的容器，并挂载1号和2号GPU
$ docker run --gpus '"device=1,2"' nvidia/cuda:9.0-base nvidia-smi
```

如果大家相对容器底层是如何构建的有更深入的了解，可以参考实例[通过Go语言从头构建容器](https://medium.com/swlh/build-containers-from-scratch-in-go-part-1-namespaces-c07d2291038b)。
通过这个实例，读者可以了解容器的底层是通过哪些调用所构建出来，同时用户可以构建自己的个性化容器。


### 小结与讨论
- 容器与镜像解决了环境依赖，资源隔离进而奠定未来调度系统多租的基石 
- 相比传统OS，在GPU技术栈还不完善的功能是？

### GPU虚拟化拓展

目前代表性的GPU虚拟化资源隔离技术有以下几种：

- API转发(Remoting)技术：
  - 包装GPU APIs作为客户前端，通过一个转发层作为后端，协调所有对GPU的访问。挑战之一在于要最小化前端和后端的通信代价，同时API转发面临着充分支持的挑战功能，由于侵入的复杂性修改客户图形软件堆栈，以及客户和主机图形之间不兼容软件栈。
  - 代表性工作：[GVirtuS](https://github.com/cjg/GVirtuS), [vCUDA](https://github.com/tkestack/vcuda-controller), [rCUDA](http://www.rcuda.net/), [qCUDA](https://github.com/coldfunction/qCUDA)。
- Direct GPU直通(pass-through)技术：
  - 在Direct GPU pass-through技术中，GPU被单个虚拟机独占且永久地直接访问，其实现的GPU资源隔离粒度为单块GPU，不支持热迁移。GPU直通是一种允许Linux内核直接将内部GPU呈现给虚拟机的技术，该技术实现了96-100%的本地性能，但GPU提供的加速不能在多个虚拟机之间共享。
  - 代表性工作：例如，NIVIDIA对公有云厂商[Amazon AWS, Microsoft Azure, Google Cloud Platform, Alibaba Cloud](https://docs.nvidia.com/grid/cloud-service-support.html)提供GPU pass-through技术等。
- [中介设备(Mediated)直通(pass-through)](https://www.usenix.org/system/files/conference/atc14/atc14-paper-tian.pdf)技术：
  - 直通传递性能关键型资源和访问，而在设备上中介代理(Mediating)特权操作，使用性能好，功能齐全，共享能力强。
  - 代表性工作：NIVIDIA对公有云厂商[Amazon AWS, Microsoft Azure, Google Cloud Platform, Alibaba Cloud](https://docs.nvidia.com/grid/cloud-service-support.html)提供的vGPU技术. 
- 设备(Device)模拟(emulation)技术：
  - GPU架构非常复杂，变化很快，它们的内部细节通常是保密的。完全虚拟化新一代的GPU通常是不可行，只有旧的和更简单的一代才容易通过emulation的方式虚拟化。并且其实现非常复杂性能极低，所以不符合今天的要求的需求。
  - 代表性工作：[VMware SVGA 3D software renderer](https://techzone.vmware.com/resource/deploying-hardware-accelerated-graphics-vmware-horizon-7), [VirtualBox VMSVGA graphics controller](https://www.virtualbox.org/manual/)等。

由于目前一般训练平台中部署的大多数GPU为NVIDIA GPU，其提供的最新的代表性GPU资源隔离技术有以下几种：
- [NVIDIA MULTI-INSTANCE GPU](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)：其在最新的A系列GPU中才能支持。多实例 GPU (MIG) 可扩展每个 NVIDIA A100 GPU的共享能力和利用率。MIG可将 A100 GPU 划分为多达七个实例，每个实例均与各自的高带宽显存、缓存和计算核心完全隔离。
- [NVIDIA MULTI-PROCESS SERVICE](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)：多进程服务(MPS)是CUDA应用程序编程接口(API)的一种可选的二进制兼容实现。MPS运行时架构旨在透明地支持多进程CUDA应用程序，通常是MPI作业，在最新的NVIDIA(基于开普勒的)gpu上利用[Hyper-Q](https://developer.download.nvidia.com/compute/DevZone/C/html_x64/6_Advanced/simpleHyperQ/doc/HyperQ.pdf)功能。Hyper-Q允许CUDA内核在同一GPU上并发处理，当GPU计算能力未被单个应用进程充分利用时，这可以提高性能。不过研究工作[Gandiva](https://www.usenix.org/system/files/osdi18-xiao.pdf)发现，MPS会导致P40/P100的巨大开销gpu。然而，[V100](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf) 中对MPS的硬件支持建议使用MPS可能能够在V100减少相应开销并提升利用率。


## 参考文献 

- https://man7.org/linux/man-pages/man7/cgroups.7.html
- https://man7.org/linux/man-pages/man7/namespaces.7.html
- https://www.docker.com/
- https://docs.docker.com/engine/reference/commandline/image/
- https://hub.docker.com/
- https://www.docker.com/resources/what-container
- https://github.com/NVIDIA/nvidia-docker 
- Use the AUFS storage driver
- Making Containers More Isolated: An Overview of Sandboxed Container Technologies
- Jörg Thalheim, Pramod Bhatotia, Pedro Fonseca, and Baris Kasikci. 2018. CNTR: lightweight OS containers. In Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference (USENIX ATC '18). USENIX Association, USA, 199–212.
- Edward Oakes, Leon Yang, Dennis Zhou, Kevin Houck, Tyler Harter, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau. 2018. SOCK: rapid task provisioning with serverless-optimized containers. In Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference (USENIX ATC '18). USENIX Association, USA, 57–69.
- https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf
- https://www.nvidia.com/en-us/technologies/multi-instance-gpu/
- http://www.rcuda.net/
- https://www.slideshare.net/knoldus/union-filesystem-a-building-blocks-of-a-container
- https://www.nvidia.com/en-us/technologies/multi-instance-gpu/
- https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf
- https://en.wikipedia.org/wiki/GPU_virtualization
- https://slidetodoc.com/gpu-virtualization-on-vmwares-hosted-io-architecture-micah/
- https://www.usenix.org/system/files/conference/atc14/atc14-paper-tian.pdf
