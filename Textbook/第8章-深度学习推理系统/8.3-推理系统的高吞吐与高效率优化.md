<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->


# 8.3 推理系统的高吞吐与高效率优化

- [8.3 推理系统的高吞吐与高效率优化](#83-推理系统的高吞吐与高效率优化)
  - [8.3.1 加速器并行](#831-加速器并行)
  - [8.3.2 动态批尺寸](#832-动态批尺寸)
  - [8.3.3 多模型装箱](#833-多模型装箱)
  - [参考文献](#参考文献)

- 需要高吞吐的目的
  - 突发的请求数量暴增
  - 不断扩展的用户和设备
- 达到高吞吐的策略：
  - 利用加速器并行
    - 批处理请求
    - 利用优化的BLAS矩阵运算库, SIMD指令和GPU等加速器
- 自适应批尺寸(Batch Size)
- 多模型装箱使用加速器
- 容器扩展副本(Replica)部署

<center> <img src="./img/3/8-3-1-throughput.png" width="400" height="200" /></center>
<center>图8-3-1. Nvidia深度学习性能测试</center>

## 8.3.1 加速器并行

加速器的低效率使用

- 由于等待批处理请求，可能造成GPU空闲

<center> <img src="./img/3/8-3-2-lowutil.png" width="400" height="180" /></center>
<center>图8-3-2. 加速器的低效率使用</center>


单加速器运行多模型

- 时分复用策略，将等待时间给其他模型进行执行
- 同时可以动态调整Batch Size减少空闲时间

<center> <img src="./img/3/8-3-3-multimodel.png" width="400" height="120" /></center>
<center>图8-3-2. 加速器的低效率使用</center>

## 8.3.2 动态批尺寸

提升批处理(Batch Size)可以提升吞吐量

- 对于高请求数量和频率的场景
  - 通过大的批处理(Batch Size)可以提升吞吐
  - 但是需要满足一定的延迟约束

定义优化问题

$$max_{batch\_size}\{Throughput(batch\_size)\}$$

约束

$$latency(batch\_size) + overhead(batch\_size) \leq latency\_SLA $$

动态批处理尺寸(Batch Size)

- Additive Increase Multiplicative Decrease (AIMD) 策略
- 加性增加(Addictive Increase): 
  - 将批次大小累加增加固定数量，直到处理批次的延迟超过目标延迟为止
- 乘性减少(Multiplicative Decrease): 
  - 当达到后，执行一个小的乘法回退，将批次大小减少了10％
  - 因为最佳批次大小不会大幅波动，所以使用的退避常数要比其他AIMD方案小得多


## 8.3.3 多模型装箱


<center> <img src="./img/3/8-3-4-fragments.png" width="400" height="180" /></center>
<center>图8-3-3. 空闲GPU资源</center>

加速器的低效率使用

在延迟SLA约束下，模型在指定的GPU下按最大吞吐量进行分配，但是可能仍有空闲资源

装箱

Best-fit策略装箱(bin-packing)碎片化的模型请求

<center> <img src="./img/3/8-3-5-packing.png" width="400" height="300" /></center>
<center>图8-3-3. 空闲GPU资源</center>


当前吞吐量和效率的优化策略是否会对延迟产生影响？
设计其他策略进行吞吐量或使用效率的优化？

## 参考文献

- [Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications](https://arxiv.org/abs/1811.09886)
- [Clipper: A Low-Latency Online Prediction Serving System](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [TFX: A TensorFlow-Based Production-Scale Machine Learning Platform](https://research.google/pubs/pub46484/)
- [TensorFlow-Serving: Flexible, High-Performance ML Serving](https://arxiv.org/abs/1712.06139)
- [Optimal Aggregation Policy for Reducing Tail Latency of Web Search](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
- [A Survey of Model Compression and Acceleration for Deep Neural Networks](https://arxiv.org/abs/1710.09282)
- [CSE 599W: System for ML - Model Serving](https://dlsys.cs.washington.edu/)
- https://developer.nvidia.com/deep-learning-performance-training-inference 
- [DEEP COMPRESSION:   COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING](https://arxiv.org/abs/1510.00149) 
- [Learning both Weights and Connections for Efficient Neural Networks](https://arxiv.org/abs/1506.02626)
- [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)
- [Halide: A Language and Compiler for Optimizing Parallelism,Locality, and Recomputation in Image Processing Pipelines](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
- [TVM: An Automated End-to-End Optimizing Compiler for Deep Learning](https://arxiv.org/abs/1802.04799)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)


