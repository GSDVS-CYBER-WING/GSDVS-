<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 8.1 推理系统简介

- [8.1 推理系统简介](#81-推理系统简介)
  - [8.1.1 对比推理与训练过程](#811-对比推理与训练过程)
  - [8.1.2 推理系统的优化目标与约束](#812-推理系统的优化目标与约束)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)
## 8.1.1 对比推理与训练过程

<center> <img src="./img/1/8-1-1-app.png" width="500" height="180" /></center>
<center>图8-1-1. 典型深度学习推理应用</center>

深度学习模型的生命周期

<center> <img src="./img/1/8-1-2-lifecycle.png" width="500" height="200" /></center>
<center>图8-1-2. 深度学习模型的生命周期</center>

训练阶段:
- 数据处理
- 模型训练

推理阶段:
- 部署
- 推理

<center> <img src="./img/1/8-1-3-traininfer.png" width="500" height="300" /></center>
<center>图8-1-3. 模型训练与推理阶段</center>


推理相比训练的新特点与挑战

- 模型被部署为长期运行的服务
  - 服务有明确对请求的低延迟高吞吐需求
- 推理有更苛刻的资源约束
  - 更小的内存，更低的功耗等
- 推理不需要反向传播梯度下降
  - 可以牺牲一定的数据精度
- 部署的设备型号更加多样
  - 需要定制化的优化


模型部署与推理实例

```
# Convert the Tensorflow or other framework model to Serving System model format (UFF).
…
uff_model = uff.from_tensorflow(tf_model, OUTPUT_NAMES)
# Import the UFF model to TensorRT and build an engine.
…
engine = trt.utils.uff_to_trt_engine(G_LOGGER, uff_model, parser, 1, 1 << 20)
# Get the test image. 
…
img = Image.open(path)
…
# Create the context for the engine
context = engine.create_execution_context()
...
# Copy input to device and execute model
… 
bindings = [int(d_input), int(d_output)]
…
cuda.memcpy_htod_async(d_input, img, stream)
context.enqueue(1, bindings, stream.handle, None)
# Last, get the prediction and transfer prediction back
…
cuda.memcpy_dtoh_async(output, d_output, stream)
…
print("Prediction: ", LABELS[np.argmax(output)])
>>> Prediction:  n01608432 kite # The result is right due to imagenet_1000.txt does not include 'seagull’.
```

<center> <img src="./img/1/8-1-4-servingsys.png" width="500" height="200" /></center>
<center>图8-1-4. 推理服务系统</center>


## 8.1.2 推理系统的优化目标与约束

<center> <img src="./img/1/8-1-5-recommendsys.png" width="130" height="200" /></center>
<center>图8-1-5. 推荐系统</center>


在线推荐系统的服务需求

例如某在线新闻APP公司希望部署内容个性化推荐服务并期望该服务能满足以下需求：

- 低延迟：
  - 互联网上推荐文章延迟（<100毫秒）
- 高吞吐：
  - 突发新闻驱动的暴增人群的吞吐量需求
- 扩展性：
  - 扩展到不断增长的庞大的用户群体
- 准确度：
  - 随着新闻和读者兴趣的变化提供准确的预测 

推理系统部署灵活性需求

<center> <img src="./img/1/8-1-6-flexibility.png" width="430" height="200" /></center>
<center>图8-1-6. 推理系统部署需要支持多种框架和硬件</center>

机器学习服务的部署，优化和维护困难且容易出错
- 框架多样：
  - 大多数框架都是为训练设计和优化	
  - 开发人员需要将必要的软件组件拼凑在一起
  - 跨多个不断发展的框架集成和推理需求
- 硬件多样：
  - 多种部署硬件的支持

设计推理系统的优化目标

- 延迟(Latency):
  - 满足服务等级协议的延迟
- 吞吐量(Throughputs):
  - 暴增负载的吞吐量需求
- 效率(Efficiency): 
  - 高效率，低功耗使用GPU, CPU
- 灵活性(Flexibility): 
  - 支持多种框架, 提供构建不同应用的灵活性
- 扩展性(Scalability):
  - 扩展支持不断增长的用户或设备

推理系统的约束

<center> <img src="./img/1/8-1-7-constraint.png" width="300" height="200" /></center>
<center>图8-1-6. 推理系统部署需要支持多种框架和硬件</center>

- SLA对延迟的约束
- 资源约束
  - 设备端电池约束
  - 设备与服务端内存约束
  - 云端资源的预算约束
  - …
- 准确度(Accuracy)约束
  - 使用近似模型产生的一些误差可以接受

 ## 小结与讨论

深度学习推理系统设计需要考虑多目标和约束
推理系统相比传统服务系统有哪些新的挑战？
云和端的服务系统有何不同的侧重和挑战？

## 参考文献

- [Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications](https://arxiv.org/abs/1811.09886)
- [Clipper: A Low-Latency Online Prediction Serving System](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [TFX: A TensorFlow-Based Production-Scale Machine Learning Platform](https://research.google/pubs/pub46484/)
- [TensorFlow-Serving: Flexible, High-Performance ML Serving](https://arxiv.org/abs/1712.06139)
- [Optimal Aggregation Policy for Reducing Tail Latency of Web Search](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
- [A Survey of Model Compression and Acceleration for Deep Neural Networks](https://arxiv.org/abs/1710.09282)
- [CSE 599W: System for ML - Model Serving](https://dlsys.cs.washington.edu/)
- https://developer.nvidia.com/deep-learning-performance-training-inference 
- [DEEP COMPRESSION:   COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING](https://arxiv.org/abs/1510.00149) 
- [Learning both Weights and Connections for Efficient Neural Networks](https://arxiv.org/abs/1506.02626)
- [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)
- [Halide: A Language and Compiler for Optimizing Parallelism,Locality, and Recomputation in Image Processing Pipelines](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
- [TVM: An Automated End-to-End Optimizing Compiler for Deep Learning](https://arxiv.org/abs/1802.04799)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)