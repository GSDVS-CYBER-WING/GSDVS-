<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 8.5 MLOps

MLOps 是一种用于人工智能(包含机器学习与深度学习)工程的方法，它将机器学习（例如，模型的训练与推理）开发与机器学习系统（深度学习框架，自动化机器学习系统）统一起来操作与维护（Ops部分）。MLOps的过程希望标准化和自动化机器学习全生命周期的关键步骤。MLOps 提供了一套标准化的流程和工具，用于构建、部署、快速可靠地运行机器学习全流程和机器学习系统。相比于传统的DevOps不同之处在于，当用户部署Web服务时，用户关心的是每秒查询数(QPS)、负载平衡(Load Balance)等。在部署机器学习模型时，用户还需要关注数据的变化、模型的变化等。这些是MLOps所要解决的新的挑战。 

[TOC]

## 8.5.1 MLOps的生命周期

推理系统本身就像传统Web服务发布代码包一样，需要定期发布模型，提供新功能或更好的效果。
这些挑战类似于应用程序开发团队在创建和管理应用程序时面临的挑战。借鉴传统的软件工程最佳实践，业界使用了DevOps，这是管理应用程序开发周期操作的行业标准。为了应对深度学习全生命周期的挑战，组织需要一种将 DevOps 的敏捷性带入ML 生命周期的方法，业界称这种方法为[MLOps](https://docs.microsoft.com/en-us/learn/modules/start-ml-lifecycle-mlops/2-mlops-introduction)

<center> <img src="./img/4/8-4-5-mlops.png" width="500" height="300" /></center>
<center>图8-4-2. 模型构建与部署 </center>

接下来，我们以其中的代表性的问题进行介绍。

## 8.5.2 机器学习与深度学习模型管理

在MLOps中，其中较为关键的是模型的版本管理：线上发布，回滚等策略。因为近些年，软件逐渐由客户端软件演化为在线部署的服务，而模型最终也是部署于一定的软件系统中，所以越来越多的模型部署于在线服务中(online service)。在在线服务中， 每隔一段时间训练出的新版本模型替换线上模型，但是可能存在缺陷，另外如果新版本模型发现缺陷需要回滚。

## 8.5.3 线上发布与回滚策略

模型生命周期管理的策略实例有：[金丝雀(canary)策略，回滚(rollback)策略](https://arxiv.org/pdf/1712.06139.pdf)。
- 金丝雀策略
  - 当获得一个新训练的模型版本时，当前服务的模型成为第二新版本时，用户可以选择同时保持这两个版本
  - 将所有推理请求流量发送到当前两个版本，比较它们的效果
  - 一旦对最新版本达标，用户就可以切换到仅使用最新版本
  - 该策略需要更多的高峰资源，避免将用户暴露于缺陷模型
- 回滚策略
  - 如果在当前的主要服务版本上检测到缺陷，则用户可以请求切换到特定的较旧版本
  - 卸载和装载的顺序应该是可配置的
  - 当问题解决并且获取到新的安全版本模型时，从而结束回滚

## 8.6.4 代表性MLOps工具与服务

- [MLflow](https://mlflow.org/)：是由UCB开源的MLOps工具，其提供标准化的API，Python社区友好，并让MLOps概念深入人心。
- [Amazon SageMaker MLOps](https://aws.amazon.com/sagemaker/mlops/)：此服务为亚马逊云服务机器学习平台SageMaker提供的MLOps服务，其提供全流程的机器学习声明周期管理，并提供特色的模型调试与诊断功能，将专家级调试经验自动化。
- [Azure Machine Learning](https://azure.microsoft.com/en-us/services/machine-learning/mlops/)：微软Azure Machine Learning服务也提供了MLOps的功能，并持续构建全套解决方法，与微软云集成度最佳。
- [Google Cloud MLOps](https://cloud.google.com/resources/mlops-whitepaper)：Google Cloud的MLOps功能不仅较早提出，同时向业界发布白皮书，布道相关概念与技术。

## 参考文献

- [Park, Jongsoo et al. “Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications.” ArXiv abs/1811.09886 (2018): n. pag.](https://arxiv.org/abs/1811.09886)
- [Crankshaw, Daniel et al. “Clipper: A Low-Latency Online Prediction Serving System.” NSDI (2017).](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17). Association for Computing Machinery, New York, NY, USA, 1387–1395. DOI:https://doi.org/10.1145/3097983.3098021](https://research.google/pubs/pub46484/)
- [Olston, Christopher et al. “TensorFlow-Serving: Flexible, High-Performance ML Serving.” ArXiv abs/1712.06139 (2017): n. pag.](https://arxiv.org/abs/1712.06139)
- https://www.microsoft.com/en-us/research/project/ai-tooling-and-mlops/
- ttps://docs.microsoft.com/en-us/learn/modules/start-ml-lifecycle-mlops/2-mlops-introduction
- https://aws.amazon.com/sagemaker/mlops/
- https://azure.microsoft.com/en-us/services/machine-learning/mlops/
- https://cloud.google.com/resources/mlops-whitepaper