<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->
# 7.2训练作业，镜像与容器

集群管理系统对环境依赖和资源隔离问题，需要通过镜像和运行时等机制解决。本章将围绕集群管理系统上运行作业的运行时进行介绍。

- [7.2训练作业，镜像与容器](#72训练作业镜像与容器)
  - [7.2.1 深度学习作业](#721-深度学习作业)
  - [7.2.2 环境依赖：镜像(Image)](#722-环境依赖镜像image)
  - [7.2.3 运行时资源隔离：容器](#723-运行时资源隔离容器)
  - [7.2.4 从操作系统视角看GPU技术栈](#724-从操作系统视角看gpu技术栈)
    - [**GPU技术栈的操作系统抽象**](#gpu技术栈的操作系统抽象)
    - [**观测深度学习作业系统调用(System Call)**](#观测深度学习作业系统调用system-call)
    - [**GPU虚拟化**](#gpu虚拟化)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

 
## 7.2.1 深度学习作业

当深度学习开发者在本地机器或者独占服务器进行模型的开发与训练时，环境问题较少，还没有暴露更多问题和挑战。例如请大家思考下面列出的几点因素：

- 独占环境，无需考虑环境，资源隔离问题。
- 环境依赖路径: 本地/anaconda3。
  - 用户通过Python层的包管理软件或环境变量配置路径，即可完成Python库这层的依赖软件的隔离。
- GPU环境依赖: 本地/usr/local/cuda。
  - 用户本地NVIDIA CUDA等底层库较为固定，也可以通过环境变量较为方便的进行切换。
- 数据路径: 本地/data
  - 用户数据直接上传到服务器本地磁盘，带宽较高。
- 直接执行启动脚本: 存储在服务器磁盘，修改，调试监控等较为方便。

```
python train.py --batch_size=256  --model_name=resnet50
```

<style>table{margin: auto;}</style>
<center><img src="img/2/7-2-2-submit.png" ch="300" width="1000" height="380"></center>
<center>图7-2-1. 4块GPU的服务器</center>



当深度学习作业准备提交到平台，用户需要提供什么信息呢？我们可以参考以下实例为例，实例来源于[OpenPAI](https://github.com/microsoft/pai)的提交作业样本模板。

```
{
    "jobName": "restnet",
    "image": "example.tensorflow:stable",
    "dataDir": "/tmp/data",
    "outputDir": "/tmp/output",
    ...
    "taskRoles": [
        {
            ...
            "taskNumber": 1,
            "cpuNumber": 8,
            "memoryMB": 32768,
            "gpuNumber": 1,
            "command": "python train.py --batch_size=256 \ 
		--model_name=resnet50"
        }
    ]
}
```

从作业提交模板中我们主要关注几个方面，并思考平台需要提供怎样的支持。

- 环境依赖：
  - 问题：平台集群中的机器都是相同的操作系统与环境，如何支持用户使用不同的深度学习框架（例如，TensorFlow和PyTorch）和版本？
  - 解决方法：通过"image"填写的Docker镜像名，解决环境依赖问题。用户需要提前将打包好的依赖构建为Docker镜像，并提交到指定的镜像中心，供作业下载。 
- 数据与代码：
  - 问题：平台上一般运行的是深度学习训练作业，每个作业都需要一定的数据作为输入，同时执行相应的代码，如果将数据和代码直接上传会造成接受用户请求的服务器负载过大，同时不能复用已经上传的数据和代码。
  - 解决方法：通过"dataDir"和"outputDir"填写作业依赖的数据和输出路径。用户上传数据和代码到平台指定的文件系统中的相应路径下，未来平台将数据和代码挂载到相应的作业进程。
- 资源申请量：
  - 问题：用户可能会提交运行使用单GPU的作业，多块GPU的作业和分布式的作业，面对多样的用户需求，平台需要用户明确告知，否则容易提供过多资源浪费，或过少资源造成作业无法启动。
  - 解决方法：用户明确声明需要使用的计算资源（例如，GPU和CPU）和内存，这样让平台根据指定调度策略将匹配的空闲资源进行分配。
- 资源隔离：
  - 问题：用户作业被分配指定资源后，可能多个作业在一台服务器执行，如何保证作业之间尽量不互相干扰？
  - 解决方法：平台可以通过[容器](https://www.docker.com/resources/what-container)/[cgroup](https://en.wikipedia.org/wiki/Cgroups)技术，将进程进行资源限定和隔离。
- 任务部署模式：
  - 问题：对于分布式的作业，如果用户不说明，平台无法知道并行化的策略，进而无法确认需要启动的任务数量。需要用户显式的声明。
  - 解决方法：对于分布式的作业，用户要明确告知需要启动的任务数量，进而平台能够启动多个任务副本进行训练。
- 作业启动命令：
  - 问题：当平台给作业分配资源，需要用户告知作业的启动命令，进而执行相应的代码。
  - 解决方法：用户需要明确在作业中描述相应的代码启动入口命令，进而让作业代码能够启动并执行。

通过以上问题描述和解决方法，相信您已经了解在平台中执行的作业和本地执行的作业差异和需要考虑的问题，我们将在后面章节的内容中，逐步展开其中的重要技术点和原理。

当用户在平台上执行作业，第一个比较大的问题就是本地开发环境和平台集群环境差异：
- 服务器上没有预装好所需要的个性化环境?
- 不同作业需要的框架，依赖和版本不同，安装繁琐且重复？
- 部署服务器上可能会有大量重复安装的库，占用空间？
- 深度学习特有问题：深度学习作业需要安装CUDA依赖和深度学习框架等
  
所以平台朝着以下目标去选用相应的技术方案解决问题：
复用整体安装环境并创建新环境，层级构建依赖，复用每一层级的依赖。这样既保证能有个性化环境的同时，也能保证性能和资源消耗更小。

当用户在平台上执行作业，第二个比较大的问题就是用户想像独占资源那样在执行过程中不受其他作业对资源争用产生的干扰：
- 集群资源被共享，如何保证作业互相之间不干扰和多占用资源？
- 如何能够让不同作业可以运行在不同的命名空间防止冲突？
- 如何保证隔离的同时，作业启动的越快越好？
- 深度学习特有问题：GPU的核和内存如何隔离？

所以平台朝着以下目标去选用相应的技术方案解决问题：
能够尽可能细粒度的进行资源隔离，同时减少由于资源隔离(虚拟化)技术造成的新的开销。

## 7.2.2 环境依赖：镜像(Image)

目前在主流的平台系统中，通过[Docker镜像](https://docs.docker.com/get-started/overview/#docker-objects)来解决这个问题。

而Docker镜像的本质是底层通过[Union文件系统(Unionfs)](https://en.wikipedia.org/wiki/UnionFS)等机制而实现，它使得Docker能够高效地存储镜像层。
Unionfs是Linux、FreeBSD 和 NetBSD 的文件系统服务，其实现了[联合挂载(Union Mount)](https://en.wikipedia.org/wiki/Union_mount)。它允许独立文件系统的文件和目录（称为分支）透明覆盖，形成一个单一的连贯文件系统。在合并的分支中具有相同路径的目录的内容将在新的虚拟文件系统内的单个合并目录中一起看到。在计算机操作系统中，联合挂载(Union Mount)是一种将多个目录组合成一个似乎包含其组合内容的方法。Linux、BSD 及其几个后续版本和 Plan 9 支持联合挂载，具有相似但略有不同的行为。这样的机制使得Docker能够更加高效的存储文件和包。
Docker支持多种Unionfs。例如, AUFS, OverlayFS等。

接下来我们看一个实例去理解镜像。

首先，用户需要书写Dockerfile，然后通过Dockerfile中的命令，将构建镜像文件。未来用户可以将文件上传到指定的镜像中心(Docker Hub)，未来平台启动作业后，会下载相应镜像到指定服务器，为作业配置好相应的依赖。

下面就是一个Dockfile实例，我们可以通过其中的注释看到，其实就是在执行在服务器中的脚本命令，进行相关库和依赖的安装。

```dockerfile
# 设置镜像使用的基础镜像
FROM nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04
# 设置构建镜像时加入的参数
ARG PYTHON_VERSION=3.6
…
# 构建镜像时所执行的脚本
RUN apt-get update && apt-get install -y --no-install-recommends \
…
RUN curl -o ~/miniconda.sh 
…
 /opt/conda/bin/conda install -y -c pytorch magma-cuda100 && \
…
# 设置指令的工作目录
WORKDIR /opt/pytorch
# 复制文件到镜像中
COPY . .
…
WORKDIR /workspace
RUN chmod -R a+w .
```

经过上面的Dockfile构建后，会生成如下的镜像。我们从镜像中可以看到其中的文件就是Dockerfile中所下载和安装的文件。
<style>table{margin: auto;}</style>
<center><img src="img/2/7-2-4-pytorchimage.png" ch="300" /></center>
<center>图7-2-2. PyTorch镜像</center>

## 7.2.3 运行时资源隔离：容器

通过镜像解决了环境依赖问题，那么运行时的资源隔离问题如何解决呢？目前平台一般通过容器解决，而容器解决资源隔离问题时主要通过cgroups解决资源隔离，通过命名空间(Namespace)解决命名空间隔离。

- 容器的定义: 
  - Linux容器（LXC）是一组 1 个或多个与系统其余部分隔离的进程。Linux Containers是一种操作系统级别的虚拟化方法，用于使用单个Linux内核在控制主机上运行多个隔离的Linux系统（容器）。 
- 支撑技术：	
  - 控制组[Cgroups](https://man7.org/linux/man-pages/man7/cgroups.7.html)(缩写自Control Groups)：是一种Linux内核特性，它能够控制(Control)，计数（Accounting）和隔离(Isolation)一组进程的资源（例如，CPU，内存，磁盘I/O，网络等）及使用。
  - 命名空间[Namespace](https://man7.org/linux/man-pages/man7/namespaces.7.html): 命名空间将全局系统资源包装在一个抽象中，使命名空间内的进程看起来他们拥有自己独立的全局资源实例。命名空间可以实现如，pid，net，mnt，ipc，user等的包装和抽象

如图所示，用户可以通过创建控制组(Cgroups)进行资源的限制，之后启动进程时，通过控制组(Cgroups)约束资源使用量，并在运行时进行隔离。
<style>table{margin: auto;}</style>
<center><img src="img/2/7-2-8-cgroups.png" ch="300" width="500" height="300"/></center>
<center>图7-2-3. 控制组实例</center>

如图所示，用户可以通过创建命名空间(Namespaces)进行资源的包装，之后启动进程时，进程内只能看到命名空间内的资源，无法感知其进程外主机其他的资源。


<style>table{margin: auto;}</style>
<center><img src="img/2/7-2-7-namespace.png" ch="300" width="500" height="500"/></center>
<center>图7-2-4. 命名空间实例</center>

由于深度学习目前依赖GPU进行训练，为了让容器能支持挂载GPU，一般GPU的厂商都会提供对Docker的特定支持来提供相应的功能。例如，NVIDIA提供了针对NVDIA GPU的支持，用户可以参考官方[nvidia-docker文档](https://github.com/NVIDIA/nvidia-docker)进行环境配置。但是由于软硬件的虚拟化支持不像传统CPU充分，所以目前主流方式还是GPU为粒度的挂载和隔离，无法像传统操作系统对CPU进行时分复用隔离黑内存隔离。
当完成环境配置之后，用户可以使用挂载NVIDIA GPU的Docker容器实例。我们可以可以通过以下命令启动和实用挂载GPU的容器。

```
# 通过官方CUDA镜像测试nvidia-smi命令，并挂载所有GPU
$ docker run --gpus all nvidia/cuda:9.0-base nvidia-smi

# 启动一个可以访问GPU的容器，并挂载2块GPU
$ docker run --gpus 2 nvidia/cuda:9.0-base nvidia-smi

# 启动一个可以访问GPU的容器，并挂载1号和2号GPU
$ docker run --gpus '"device=1,2"' nvidia/cuda:9.0-base nvidia-smi
```

如果大家相对容器底层是如何构建的有更深入的了解，可以参考实例[通过Go语言从头构建容器](https://medium.com/swlh/build-containers-from-scratch-in-go-part-1-namespaces-c07d2291038b)。
通过这个实例，读者可以了解容器的底层是通过哪些调用所构建出来，同时用户可以构建自己的个性化容器。



## 7.2.4 从操作系统视角看GPU技术栈

GPU在当前计算机中被抽象为设备，操作系统通过ioctl系统调用进行设备控制。在以往的工作中，有两个方向的思路尝试去将GPU技术栈纳入到操作系统的管理中并提供多租，虚拟化等的支持。
<style>table{margin: auto;}</style>
<center> <img src="./img/2/7-2-9-osgpu.png" ch="500" width="700" height="350" /></center>
<center>图7-2-5. CPU和GPU技术栈与操作系统抽象(<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2011/05/hotos11-final.pdf">图片来源</a>)</center>

从图中可以看到，CPU程序中有大量的系统调用支持，操作系统提供对各种硬件抽象管理与用户多进程多租的支持。但是对GPU程序，当前模式更像是Client/Server抽象，GPU是设备，CPU程序提交作业到GPU并获取响应结果，但是对操作系统GPU本身是一个黑盒，只能通过[ioctl](https://man7.org/linux/man-pages/man2/ioctl.2.html)进行有限的交互与控制。

### **GPU技术栈的操作系统抽象**

1. 思路1：操作系统纳入GPU。让GPU能被ioctl以外的系统调用所管理。
   1. 当前思路依赖设备供应商在驱动层暴露更多信息被操作系统管理，但是当前一些厂商（例如，英伟达）这部分为闭源，且只能通过ioctl控制设备，很难分析语义。2011年HotOS上，Christopher J. Rossbach等人在[Operating Systems must support GPU abstractions
](https://www.microsoft.com/en-us/research/wp-content/uploads/2011/05/hotos11-final.pdf)曾有相关工作提出在操作系统层面抽象GPU管理，但是这篇工作只提出构想，并受限于设备供应商的设备驱动的支持，从之后的发展状况来看，受限于设备供应商的配合和支持，当前道路并不没有走通。在ATC13，Konstantinos Menychtas等人提出[Enabling OS Research by Inferring Interactions in the Black-Box GPU Stack](https://www.usenix.org/system/files/conference/atc13/atc13-menychtas.pdf)当前GPU资源是黑盒，不能被操作系统所管理，所以对系统调用进行拦截和刻画抽象出程序状态机，启发后续操作系统对GPU设备的研究。之后的工作也层尝试设计从GPU调用操作系统系统调用（[ISCA18 Generic System Calls for GPUs](http://www.cs.yale.edu/homes/abhishek/jvesely-isca18.pdf)）。但是以上的工作由于实用性，部署难度和受限于厂商支持，在后续的业界平台或研究工作中没有过多的跟随工作。
2. 思路2：GPU编程接口上抽象操作系统。此类工作以设备API作为系统调用的视角抽象设备管理，在设备API层以下进行拦截或管理达到多租或虚拟化的效果。
   1. 厂商官方工作：[MPS](https://docs.nvidia.com/deploy/mps/index.html)，[Unified Memory](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)等工作是Nvidia将传统操作系统的多进程调度，虚拟内存等经典设计思路在CUDA上进行的实现，进而能够实现复用GPU，打破GPU显存瓶颈的效果。
   2. 研究工作：以[rCUDA](http://www.rcuda.net/)，[Singularity](https://arxiv.org/abs/2202.07848)为代表的工作在[Nvidia Runtime API](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html)，[Driver API](https://docs.nvidia.com/cuda/cuda-driver-api/index.html)上进行拦截，提供时分复用，内存隔离，和动态迁移的功能。
   3. 第三方公司的工作：以[OrionX猎户座](https://blog.csdn.net/m0_49711991/article/details/107979798)为代表的产品，设计类似传统操作系统的内核态抽象，在大规模集群上提供资源池化，并通过内核态抽象让系统更安全和稳定。

### **观测深度学习作业系统调用(System Call)**

[strace](https://strace.io/)
是用于 Linux 的诊断、调试和指导性用户空间实用程序。 它用于监视和篡改进程与 Linux 内核之间的交互，包括系统调用、信号传递和进程状态的变化。本小节我们可以通过strace观测深度学习程序如何在操作系统的系统调用层和GPU打交道，以另一个抽象层次和视角深入理解深度学习程序的底层原理。
1. 例如在Ubuntu操作系统中，执行一下安装命令：
```
apt-get install strace
```
2. 假设用户有一个训练程序（train.py），里面是PyTorch训练卷积神经网络。

```
strace python train.py
```
3. 观测系统调用日志。

我们可以观察主要调用的是ioctl和mmap等系统调用，但是从ioctl很难获取到可读和可解释信息。在之前ATC13研究工作[Enabling OS Research by Inferring Interactions
in the Black-Box GPU Stack](https://www.usenix.org/system/files/conference/atc13/atc13-menychtas.pdf)中也有对ioctl进行收集分析并抽象出状态机的工作。

### **GPU虚拟化**

目前代表性的GPU虚拟化资源隔离技术有以下几种：

- 应用程序编程接口远程处理(API Remoting)技术：
  - 包装GPU APIs作为客户前端，通过一个转发层作为后端，协调所有对GPU的访问。挑战之一在于要最小化前端和后端的通信代价，同时API转发面临着充分支持的挑战功能，由于侵入的复杂性修改客户图形软件堆栈，以及客户和主机图形之间不兼容软件栈。
  - 代表性工作：[GVirtuS](https://github.com/cjg/GVirtuS), [vCUDA](https://github.com/tkestack/vcuda-controller), [rCUDA](http://www.rcuda.net/), [qCUDA](https://github.com/coldfunction/qCUDA)。
- 直接GPU直通(Direct GPU Pass-Through)技术：
  - 在Direct GPU pass-through技术中，GPU被单个虚拟机独占且永久地直接访问，其实现的GPU资源隔离粒度为单块GPU，不支持热迁移。GPU直通是一种允许Linux内核直接将内部GPU呈现给虚拟机的技术，该技术实现了96-100%的本地性能，但GPU提供的加速不能在多个虚拟机之间共享。
  - 代表性工作：例如，NIVIDIA对公有云厂商[Amazon AWS, Microsoft Azure, Google Cloud Platform, Alibaba Cloud](https://docs.nvidia.com/grid/cloud-service-support.html)提供GPU pass-through技术等。
- [中介直通(Mediated Pass-Through)](https://www.usenix.org/system/files/conference/atc14/atc14-paper-tian.pdf)技术：
  - 直通传递性能关键型资源和访问，而在设备上中介代理(Mediating)特权操作，使用性能好，功能齐全，共享能力强。
  - 代表性工作：NIVIDIA对公有云厂商[Amazon AWS, Microsoft Azure, Google Cloud Platform, Alibaba Cloud](https://docs.nvidia.com/grid/cloud-service-support.html)提供的vGPU技术. 
- 设备仿真(Device Emulation)技术：
  - GPU架构非常复杂，变化很快，它们的内部细节通常是保密的。完全虚拟化新一代的GPU通常是不可行，只有旧的和更简单的一代才容易通过emulation的方式虚拟化。并且其实现非常复杂性能极低，所以不符合今天的要求的需求。
  - 代表性工作：[VMware SVGA 3D software renderer](https://techzone.vmware.com/resource/deploying-hardware-accelerated-graphics-vmware-horizon-7), [VirtualBox VMSVGA graphics controller](https://www.virtualbox.org/manual/)等。

由于目前一般训练平台中部署的大多数GPU为NVIDIA GPU，其提供的最新的代表性GPU资源隔离技术有以下几种：
- [Nvidia MULTI-INSTANCE GPU (简称MIG)](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)：其在最新的A系列GPU中才能支持。多实例 GPU (MIG) 可扩展每个NVIDIA A100 GPU的共享能力和利用率。MIG可将 A100 GPU 划分为多达七个实例，每个实例均与各自的高带宽显存、缓存和计算核心完全隔离。
- [Nvidia MULTI-PROCESS SERVICE（简称MPS）](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)：多进程服务(MPS)是CUDA应用程序编程接口(API)的一种可选的二进制兼容实现。MPS运行时架构旨在透明地支持多进程CUDA应用程序，通常是MPI作业，在最新的NVIDIA(基于开普勒的)GPU上利用[Hyper-Q](https://developer.download.nvidia.com/compute/DevZone/C/html_x64/6_Advanced/simpleHyperQ/doc/HyperQ.pdf)功能。Hyper-Q允许CUDA内核在同一GPU上并发处理，当GPU计算能力未被单个应用进程充分利用时，这可以提高性能。不过研究工作[Gandiva](https://www.usenix.org/system/files/osdi18-xiao.pdf)发现，MPS会导致P40/P100的巨大开销GPU。然而，[V100](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf) 中对MPS的硬件支持建议使用MPS可能能够在V100减少相应开销并提升利用率。


## 小结与讨论

本章我们主要介绍异构计算集群管理系统的运行时，在运行时中，资源与环境隔离是核心问题，虽然我们看到有很多前沿的公司与开源软件在视图解决，但是其底层还是基于操作系统和硬件的底层原语的支持。容器与镜像解决了环境依赖，资源隔离进而奠定未来调度系统多租的基石。

请读者思考，运行时设计的过程中，GPU面临了什么新的问题？
相比传统操作系统，在GPU技术栈还不完善的功能是？
  
## 参考文献 

- https://man7.org/linux/man-pages/man7/cgroups.7.html
- https://man7.org/linux/man-pages/man7/namespaces.7.html
- https://www.docker.com/
- https://docs.docker.com/engine/reference/commandline/image/
- https://hub.docker.com/
- https://www.docker.com/resources/what-container
- https://github.com/NVIDIA/nvidia-docker 
- Use the AUFS storage driver
- Making Containers More Isolated: An Overview of Sandboxed Container Technologies
- Jörg Thalheim, Pramod Bhatotia, Pedro Fonseca, and Baris Kasikci. 2018. CNTR: lightweight OS containers. In Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference (USENIX ATC '18). USENIX Association, USA, 199–212.
- Edward Oakes, Leon Yang, Dennis Zhou, Kevin Houck, Tyler Harter, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau. 2018. SOCK: rapid task provisioning with serverless-optimized containers. In Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference (USENIX ATC '18). USENIX Association, USA, 57–69.
- https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf
- https://www.nvidia.com/en-us/technologies/multi-instance-gpu/
- http://www.rcuda.net/
- https://www.slideshare.net/knoldus/union-filesystem-a-building-blocks-of-a-container
- https://www.nvidia.com/en-us/technologies/multi-instance-gpu/
- https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf
- https://en.wikipedia.org/wiki/GPU_virtualization
- https://slidetodoc.com/gpu-virtualization-on-vmwares-hosted-io-architecture-micah/
- https://www.usenix.org/system/files/conference/atc14/atc14-paper-tian.pdf
