<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 1.5 深度学习样例背后的系统问题

算法工程师通过Python和框架书写人工智能程序，而人工智能程序底层的系统问题被当前层的抽象隐藏，到底在每个代码部分具体底层发生了什么？有哪些有意思的系统设计问题？我们将从一个实例启发读者，并和后面各个章节构建起桥梁与练习，给读者构建除目录之外的第二个内容索引。

- [1.5 深度学习样例背后的系统问题](#15-深度学习样例背后的系统问题)
  - [1.4.1 一个深度学习样例与其中的系统问题](#141-一个深度学习样例与其中的系统问题)
  - [1.4.2 隐藏在算子(Operator)实现中的系统问题](#142-隐藏在算子operator实现中的系统问题)
  - [1.4.3 更大范围的系统问题](#143-更大范围的系统问题)
  - [小节与讨论](#小节与讨论)
  - [参考文献](#参考文献)

## 1.4.1 一个深度学习样例与其中的系统问题

下面的实例是PyTorch在MNIST数据集上训练一个卷积神经网络的实例。

```
# 本实例来源PyTorch官方实例 https://github.com/pytorch/examples/blob/main/mnist/main.py 
...
# 读者可以参考第3章理解深度学习框架的底层原理和设计
import torch
...

# 如果模型层数多，权重多到无法在单GPU显存放置，我们需要通过模型并行方式进行训练，读者可以参考第6章进行了解
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 请参考1.4.2小节，通过循环实现卷积理解卷积的执行逻辑并思考其中的潜在系统问题
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        # 我们能否调整超参数32为64？如何高效的搜索最有的配置？这些内容我们将在第9章展开介绍
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output


def train(args, model, device, train_loader, optimizer, epoch):
    # 如何进行高效的训练，运行时是如何执行的？我们将在第3章进行介绍
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        ...


def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            # 推理系统如何高效进行模型推理？我们将在第8章进行介绍
            output = model(data)
            ...


def main():
    ...
    # 当前语句决定了使用哪种加速器以及体系结构，读者可以通过第4章了解。
    device = torch.device("cuda" if use_cuda else "cpu")
    # 如果batch size过大，造成单GPU内存无法容纳模型及中间激活的张量，读者可以参考第6章进行了解如何分布式训练
    train_kwargs = {'batch_size': args.batch_size}
    test_kwargs = {'batch_size': args.test_batch_size}
    ...
    # 如何高效的进行数据读取？这些内容我们将在第7章进行介绍。
    # 如果我们训练的数据集和模型是预测系统优化配置，我们想训练的模型是优化系统配置，那么读者可以参考第13章，思考如何将AI应用到系统优化
    # 如果我们的数据集没有提前准备好，需要实时和环境交互获取，那么读者可以参考第10章进行理解
    dataset1 = datasets.MNIST('../data', train=True, download=True,
                       transform=transform)
    dataset2 = datasets.MNIST('../data', train=False,
                       transform=transform)
    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)
    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)
    model = Net().to(device)
    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)
    ... 
    for epoch in range(1, args.epochs + 1):
        train(args, model, device, train_loader, optimizer, epoch)
        # 模型如果训练完成需要部署，我们如何压缩和量化后再部署？读者可以参考第11章进行了解
        test(model, device, test_loader)
        ... 

    if args.save_model:
        torch.save(model.state_dict(), "mnist_cnn.pt")

# 如果用户提交多个这样的训练作业，系统如何调度和管理资源？读者可以参考第7章进行了解
if __name__ == '__main__':
    main()
```

## 1.4.2 隐藏在算子(Operator)实现中的系统问题

底层算子的具体实现时由其对应的矩阵运算翻译为对应的循环（目前我们简化问题，忽略stride等其他超参数影响）。

我们以卷积算子为例，卷积的计算可以表达为多层循环，我们以下面代码为例进行分析。
```
# 为简化阐述计算过程，我们简化了维度(Dimension)的形状推导(Shape Inference)。

# 批尺寸维度
for n in range(N):
  # 输出张量通道维度
  for k in range(K):
    # 输入张量通道维度
    for c in range(C):
      # 输出张量高度维度
      for h in range(H):
        # 输出张量宽度维度
        for w in range(W):
          # 卷积核高度维度
          for fh in range(FH):
            # 卷积核宽度维度
            for w in range(FW):
              # 乘加运算
              output[h,w,k] += input[h + fw, w + fh, c] * kernel[fw,fh,c,k]  
```

在这其中有很多有趣的问题问题读者可以思考与预估：
- 其中参与计算的输入，权重和输出张量能否完全放入GPU缓存(L1，L2)？如果不能放入则需要通过块(Tile)优化进行切片，这些内容将在第5章着重介绍。
- 循环执行的主要计算语句是否有局部性可以利用？空间局部性（缓存线内相邻的空间是否会被连续访问）以及时间局部性（同一块内存多久后还会被继续访问），这样我们可以通过预估后，尽可能的通过编译调度循环执行，这些内容将在第5章着重介绍。
- 如果有些权重为0是否可以不进行计算？读者可以参考第11章稀疏性(Sparsity)部分进行了解。
- 读者可以[预估](https://www.microsoft.com/en-us/research/uploads/prod/2020/09/dnnmem.pdf)各个层的输出(Output)张量，输入(Input)张量，和内核(Kernel)张量大小，进而评估是否需要多卡，内存管理策略设计，以及换入换出策略等。读者可以参考第5，8章相关内存优化与管理内容。
- 那么当算子与算子在运行时按一定调度次序执行，框架如何进行运行时管理，请读者参考第3章相关内容理解

## 1.4.3 更大范围的系统问题

- 更多的超参数组合与模型结构探索
  - 之前我们看到的实例本身是单个模型的样例，但是深度学习模型可以通过变换其中的超参数和模型结构获取和训练更好的结果，这种探索式的过程也叫做自动化机器学习，读者可以参考第9章-自动化机器学习系统了解相关领域内容与挑战。
- 共享的资源与多租的环境
  - 如果我们现在的GPU等训练资源都是被公司或组织机构集中管理，用户需要共享使用资源进而提升资源整体利用率，那么在这种环境下系统如何提供给算法工程师接近单机的使用环境体验让算法工程师更加简便高效的使用资源？读者可以参考第7章-异构计算集群调度与资源管理系统进行了解平台如何应对当前的挑战。
- 假设数据无法离线提前准备好？
  - 如果数据没有提前准备好，需要系统提供更加多样的训练方式，深度学习系统需要不断与环境或者模拟器交互，通过强化学习方式进行训练，读者可以参考第10章-强化学习系统进行了解，强化学习系统如何在更复杂与多样的场景下进行模型训练以及数据获取。
- 数据和人工智能模型的安全与隐私如何保障？
  - 当前深度学习为数据驱动的方法，同时会产生交付的模型文件，模型泄露，篡改以及本身的缺陷会造成潜在的安全风险。如何保障深度学习整体的安全与隐私相比传统安全领域遇到了新的挑战，读者可以参考第12章-人工智能安全与隐私进行了解。
- 之前我们大部分了解的是针对人工智能负载做系统设计也称作System for AI，反过来我们也可以思考如何通过人工智能这种数据驱动的方法反过来指导系统设计与优化，也就是AI for System，读者可以参考第13章-人工智能优化计算机系统进行了解。

## 小节与讨论

本章我们主要通过一些实例启发读者建立本书各个章节之间的联系，由于系统的多层抽象造成我们实践人工智能的过程中已经无法感知底层系统的运行机制，希望读者结合后面章节的学习后，能够看到深度学习系统底层的作用和复杂性，并从中指导上层人工智能作业和代码更加高效的书写。

请读者读完后面章节后再回看当前章节，并重新思考当前书写的人工智能Python代码底层发生了什么？

## 参考文献
- 