<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->


# 8.3 推理系统的高吞吐与高效率优化

- [8.3 推理系统的高吞吐与高效率优化](#83-推理系统的高吞吐与高效率优化)
  - [8.3.1 加速器并行](#831-加速器并行)
  - [8.3.2 动态批尺寸](#832-动态批尺寸)
  - [8.3.3 多模型装箱](#833-多模型装箱)
  - [参考文献](#参考文献)

- 需要高吞吐的目的
  - 突发的请求数量暴增
  - 不断扩展的用户和设备
- 达到高吞吐的策略：
  - 利用加速器并行
    - 批处理请求
    - 利用优化的BLAS矩阵运算库, SIMD指令和GPU等加速器
- 自适应批尺寸(Batch Size)
- 多模型装箱使用加速器
- 容器扩展副本(Replica)部署


## 8.3.1 加速器并行

加速器的低效率使用

- 由于等待批处理请求，可能造成GPU空闲

单加速器运行多模型

- 时分复用策略，将等待时间给其他模型进行执行
- 同时可以动态调整Batch Size减少空闲时间


## 8.3.2 动态批尺寸

提升批处理(Batch Size)可以提升吞吐量

- 对于高请求数量和频率的场景
  - 通过大的批处理(Batch Size)可以提升吞吐
  - 但是需要满足一定的延迟约束

定义优化问题

$$max_{batch\_size}\{Throughput(batch\_size)\}$$

约束

$$latency(batch\_size) + overhead(batch\_size) \leq latency\_SLA $$

动态批处理尺寸(Batch Size)

- Additive Increase Multiplicative Decrease (AIMD) 策略
- 加性增加(Addictive Increase): 
  - 将批次大小累加增加固定数量，直到处理批次的延迟超过目标延迟为止
- 乘性减少(Multiplicative Decrease): 
  - 当达到后，执行一个小的乘法回退，将批次大小减少了10％
  - 因为最佳批次大小不会大幅波动，所以使用的退避常数要比其他AIMD方案小得多


## 8.3.3 多模型装箱

加速器的低效率使用

在延迟SLA约束下，模型在指定的GPU下按最大吞吐量进行分配，但是可能仍有空闲资源

装箱

Best-fit策略装箱(bin-packing)碎片化的模型请求


当前吞吐量和效率的优化策略是否会对延迟产生影响？
设计其他策略进行吞吐量或使用效率的优化？

## 参考文献

- Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications
- Clipper: A Low-Latency Online Prediction Serving System
- TFX: A TensorFlow-Based Production-Scale Machine Learning Platform
- TensorFlow-Serving: Flexible, High-Performance ML Serving
- Optimal Aggregation Policy for Reducing Tail Latency of Web Search
- A Survey of Model Compression and Acceleration for Deep Neural Networks
- CSE 599W: System for ML - Model Serving
https://developer.nvidia.com/deep-learning-performance-training-inference 
- DEEP COMPRESSION:   COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING
Learning both Weights and Connections for Efficient Neural Networks
- DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT
- Halide: A Language and Compiler for Optimizing Parallelism,Locality, and Recomputation in Image Processing Pipelines
- TVM: An Automated End-to-End Optimizing Compiler for Deep Learning
- 8-bit Inference with TensorRT


