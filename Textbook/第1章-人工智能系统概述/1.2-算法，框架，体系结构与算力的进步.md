<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 1.2 算法，框架，体系结构与算力的进步

催生这轮人工智能热潮的原因有三个重要因素：大数据的积累、超大规模的计算能力支撑、机器学习尤其是深度学习算法都取得了突破性进展。我们将围绕以上重要的三方面因素展开，启发和联系后面章节中的系统设计动机。

本小节将围绕以下内容进行介绍：
- [1.2 算法，框架，体系结构与算力的进步](#12-算法框架体系结构与算力的进步)
  - [1.2.1 大数据和分布式系统](#121-大数据和分布式系统)
  - [1.2.2 深度学习算法的进步](#122-深度学习算法的进步)
    - [1. MNIST数据集上深度学习超越机器学习算法](#1-mnist数据集上深度学习超越机器学习算法)
    - [2. IMAGENET数据集上深度学习取得不断的突破](#2-imagenet数据集上深度学习取得不断的突破)
    - [3. 其他领域算法的进步](#3-其他领域算法的进步)
  - [1.2.3 计算机体系结构和计算能力的进步](#123-计算机体系结构和计算能力的进步)
  - [1.2.4 计算框架的进步](#124-计算框架的进步)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

## 1.2.1 大数据和分布式系统

随着数字化发展，信息系统和平台不断沉淀了大量数据。由于人工智能的算法是数据驱动的方式解决问题，从数据中不断学习出规律和模型，进而完成预测任务。

互联网公司由于有海量的用户，大规模的数据中心，信息系统完善，所以可以较早沉淀出大规模数据，并应用人工智能技术。互联网服务和大数据平台给深度学习带来了大量的数据集。

例如，以下几种服务中沉淀和形成了相应领域代表性的数据集：
- 搜索引擎(Search Engine)
  - 图像检索(Image Search): ImageNet, Coco等计算机视觉数据集
  - 文本检索(Text Search): Wikipedia等自然语言处理数据集
- 商业网站
  - Amazon, Taobao: 推荐系统数据集, 广告数据集
- 其他互联网服务(Internet Services)
  - 对话机器人服务XiaoIce, Siri, Cortana：问答数据集

互联网公司有海量的用户，这些用户不断使用互联网服务，并沉淀相应的用户数据。这些数据随着时间的流逝和新业务功能的推出，数据量越来越大，数据模式越来越丰富。所以互联网公司较早的开发和部署了的大数据管理与处理平台。基于这些海量数据，互联网公司通过数据驱动的方式，训练人工智能模型 ，进而优化和提升业务用户体验（例如，点击率预测让用户获取感兴趣的信息），让更多的用户使用服务，进而形成循环。相较于学术界，互联网公司作为工业界的代表，较早地将深度学习的发展推到了更加实用，落地的阶段，并不断驱动人工智能算法与系统的不断演进和发展。

以图中为例，同样是图像分类问题，从最开始的MNIST数据规模较小，随着数据的沉淀在Web中沉淀了数以亿计的图像数据：

<style>table{margin: auto;}</style>
|MNIST|ImageNet|Web Images|
|---|---|---|
|6万样本|1600万样本|10亿量级图像样本|
|10分类|1000分类|开放分类|

<center>图1-2-1. 不同图像分类问题数据集的数据量</center>

这些海量的数据集为深度学习系统的发展产生了以下的影响：
- 推动深度学习算法不断在指定任务上产生更高的准确度与更低的误差。让深度学习有更广泛的应用，进而产生商业价值，让工业界和学术界投入更多资源进行研究。这样产生了针对深度学习的系统与硬件发展的用户基础，应用驱动力和资源投入。
- 海量的数据集让单机越来越难以完成深度学习模型的训练，进而产生了分布式训练和平台的需求，让传统的机器学习库不能满足相应的需求。
- 多样的数据格式和任务，产生了模型结构的复杂性，驱动框架或针对深度学习的程序语言需要有更灵活的表达能力。 
- 同时伴随着性能，稳定性等需求得到满足，数据安全与模型安全问题挑战也变的日益突出。


综上所述，深度学习系统本身的设计相较于传统机器学习库有更多样的表达需求，更大规模的数据集和更广泛的用户基础，进而驱动深度学习系统设计和社区不断变的越来越多样。

## 1.2.2 深度学习算法的进步

除了数据本身不断的沉淀，深度学习算法和模型的预测效果不断取得突破性进展，同时算法研究员和工程师不断设计新的算法和模型提升预测效果，但是新的算法和模型结构造成原有系统的灵活性不足以支持，进而产生了系统前端设计和执行过程优化新的挑战。

### 1. MNIST数据集上深度学习超越机器学习算法

[MNIST](http://yann.lecun.com/exdb/mnist/)手写数字识别数据库是一个大型手写数字图像数据集，在早期通常用于训练和研究各种图像分类的深度学习模型，当前也常常用于教学或神经网络结构(NAS)搜索的研究。该数据集当前还在广泛用于机器学习领域的训练和测试。 

我们可以观察下图，观察不同的机器学习算法取得的效果。
<style>table{margin: auto;}</style>
<center><img src="./img/2/2-3-5-mnist.png" ch="500" /></center>
<center>图1-2-1. MNIST数据集上各算法的Test Error (%)</center>



从图中可以观察到这样的趋势：

1998年，一个简单的卷积神经网络可以取得和SVM取得的最好效果相同。

2012年，一个深度卷积神经网络可以将错误率降低到0.23% (2012), 这样的结果已经可以和人所达到的错误率0.2%非常接近。

从图中可以观察到，在MNIST数据集上的表现，让研究者们看到了深度学习模型提升预测效果的潜力，进而不断尝试新的深度学习模型和更复杂的问题挑战。


### 2. IMAGENET数据集上深度学习取得不断的突破


随着每年ImageNet数据集上的新模型取得突破，我们看到新的深度学习模型结构和训练方式被研究者和工程师设计出来。
除了深度学习算法本身的潜力，通过在IMAGENET数据集上不断验证不同的深度学习模型结构，我们观察到，更加复杂的模型结构有潜力提升当前预测的效果。
<style>table{margin: auto;}</style>
<center><img src="./img/2/2-3-4-dl-imagenet-improve.png" ch="500" /></center>
<center>图1-2-1. 不同的深度学习模型在IMAGENET数据集上的效果不断取得突破</center>

我们可以观察到，新的模型不断在以下方面演化进而提升效果：
- 更好的激活函数和层: ReLU, BatchNormalization等。
- 更复杂的网络结构和更多的模型权重。 
- 更好的训练方法: Regularization, Initialization, Learning methods等。

这些新的变化，取得更好的效果驱动者算法工程师与研究员不断投入设计新的模型，同时也要求深度学习系统不断提供新的操作符支持，训练算法支持，进而驱动框架和编译器对前段，中间表达，和系统算法协同设计的演进和发展。

### 3. 其他领域算法的进步

除了我们看到的计算机视觉，深度学习在多个领域也取得了不俗的表现，并在当年取得超越原有方案的里程碑式的效果，后续方案不断在深度学习的方法上取得新的突破。

例如： 
- 计算机视觉领域
  - 例如：2015年, ImageNet数据集上MSRA研发的Resnet取得了5项第一，并又一次刷新了CNN模型在ImageNet上的历史。
  
- 自然语言处理领域
  - 例如：2019年,在斯坦福大学举办的SQuAD （Stanford Question Answering Dataset）和 CoQA（Conversational Question Answering）挑战赛中，MSRA的NLP团队通过多阶段(Multistage)，多任务(Multitask)学习的方式取得第一。

- 语音识别领域
  - 例如：2016年，MSR提出的Combined模型系统的在NIST 2000数据集上错误率为6.2%，超越之前报告的基准测试结果。

- 增强学习领域
  - 2016年, Google DeepMind研发的AlphaGo在围棋比赛中以4:1的高分击败了世界大师级冠军李世石。OpenAI 训练出了名为 OpenAI Five 的 Dota 2 游戏智能体。2019 年 4 月，OpenAI Five 击败了一支 Dota 2 世界冠军战队，这是首个击败电子竞技游戏世界冠军的人工智能系统。

由于不同领域的输入数据格式不同，预测输出结果不同，数据获取方式不同，造成模型结构和训练方式产生非常多样的需求，各家公司和组织不断研发新的框架以支持数据科学家快速验证和实现新的想法，工程化部署和批量训练成熟的模型。所以我们可以看到，由最开始AlexNet是作者直接通过[CUDA](https://code.google.com/archive/p/cuda-convnet)实现深度学习模型，到目前有通过Python语言灵活和轻松调用的框架，背后是系统工程师贴合实际需求不断研发新的工具，并推动深度学习生产力提升的结果。

## 1.2.3 计算机体系结构和计算能力的进步

<center><img src="./img/2/2-3-1-arch-improve.png" ch="500" /></center>
<center>图1-2-1. 计算机体系结构和计算能力的进步</center>

从1960年以来，计算机性能的增长主要来自摩尔定律，到二十世纪初大概增长了10的8次方倍。
但是由于摩尔定律的停滞，性能的增长逐渐放缓了。单纯靠工艺尺寸的进步，无法满足各种应用对性能的要求。

于是，人们就开始为应用定制硬件，通过消除通用处理器中冗余的功能部分，来进一步提高对特定应用的计算性能。
比如，图形图像处理器，GPU就对图像类算法做专用加速。后来出现GPGPU，也就是通用GPU，对适合于抽象为单指令流多数据流（SIMD）的高并行算法与工作负载都能起到不错的加速效果。

为了更高的性能，这些年人工智能芯片也大行其道。其中一个代表就是TPU。通过对矩阵乘法定制硬件，进一步提高了性能。通过定制化硬件，我们又将处理器性能提升了大约10的5次方量级。

然而可惜的是，经过这么多年的发展，虽然处理器性能提升这么多，我们机器的数值运算能力早已是人类望尘莫及了，里面的程序仍然是人类指定的固定代码，智能程度还远远不及生物大脑。从智力程度来说，大约也就只相当于啮齿动物，距离人类还有一定距离。

我们可以看到随着硬件的发展，虽然算力逐渐逼近人脑，让深度学习取得了突破。同时我么也看到，计算力也可能在短期内成为瓶颈，那么人工智能系统的性能下一代的出路在哪？

所以我们在后面会看到，除了单独芯片的不断迭代进行性能放大(Scale up)，系统工程师不断设计更好的分布式计算系统将计算并行开来达到向外扩展(Scale out)，同时发掘深度学习的作业特点，如稀疏性等通过算法，系统硬件协同设计，进一步提升计算效率和性能。


## 1.2.4 计算框架的进步

算法工程师和研究员为了实现深度学习模型，完成训练，并部署执行，抛开其他需求，这其中都离不开深度学习框架的支持，例如：PyTorch，TensorFlow等。
计算框架对用户提供编程接口，隐藏硬件细节，同时将用户书写的深度学习程序进行编译优化并部署在专用硬件上进行执行。其中计算框架属于深度学习系统中的核心，构建了算法工程师和底层硬件之间的桥梁。通过业界的开源社区发展和学术研究进展，我们观察到，深度学习框架大致经过以下的发展脉络。

**第一代框架**：

以[Caffe](https://caffe.berkeleyvision.org/)为代表的第一代框架，其编程范式为通过配置文件进行模型构建，框架将模型翻译成粗粒度的算子（例如，卷积层，池化层），并调用底层硬件提供的优化算子库（如NVIDIA cuDNN，CUDA）等进行高效执行。其特点是简单构建方便，但是灵活性不足，用户容易写出错误的程序，同时底层优化依赖硬件厂商库，框架对运行时调度没有过多优化机会。

**第二代框架**：

以[TenorFlow](https://github.com/tensorflow/tensorflow)和[PyTorch](https://github.com/pytorch/pytorch)为代表的第二代框架，目前是有最为广泛用户基础的计算框架。其中通常业界将框架按照编程范式分类两类：

- 声明式编程(Declarative programming)
  - 代表性框架：TensorFlow, Keras, CNTK, Caffe2
  - 特点：用户只需要表达模型结构和需要执行的任务，无需关注底层的执行流程，框架提供计算图优化，让用户无需关心底层优化细节，但是对用户来说不容易调试。

- 命令式编程(Imperative programming）
  - 代表性框架：PyTorch, Chainer, DyNet
  - 特点：用户不仅表达模型结构，还需要表达执行步骤，并且按照每一步定义进行执行，由于无法像声明式编程获取完整计算图并优化后执行，所以难以提供全面的计算图优化，但是由于其简单易用，灵活性高，在模型研究人员中也有很高的用户基础，并不断在新的研究工作中被广泛使用，从而打下增长势头较好且广泛的用户基础。

<style>table{margin: auto;}</style>
<center><img src="./img/2/2-3-2-framework1to2.png" ch="500"></center>
<center>图1-2-2. 第一代框架到第二代框架的进步</center>

但是虽然框架解决了大部分的问题，同时我们也看到第二代框架以Python语言作为前端语言，并结合使用Numpy, Scipy等数据处理库构建深度学习的程序，但是我们也可以看到，数据预处理等其他阶段的执行与深度学习模型执行阶段的库与协同优化的割裂，除深度学习框架之外的库无法利用GPU等专有硬件等现实问题也产生了很多挑战。
同时Python语言其特点是简单，但是不利于优化与错误检测。
由于性能，并发，部署和跨语言调用等问题在不断演化的深度学习研究与工程化对性能和稳定性越来越极致要求的趋势下捉襟见肘。目前也有趋势是提供静态语言前端（例如，Swift等），后端提供编译器（例如，TVM,TensorFlow XLA等）进行优化尝试规避和解决当前框架已有的问题。

**第三代框架**:

我们除了设计框架解决当前的问题，还应该思考关注和设计下一代的框架支持未来的模型趋势。
<style>table{margin: auto;}</style>
<center><img src="./img/2/2-3-3-framework-2to3.png" ch="500"></center>
<center>图1-2-3. 第二代框架到第三代框架的发展趋势</center>

- 框架应在有更加全面功能的编程语言前端下构建，并提供灵活性和表达力，例如：控制流的支持，递归和稀疏性的原生表达与支持。这样才能应对大的（Large）、动态（Dynamic）的和自我修改（Self-Modifying）的深度学习模型趋势。我们无法准确预估深度学习模型在多年后会是什么样子，但从现在的趋势看，它们将会更大、更稀疏、结构更松散。下一代框架应该更好地支持像[Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)模型这样的动态模型，像预训练模型或专家混合模型这样的大型模型，以及需要与真实或模拟环境频繁交互的强化学习模型等多样的需求。

- 框架同时应该不断跟进并提供针对多样且新的硬件特性下的编译优化与运行时调度的优化支持。例如：SIMD (单指令流多数据流)到MIMD（多指令流多数据流）的支持，稀疏性的硬件内优化，分布式计算等。这样才能利用张量计算单元、稀疏性和混合精度支持，SoW（System on Wafer）和大规模训练集群等专有硬件趋势。
  
## 小结与讨论

本章我们主要围绕深度学习系统的算法，框架与体系结构展开，对系统研究，除了理解上层深度学习算法，也需要理解底层的体系结构，并利用两者之前的巨大的优化空间进行设计和权衡。

请读者思考未来的深度学习框架和系统应该是怎样的？

## 参考文献

- Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional Architecture for Fast Feature Embedding. In <i>Proceedings of the 22nd ACM international conference on Multimedia</i> (<i>MM '14</i>). Association for Computing Machinery, New York, NY, USA, 675–678. DOI:https://doi.org/10.1145/2647868.2654889
- Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,
Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis,
Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,
Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia,
Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster,
Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens,
Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,
Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas,
Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng.
TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.
- Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. ArXiv, abs/1912.01703.
- Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. 2017. In-Datacenter Performance Analysis of a Tensor Processing Unit. In <i>Proceedings of the 44th Annual International Symposium on Computer Architecture</i> (<i>ISCA '17</i>). Association for Computing Machinery, New York, NY, USA, 1–12. DOI:https://doi.org/10.1145/3079856.3080246
- https://github.com/BradLarson/swift/blob/main/docs/WhySwiftForTensorFlow.md
- Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang, C., & Zhang, Z. (2015). MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems. ArXiv, abs/1512.01274.
- Neubig, G., Dyer, C., Goldberg, Y., Matthews, A., Ammar, W., Anastasopoulos, A., Ballesteros, M., Chiang, D., Clothiaux, D., Cohn, T., Duh, K., Faruqui, M., Gan, C., Garrette, D., Ji, Y., Kong, L., Kuncoro, A., Kumar, M., Malaviya, C., Michel, P., Oda, Y., Richardson, M., Saphra, N., Swayamdipta, S., & Yin, P. (2017). DyNet: The Dynamic Neural Network Toolkit. ArXiv, abs/1701.03980.
- Tokui, S., Okuta, R., Akiba, T., Niitani, Y., Ogawa, T., Saito, S., Suzuki, S., Uenishi, K., Vogel, B.K., & Vincent, H.Y. (2019). Chainer: A Deep Learning Framework for Accelerating the Research Cycle. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
- Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (pp. 248–255).
- Frank Seide and Amit Agarwal. 2016. CNTK: Microsoft's Open-Source Deep-Learning Toolkit. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16). Association for Computing Machinery, New York, NY, USA, 2135. DOI:https://doi.org/10.1145/2939672.2945397
- Deng, L. (2012). The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6), 141–142.
- Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: an automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 579–594.
- https://www.tensorflow.org/xla
