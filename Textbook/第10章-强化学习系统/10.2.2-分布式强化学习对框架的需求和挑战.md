<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

  # 10.2.2 (分布式)强化学习对框架的需求和挑战

  ## 强化学习系统面临的挑战和机器学习系统相比，有什么不同？为什么强化学习不能复用过去的机器学习或者深度学习的平台呢？

  事实上，在Github上关于强化学习的开源库有将近2w+个。但是大量的框架，都是难以复用的代码。
  主要的原因有以下几点如下：

- 强化学习算法的**可复现性**比较差。随机种子，参数，代码库等因素的变化，都会对强化学习的结果有较大的影响。 

  - 例如，在Rainbow [4] 里介绍可以结合DQN使用的六种技巧, 包括：Dueling DQN, DDQN, Prioritized DQN, Noisy DQN等。从图10.2.2.1上我们可以看出，应用不同的技巧会使得DQN的收敛值发生改变。同时采用6种技巧的Rainbow可以得到超越其他曲线的效果。这些技巧反映在代码上可能仅仅有几行代码的差别。

  - 而另一篇文章[1]里，给PPO带来真正的性能上提升以及将策略（policy）约束在信任域）trust region）内的效果，都不是通过PPO论文中提出的对新策略和原策略的比值进行裁切（clip）带来的，而是通过代码层面（code-level）的一些技巧带来的。这侧面印证了，代码库的不同实现也会对强化学习有比较大的影响。

  <div align="center">
  <img src="./img/trick_of_rainbow.png" ch="500" width="60%"/>
  </div>
  <center>图10.2.2.1 结合了不同技巧的DQN的表现 </center>


- 强化学习的**执行策略**多种多样。这个执行策略包括：运行的硬件或者机器（例如：GPU或者CPU， 单机或者集群），模块之间的交互模式（同步或者异步），通信的内容（数据或者梯度）等

  <div align="center">
  <img src="./img/exection_mode.png" ch="500" width="60%"/>
  </div>
  <center>图10.2.2.2 强化学习里不同的执行策略维度 </center>

  - 在传统的机器学习或者深度学习里，数据集通常是固定的。模型的任务就是拟合数据以求解最优解或者极优解，因此模型可以只跑在单一的机器（例如GPU）上。而强化学习因为数据是边采样得到的，因而采样和训练可以在不同的硬件或者机器上。例如，在IMPALA算法架构里， 采样的推理部分跑在128个CPU上，而模型训练部分跑在1个GPU上。
  
  - 同时，根据采样和训练过程是否解耦，可以将强化学习分为同步算法或者异步算法。例如，PPO是一种同步算法，因为它会在模型采样到一定量的数据后开始训练，然后继续采样，往复迭代直到收敛。而ApeX是一种异步算法，它的采样进程将样本存储到重放缓冲区里，而训练进程从里面异步地采样数据进行训练。在ApeX-DQN里，采样过程和训练过程是异步进行交互的。

- 不同的强化学习算法的**结构差异**很大。

   下图来源于RLlib[3]，从下表里可以看出，不同的算法族群，他们的架构差异是很大的，体现在是否使用以下组件上：策略评估（policy evaulation），重放缓冲区（replay buffer），基于梯度优化器（gradient-based optimizer），以及其他异构的难以归类的组件。


  |  算法类   | 需要策略评估  |  有重放缓冲区 | 有基于梯度的优化器  | 其他的模块|
  |  ----  |  ----  |  ----  |  ----  |  ----  |
  | DQN Based | &#10004; | &#10004; | &#10004; | | 
  | Actor-critic Based | &#10004; | &#10004; | &#10004; | | 
  | Policy Gradient | &#10004; | &#10004; | &#10004; | | 
  | Model Based | &#10004; | &#10004; | &#10004; | Model-based Planning| 
  | Multi-agent | &#10004; | &#10004; | &#10004; | | 
  | Evolutionary Methods | &#10004; | | | Derivate Optimization| 
  | AlphaGo | &#10004; | &#10004; | &#10004; | MCTS; Derivate Optimization| 


- 分布式强化学习**算法和架构互相影响**。新的架构通常可以让算法在原来的基础上跑得更快，但因此可能会带来收敛不好的问题；而通常需要提出新的算法，来解决新架构带来的收敛问题。具体可以参考上一个章节的内容。

   
重新总结以上四点强化学习对框架的**挑战**, 即：

   - 强化学习算法复现比较困难；
   - 分布式强化算法的执行策略多种多样；
   - 不同的强化算法结构存在差异；
   - 分布式强化算法和架构互相影响和变化；

   因而，大部分的开源库都只针对特定的算法和架构模式，难以满足通用分布式RL框架的需求。


## 强化学习对于框架的**需求**有哪些呢？

  通过对于挑战的分析，我们可以能总结出以下需求（包括但不仅限于）:

  - 可扩展性的框架。
    - 支持用户友好且通用的强化学习算法接口。
   
      不同的强化学习的算法的结构差异很大，导致当前的开源框架难以用一个统一的接口去实现大部分的算法，因而用户难以在一个框架上自定义算法。这也是导致Github上许多用户自己开发特定框架的原因。

    - 内置的可复现各种RL算法。可以极大的减少用户的开发时间。

    - 支持不同的强化学习的执行策略。例如：异步或者同步。

    - 支持不同的强化学习架构。

- 高效率高并发的数据采集。

    - 在传统的机器学习里，数据集通常都是预先定义好的。和传统机器学习不同，强化学习需要迭代地收集数据和训练数据，并且自主地决定采样什么样地数据。因而，数据量的大小和模型的效果相关，采样数据的效率是强化学习的关键。

    - 由于工业界的环境可能是在单独的服务器或者机器上，因而与环境交互的时间可能会会较长，而导致采样进程的效率低下。[TODO: 加paper的例子] 将采样过程并行化通常是一个可行的策略，但同时也给用户带来了分布式编程的成本。

    - 支持与环境的多种交互方式。例如，是把数据推送（push）给环境，还是主动从环境中拉取（pull）数据。[TODO： 再明确下]

    - 提供易用的分布式编程模式（programming API），减少用户的开发成本。

- 高性能的通信框架。

  - 在强化学习尤其是分布式强化学习里，由于角色较多，不同的角色之间通信的传输的信息量级可能跨度很大。[TODO: 举信息量大的例子] 例如在图13.2.2.3中，ApeX的架构里，采样的进程会将采样到的数据，从内存里传送到GPU的内存上；而训练的进程会将更新的模型参数，从GPU的内存里传送到内存里给采样进程做推理。

    - 支持高性能的通信。
    - 减少上下文切换的代价。
    - 优化数据的传输。 例如，利用压缩技术，增加传输的吞吐量（throughput）或者减少传输数据的大小。

<div align="center">
<img src="./img/context_switch.png" ch="500" width="80%"/>
</div>
<center>图10.2.2.3 ApeX架构里的上下文切换 </center>

  ## 参考文献
- [1] Engstrom, Logan, et al. "Implementation matters in deep policy gradients: A case study on PPO and TRPO." 
- [2] Hessel, Matteo, et al. "Rainbow: Combining improvements in deep reinforcement learning." 
- [3] Liang, Eric, et al. "Ray rllib: A composable and scalable reinforcement learning library."
- [4] Hessel M, Modayil J, Van Hasselt H, et al. Rainbow: Combining improvements in deep reinforcement learning[C]//Thirty-second AAAI conference on artificial intelligence. 2018.



