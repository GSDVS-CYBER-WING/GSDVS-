<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 12.2 人工智能保密性

- [12.2 人工智能保密性](#122-人工智能保密性)
  - [12.2.1 数据保密性](#1221-数据保密性)
  - [12.2.2 模型保密性](#1222-模型保密性)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

人工智能模型的训练和使用都涉及到大量的隐私数据，甚至模型本身往往也被视作商业机密。因此需要我们特别关注整个人工智能模型生命周期中的隐私保护问题。本节我们将从数据保密性和模型保密性两个角度，来阐述相关的概念和进展。

## 12.2.1 数据保密性

所谓数据保密性，是指如何保护模型训练数据、模型推理的输入数据的隐私。我们侧重介绍针对模型训练的数据隐私保护，因为此过程中涉及到的数据量巨大，且相关攻击和防御方法都较多。本小节的最后我们将简单介绍推理过程的隐私保护。

<center> <img src="./img/2/12-2-1-inv.png" width="480" height="230" /></center>
<center>图12-2-1. 模型反转攻击的直观示意</center>

针对模型训练，有一种叫做模型反转（Model Inversion）的经典攻击，这种攻击的目标就是从训练好的模型恢复出训练数据的相关信息，因此其会对训练数据隐私产生严重的影响：

- [Fredrikson等人](https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf)在2014年展示了如何从医疗推断模型中恢复出病人的一些隐私特征。医疗推断模型可以根据基因类型以及临床变量推断出相应的医疗指导。如果攻击者知道病人的基本信息和预测结果（例如药物剂量），且可以无限次调用模型，那么攻击者就能推断出病人的基因标记。
- [Fredrikson等人](https://dl.acm.org/doi/pdf/10.1145/2810103.2813677)在2015年进一步展示了在人脸识别任务中，攻击者可以从人脸识别模型中恢复出人脸。我们假定人脸识别模型的输入时人脸图片，而输出就是相应的标签（例如人名）。那么攻击者只要知道了受害者的标签，并且可以访问人脸识别模型，就可以重构出受害者的人脸。

模型反转攻击的成功率与模型过拟合程度、批尺寸（Batch Size）等因素有关，其在小规模模型以及大批次训练的情况难度较大。另外，模型反转攻击要求攻击者拥有对模型的白盒访问权限，对应的现实场景较少。因此，一种更加高效的攻击，成员推断攻击(Membership Inference Attack)，被工业界和学术界广泛研究。

<center> <img src="./img/2/12-2-2-mem.png" width="480" height="230" /></center>
<center>图12-2-2. 成员推断攻击的直观示意</center>

在成员推断攻击中。假定攻击者拥有对模型的黑盒访问权限，其目的是对于一个给定的样本X，判断X在不在训练数据集中。我们以[Shokri等人](https://arxiv.org/pdf/1610.05820)在2017年提出的成员推断攻击为例。该攻击的核心思想就是训练一个攻击模型，该模型能根据目标模型的推理结果判断一个样本是否属于目标模型的训练数据集。具体来说有三个步骤：

- 第一步，基于目标模型，生成若干个影子模型（Shadow Model），以及对应的训练数据。影子模型数据可以基于模型搜索数据、数据统计信息以及有噪音的真实数据来生成。
- 第二步，用影子模型的数据训练一个攻击模型，用来判断样本是否在训练数据中。
- 第三步，对于要判断的样本，将其在目标模型的推理输出以及该样本作为攻击模型的输入，攻击模型的输出即为判定结果。

针对模型反转以及成员推断攻击，目前有两种主流的防御手段：差分隐私(Differential Privacy)以及联邦学习(Federated Learning)。下面我们将分别介绍其定义及特点。

差分隐私是隐私保护的一种手段，是由微软研究院的Dwork在2006年提出。其目的是通过引入随机性保证数据库查询操作的隐私安全。一个典型的例子就是社会学调查，例如调查人员希望获得调查群体中具备属性A的大致比例。对于每个受访者，其可采取下面的策略来保护自己的隐私：（1）扔一枚硬币 （2）如果正面朝上，则如实回答 （3）如果反面朝上，则随机回答是或否。因为每个人回答具备随机性，所以隐私得到了一定程度的保护。同时可以知道当调查群体足够大的时候，可以得到调查出的比例(p')和真实属性比例(p)的关系，即p'=p/2+1/4，因此真实比例p=2p'-1/2。

目前有多种达成差分隐私机制，例如适用于数值型输出的Laplace机制，其对返回结果添加Laplace分布的噪声；还有适用于非数值输出的指数机制，其以一定的随机概率从输出范围中选择输出；还有所谓的组合机制，可以将多个随机算法组合后达到相应的隐私效果。下面我们以随机梯度下降（Stochastic Gradient Descent)算法为例，论述如何保证模型参数不包含个体隐私信息，该算法由[Abadi等人](https://dl.acm.org/doi/pdf/10.1145/2976749.2978318)在2016年提出：

- 第一步，计算出梯度值，并根据梯度大小决定噪声的大小。
- 第二步，通过对梯度进行削减，使噪声规模不影响精度。
- 第三步，利用差分隐私组合规则计算隐私成本ε
- 第四步，通过调整超参数平衡隐私、准确率和性能

<center> <img src="./img/2/12-2-3-dif.png" width="480" height="280" /></center>
<center>图12-2-3. 带差分隐私的SGD算法（改编自"Deep learning with differential privacy"）</center>

联邦学习是另一种保护训练数据隐私的方法。联邦学习中，往往假定训练数据分布在边缘设备上，因隐私原因不能上传原始数据。通过联邦学习，边缘设备可以只上传参数更新，而不上传训练数据。联邦学习一般分为四个步骤：（1）中心服务器分发全局模型到各个边缘设备；（2）边缘设备基于全局模型和本地数据训练模型； （3）边缘设备上传训练产生的梯度； （4）中心服务器对收到的所有梯度进行加权求和，得到新的全局模型。这四个步骤一直持续下去，直到全局模型收敛或者损失函数足够小。

<center> <img src="./img/2/12-2-4-fed.png" width="500" height="280" /></center>
<center>图12-2-4. 联邦学习的流程</center>

联邦学习也面临很多挑战以及相应的优化。例如梯度数据也可能泄露隐私，可以进一步用差分隐私或者安全聚合（[Secure Aggregation](https://dl.acm.org/doi/pdf/10.1145/3133956.3133982)）的办法来保护；又或者模型上传和下载会产生大量的通讯数据，可以通过剪枝(Pruning)、量化（Quantization）以及稀疏化（Sparsification）等方法来降低通讯代价。

除了训练数据的隐私，模型推理时也有隐私问题。例如当我们使用医疗模型进行辅助诊断时，用户很可能并不希望自己的输入数据被模型拥有者或者运营商知晓。这时候，我们可以使用多方安全计算（Secure Multiparty Computation)、同态加密（Homomorphic Encryption）或者可信执行环境（Trusted Execution Environment,TEE)来保护模型推理时的输入数据隐私。目前前两种方案有较大的性能瓶颈，而可信执行环境的方案又存在引入额外硬件以及信任计算基(Trusted Computing Base)的问题。因此该领域还在快速发展中。


## 12.2.2 模型保密性

上一小节我们主要站在用户的角度，探讨了训练数据以及输入数据的隐私保护问题。事实上，当我们站在开发者以及模型拥有者的角度，模型也是一种高度敏感的数据。考虑到模型在实际使用中存在分发以及调用问题，其可能涉及到知识产权问题，也可能涉及与安全相关的问题。本小节我们将讨论跟模型数据保护的相关问题。

一种最常见的针对模型数据的攻击就是模型窃取(Model Stealing)。模型窃取有两种方式，第一种是直接窃取，即通过直接攻克模型的开发、存储或部署环境，获得原模型的拷贝；第二种是间接窃取，通过不断调用模型开发者提供的预测API，重构出一个训练数据集，通过训练达到与原模型类似的准确率。

如果模型被窃取，如何验证模型的所有权呢？目前可以采用模型水印（Watermarking）技术。数字水印技术已经在多媒体领域获得了长期的研究和应用。所谓模型水印技术，就是在模型的训练数据或者模型数据上加入一些特殊构造的样本（类似于后门攻击）或者噪声，在不影响模型推理准确率的情况下，又能被提取出来证明模型的所有权。水印技术的一个重要标准就是其应该抗各种攻击，例如对模型进行微调(Fine-tune)，剪枝等等也应该保证水印依然能被提取。

水印技术是一种事后溯源以及追责的方式，如果要对模型进行事前保护，那么就需要相应的加密技术。我们在12.2.1里论述的模型推理的数据隐私保护的方法（多方计算，同态加密，可信执行环境）在保护模型数据上也是有用的。这里我们就如何使用可信执行环境保护模型隐私做一个简单的介绍。

可信计算环境（TEE）是处理器中一块受保护区域，可以保证其中存储数据和运行代码的安全性（机密性和完整性）。TEE假定攻击者无法对这块保护区域进行窥探以及恶意篡改，因此如果我们将模型推理放入这块区域中，那么可以同时保护模型数据以及输入数据都不暴露于使用者以及模型拥有者。

目前，TEE中运行DNN的最大挑战来自于TEE的性能以及内存瓶颈。TEE运行时涉及到内存加密以及完整性验证，对资源隔离的要求也很高（因此可用内存较小），这都会带来一定程度的性能下降。而且现在机器学习中广泛使用的异构硬件目前也不支持TEE的模式。因此目前有不少工作从这些方面对TEE中运行DNN进行优化。


## 小结与讨论

本节主要围绕人工智能中的隐私问题，论述了相关的攻击以及防御措施。事实上，隐私是在计算机产业各个环节都存在的一个问题。而人工智能作为一门数据科学，会对隐私保护带来大量的新的挑战。

看完本章内容后，我们可以思考以下几点问题：
模型反转攻击和成员推断攻击的异同是什么?
整个深度学习模型的生命周期中，都有哪些数据需要被保护？

## 参考文献 

- [Fredrikson, Matthew, et al. "Privacy in Pharmacogenetics: An {End-to-End} Case Study of Personalized Warfarin Dosing." 23rd USENIX Security Symposium (USENIX Security 14). 2014.](https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf)
- [Fredrikson, Matt, Somesh Jha, and Thomas Ristenpart. "Model inversion attacks that exploit confidence information and basic countermeasures." Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. 2015.](https://dl.acm.org/doi/pdf/10.1145/2810103.2813677)
- [Shokri, Reza, et al. "Membership inference attacks against machine learning models." 2017 IEEE symposium on security and privacy (SP). IEEE, 2017.](https://arxiv.org/pdf/1610.05820)
- [Abadi, Martin, et al. "Deep learning with differential privacy." Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016.](https://dl.acm.org/doi/pdf/10.1145/2976749.2978318)
- [Bonawitz, Keith, et al. "Practical secure aggregation for privacy-preserving machine learning." proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. 2017.](https://dl.acm.org/doi/pdf/10.1145/3133956.3133982)
- [Gilad-Bachrach, Ran, et al. "Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy." International conference on machine learning. PMLR, 2016.](http://proceedings.mlr.press/v48/gilad-bachrach16.pdf)
- [Tramèr, Florian, et al. "Stealing Machine Learning Models via Prediction {APIs}." 25th USENIX security symposium (USENIX Security 16). 2016.](https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf)
- [Adi, Yossi, et al. "Turning your weakness into a strength: Watermarking deep neural networks by backdooring." 27th USENIX Security Symposium (USENIX Security 18). 2018.](https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-adi.pdf) 
- [Lee, Taegyeong, et al. "Occlumency: Privacy-preserving remote deep-learning inference using SGX." The 25th Annual International Conference on Mobile Computing and Networking. 2019.](https://dl.acm.org/doi/pdf/10.1145/3300061.3345447)