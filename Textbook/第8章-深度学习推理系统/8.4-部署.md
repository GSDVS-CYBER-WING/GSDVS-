<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->


# 8.4 部署

- [8.4 部署](#84-部署)
  - [8.4.1 部署扩展性](#841-部署扩展性)
  - [8.4.2 部署灵活性](#842-部署灵活性)
  - [8.4.3 模型转换与开放协议](#843-模型转换与开放协议)
  - [8.4.4 模型版本管理，线上发布，回滚策略](#844-模型版本管理线上发布回滚策略)
  - [8.4.5 移动端部署](#845-移动端部署)
  - [8.4.6 代表性推理芯片](#846-代表性推理芯片)
  - [参考文献](#参考文献)
  
## 8.4.1 部署扩展性

- 随着请求负载增加而自动和动态的部署更多的实例
- 系统需要有扩展性的原因：
  - 应对用户与请求的增长
  - 提升吞吐量

<center> <img src="./img/4/8-4-2-scalability.png" width="500" height="260" /></center>
<center>图8-4-1. Kubernetes部署训练和推理服务</center>

## 8.4.2 部署灵活性

- 服务系统需要灵活性的原因：
  - 支持加载不同框架的模型
  - 框架不断的更新，大多数是为训练优化，有些框架甚至不支持在线推理
  - 与不同语言接口和不同逻辑的应用结合
- 解决方法：
  - 接口抽象:
    - 提供构建不同应用逻辑的灵活性
    - 提供不同框架的通用抽象
- RPC：
  - 跨语言，跨进程通信
- 深度学习模型开放协议：
  - 跨框架模型转换
- 容器：
  - 运行时环境依赖与资源隔离

## 8.4.3 模型转换与开放协议

- MMdnn
  - 模型通过中间表达(IR)跨框架转换
- ONNX 
  - 模型中间表达标准
  - 模型优化与部署(ONNX Runtime)

<center> <img src="./img/4/8-4-3-onnx.png" width="500" height="250" /></center>
<center>图8-4-2. 模型构建与部署</center>

## 8.4.4 模型版本管理，线上发布，回滚策略

<center> <img src="./img/4/8-4-4-modellife.png" width="500" height="180" /></center>
<center>图8-4-2. 模型构建与部署</center>


- 需要模型版本管理的原因 
  - 每隔一段时间训练出的新版本模型替换线上模型，但是可能存在缺陷
  - 如果新版本模型发现缺陷需要回滚
- 模型生命周期管理
  - 模型生命周期
  - 金丝雀(Canary)策略
  - 回滚(Rollback)策略

金丝雀(Canary)和回滚(Rollback)策略

- 金丝雀策略
  - 当获得一个新训练的模型版本时，当前服务的模型成为第二新版本(second-newest)时，用户可以选择同时保持这两个版本
  - 将所有推理请求流量发送到当前两个版本，比较它们的效果
  - 一旦对最新版本达标，用户就可以切换到仅该版本
  - 方法需要更多的高峰资源，避免将用户暴露于缺陷模型
- 回滚策略
  - 如果在当前的主要服务版本上检测到缺陷，则用户可以请求切换到特定的较旧版本
  - 卸载和装载的顺序应该是可配置的
  - 当问题解决并且获取到新的安全版本模型时，从而结束回滚

## 8.4.5 移动端部署

移动端部署应用

- 智能设备(Smart Devices)
- 智慧城市(Smart City)
- 互联工厂(Connected Factory) 
- 智慧办公室(Smart Office)
- 智能人体监测

云端部署特点与优势

- 对功耗(energy)、热量(thermal)、模型大小(model size)没有严格限制 
- 用于训练和推理的强大硬件
- 集中的数据管理有利于模型训练
- 模型更容易在云端得到保护 
- DNN执行平台统一 

云端部署推理服务的问题：
- 云上提供所有AI服务成本高昂
- 部署严格的数据隐私问题(Data privacy)
- 数据传输成本(Data transfer cost)
- 推理服务依赖于网络的依赖
- 很难定制化(Model customization)模型

所以常常需要需要考虑在端和云混合情况下提供AI服务

移动端部署挑战

- 严格约束功耗、热量、模型大小（<10MB） 
- 硬件算力对推理服务来说不足
- 数据分散且难以训练 
- 模型在边缘更容易受到攻击 
- DNN平台多样，无通用解决方案 

移动端部署全景图

- 应用
- 高效率模型设计
- 移动端代表性框架
- 移动端芯片

<center> <img src="./img/4/8-4-1-mobilestack.png" width="500" height="200" /></center>
<center>图8-4-1. 并行启动执行作业可能产生的问题</center>

## 8.4.6 代表性推理芯片

- Intel CPU
- Google TPU 
- NVIDIA 
  - 数据中心芯片：A100
  - 移动端芯片：Jetson
- Diannao
- FPGA

## 参考文献

- [Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications](https://arxiv.org/abs/1811.09886)
- [Clipper: A Low-Latency Online Prediction Serving System](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [TFX: A TensorFlow-Based Production-Scale Machine Learning Platform](https://research.google/pubs/pub46484/)
- [TensorFlow-Serving: Flexible, High-Performance ML Serving](https://arxiv.org/abs/1712.06139)
- [Optimal Aggregation Policy for Reducing Tail Latency of Web Search](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
- [A Survey of Model Compression and Acceleration for Deep Neural Networks](https://arxiv.org/abs/1710.09282)
- [CSE 599W: System for ML - Model Serving](https://dlsys.cs.washington.edu/)
- https://developer.nvidia.com/deep-learning-performance-training-inference 
- [DEEP COMPRESSION:   COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING](https://arxiv.org/abs/1510.00149) 
- [Learning both Weights and Connections for Efficient Neural Networks](https://arxiv.org/abs/1506.02626)
- [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)
- [Halide: A Language and Compiler for Optimizing Parallelism,Locality, and Recomputation in Image Processing Pipelines](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
- [TVM: An Automated End-to-End Optimizing Compiler for Deep Learning](https://arxiv.org/abs/1802.04799)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)
- [ADVANCED AI EMBEDDED SYSTEMS](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)
- [Hot Chips 33](https://hc33.hotchips.org/)
- [The 19th ACM International Conference on Mobile Systems, Applications, and Services](https://www.sigmobile.org/mobisys/2021/program.html#ml)
- [NVIDIA AI INFERENCE TECHNICAL OVERVIEW](https://www.nvidia.com/en-us/data-center/resources/inference-technical-overview/)
- [Tutorial: Deep Learning Inference Optimizations for CPU](https://hc33.hotchips.org/assets/program/tutorials/HC2021.Intel.Guokai%20Ma.V2.pdf)
- [Survey of Machine Learning Accelerators](https://arxiv.org/pdf/2009.00993.pdf)
- https://www.nvidia.com/en-us/data-center/a100/