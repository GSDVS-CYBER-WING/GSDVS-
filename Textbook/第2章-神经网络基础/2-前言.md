
# 第2章 神经网络基础

在本章中，我们将会简要叙述当前无论在学术界还是工业界都非常热门的深度学习的基础知识 —— 神经网络。


### 参考

- 《智能之门》，胡晓武等著，高等教育出版社

[1] Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul), 2121-2159.

[2] Zeiler, M. D. (2012). ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.

[3] Tieleman, T., & Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 26-31.

[4] Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.


- 周志华老师的西瓜书《机器学习》
- http://scott.fortmann-roe.com/docs/BiasVariance.html



[1] Chawla N V, Bowyer K W, Hall L O, et al. SMOTE: synthetic minority over-sampling technique[J]. Journal of Artificial Intelligence Research, 2002, 16(1):321-357.

[2] Inoue H. Data Augmentation by Pairing Samples for Images Classification[J]. 2018.

[3] Zhang H, Cisse M, Dauphin Y N, et al. mixup: Beyond Empirical Risk Minimization[J]. 2017.


http://charleshm.github.io/2016/03/Regularized-Regression/

https://blog.csdn.net/red_stone1/article/details/80755144

https://www.jianshu.com/p/c9bb6f89cfcc

https://blog.csdn.net/red_stone1/article/details/80755144

https://www.jianshu.com/p/c9bb6f89cfcc

- 《深度学习》- 伊恩·古德费洛

- http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
- http://jmlr.org/papers/v15/srivastava14a.html
- https://blog.csdn.net/program_developer/article/details/80737724



