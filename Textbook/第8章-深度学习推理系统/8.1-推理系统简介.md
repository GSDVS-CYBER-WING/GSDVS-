<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 8.1 推理系统简介

- [8.1 推理系统简介](#81-推理系统简介)
  - [8.1.1 对比推理与训练过程](#811-对比推理与训练过程)
  - [8.1.2 推理系统的优化目标与约束](#812-推理系统的优化目标与约束)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)


## 8.1.1 对比推理与训练过程

深度学习模型已经广泛的部署在各类应用中，充斥在我们生活的周边。例如，对话机器人，新闻推荐系统，物体检测器等。这些应用中，部署了完成训练的模型。

<center> <img src="./img/1/8-1-1-app.png" width="500" height="180" /></center>
<center>图8-1-1. 典型深度学习推理应用</center>

深度学习模型的生命周期包含以下几个阶段：首先，需要从系统或其他数据源收集和准备训练数据。之后进入训练阶段，开始训练模型。完成训练之后，模型部署在推理系统中，接收用户请求，模型通过推理（Inference）处理完请求后，返回给用户响应结果。

<center> <img src="./img/1/8-1-2-lifecycle.png" width="500" height="200" /></center>
<center>图8-1-2. 深度学习模型的生命周期</center>

如果我们将训练阶段和推理阶段模型的执行过程进行抽象，我们可以总结出两者的共性和不同。训练阶段，深度学习模型常常采用梯度下降算法或类似的优化算法进行模型训练，我们可以将其拆解为三个阶段，前向传播过程将输入样本计算为输出标签，反向传播过程求解权重的梯度，梯度更新阶段将模型权重通过一定的步长和梯度进行更新。在推理阶段，则只需要执行前向传播过程，将输入样本计算为输出标签。

<center> <img src="./img/1/8-1-3-traininfer.png" width="500" height="300" /></center>
<center>图8-1-3. 模型训练与推理阶段</center>

训练作业常常在之前章节中所介绍的异构集群管理系统中进行执行，通常执行，几个小时，天，或周，是类似传统的批处理作业。而推理作业需要7X24小时运行，类似传统的在线服务。

那么具体来说，深度学习模型推理相比训练的新特点与挑战主要有以下几点：

- 模型被部署为长期运行的服务
  - 服务有明确对请求的低延迟高吞吐需求
- 推理有更苛刻的资源约束
  - 更小的内存，更低的功耗等
- 推理不需要反向传播梯度下降
  - 可以牺牲一定的数据精度
- 部署的设备型号更加多样
  - 需要定制化的优化

接下来我们通过MMdnn中自带的一个模型在TensorRT的推理过程实例来了解通过工具进行的模型推理的步骤。

```python
# 将TensorFlow或其他框架的模型文件转换为推理系统的模型格式(UFF).
…
uff_model = uff.from_tensorflow(tf_model, OUTPUT_NAMES)
# 引入UFF模型到TensorRT，之后构建一个引擎(engine).
…
engine = trt.utils.uff_to_trt_engine(G_LOGGER, uff_model, parser, 1, 1 << 20)
# 获取测试图片. 
…
img = Image.open(path)
…
# 为引擎(engine)创建上下文(context).
context = engine.create_execution_context()
...
# 将输入数据拷贝到设备并执行模型进行推理
… 
bindings = [int(d_input), int(d_output)]
…
cuda.memcpy_htod_async(d_input, img, stream)
context.enqueue(1, bindings, stream.handle, None)
# 最后，获取推理结果，并将结果拷贝到主存
…
cuda.memcpy_dtoh_async(output, d_output, stream)
…
print("Prediction: ", LABELS[np.argmax(output)])
# 打印预测结果
>>> Prediction:  n01608432 kite 
```

类似其他的推理系统也是有类似的步骤。当然，也有很多场景下是通过CPU进行推理，这样就不需要进行主存与设备内存之前的数据拷贝。

那么当把模型部署之后，我们可以通过下图看到与推理系统交互的组件。从下图我们可以看到，模型首先经过训练后会保存在文件系统中，随着训练处的模型效果不断提升，可能会产生新版本的模型，并存储在文件系统中。之后模型会通过服务系统部署上线，推理系统会加载模型到内存，对模型进行一定的版本管理，对输入数据进行批次管理与动态调整，并提供服务接口（例如，HTTP，gRPC等），供客户端调用。

<center> <img src="./img/1/8-1-4-servingsys.png" width="500" height="200" /></center>
<center>图8-1-4. 推理服务系统</center>


## 8.1.2 推理系统的优化目标与约束


我们以在线推荐系统的服务需求为例，去了解应用场景层面对推理系统产生的需求。例如，某在线新闻APP公司希望部署内容个性化推荐服务并期望该服务能满足以下需求：

- 低延迟：
  - 互联网上推荐文章延迟（<100毫秒）
- 高吞吐：
  - 突发新闻驱动的暴增人群的吞吐量需求
- 扩展性：
  - 扩展到不断增长的庞大的用户群体
- 准确度：
  - 随着新闻和读者兴趣的变化提供准确的预测 


<center> <img src="./img/1/8-1-5-recommendsys.png" width="130" height="200" /></center>
<center>图8-1-5. 推荐系统</center>


除了应用场景的需求，推理系统也需要应对不同模型训练出的框架和多样性的推理硬件所产生的部署环境多样性。推理服务的部署，优化和维护困难且容易出错。

<center> <img src="./img/1/8-1-6-flexibility.png" width="430" height="200" /></center>
<center>图8-1-6. 推理系统部署需要支持多种框架和硬件</center>


- 框架多样：
  - 大多数框架都是为训练设计和优化	
  - 开发人员需要将必要的软件组件
  - 拼凑在一起
  - 跨多个不断发展的框架集成和推理需求
- 硬件多样：
  - 多种部署硬件的支持

综上所述，我们可以总结出设计推理系统的主要的优化目标：

- 低延迟(Latency):
  - 满足服务等级协议的延迟
- 高吞吐量(Throughputs):
  - 暴增负载的吞吐量需求
- 高效率(Efficiency): 
  - 高效率，低功耗使用GPU, CPU
- 灵活性(Flexibility): 
  - 支持多种框架, 提供构建不同应用的灵活性
- 可靠性(Reliability):
  - 需要对不一致的数据，软件、用户配置和底层执行环境故障等造成的中断有弹性(resilient)。
- 扩展性(Scalability):
  - 扩展支持不断增长的用户或设备

除了以上的优化目标，设计推理系统时，相比训练系统，需要满足新的约束：

<center> <img src="./img/1/8-1-7-constraint.png" width="300" height="200" /></center>
<center>图8-1-6. 推理系统部署需要支持多种框架和硬件</center>

- SLA对延迟的约束
- 资源约束
  - 设备端电池约束
  - 设备与服务端内存约束
  - 云端资源的预算约束
  - …
- 准确度(Accuracy)约束
  - 使用近似模型产生的一些误差可以接受

 ## 小结与讨论

本小节主要围绕深度学习推理系统设计需要考虑的多目标和约束展开介绍，希望大家在深入了解推理系统之前，能对推理系统本身有一个更加全面的认识，之后的各种优化策略也是围绕这些需求进行权衡和设计。

最后大家可以进而思考以下问题，巩固之前的内容：
推理系统相比传统服务系统有哪些新的挑战？
云和端的服务系统有何不同的侧重和挑战？

## 参考文献

- [Park, Jongsoo et al. “Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications.” ArXiv abs/1811.09886 (2018): n. pag.](https://arxiv.org/abs/1811.09886)
- [Crankshaw, Daniel et al. “Clipper: A Low-Latency Online Prediction Serving System.” NSDI (2017).](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17). Association for Computing Machinery, New York, NY, USA, 1387–1395. DOI:https://doi.org/10.1145/3097983.3098021](https://research.google/pubs/pub46484/)
- [Olston, Christopher et al. “TensorFlow-Serving: Flexible, High-Performance ML Serving.” ArXiv abs/1712.06139 (2017): n. pag.](https://arxiv.org/abs/1712.06139)
- [Jeong-Min Yun, Yuxiong He, Sameh Elnikety, and Shaolei Ren. 2015. Optimal Aggregation Policy for Reducing Tail Latency of Web Search. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). Association for Computing Machinery, New York, NY, USA, 63–72. DOI:https://doi.org/10.1145/2766462.2767708](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
- [Cheng, Yu et al. “A Survey of Model Compression and Acceleration for Deep Neural Networks.” ArXiv abs/1710.09282 (2017): n. pag.](https://arxiv.org/abs/1710.09282)
- [CSE 599W: System for ML - Model Serving](https://dlsys.cs.washington.edu/)
- https://developer.nvidia.com/deep-learning-performance-training-inference 
- [Han, Song et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition (2016): n. pag.](https://arxiv.org/abs/1510.00149) 
- [Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both weights and connections for efficient neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 (NIPS'15). MIT Press, Cambridge, MA, USA, 1135–1143.](https://arxiv.org/abs/1506.02626)
- [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)
- [Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. SIGPLAN Not. 48, 6 (June 2013), 519–530. DOI:https://doi.org/10.1145/2499370.2462176](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
- [Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: an automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 579–594.](https://arxiv.org/abs/1802.04799)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)