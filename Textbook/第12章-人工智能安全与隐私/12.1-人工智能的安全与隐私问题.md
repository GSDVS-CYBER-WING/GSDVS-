<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 12.1 人工智能的安全与隐私问题

- [12.1 人工智能的安全与隐私问题](#121-人工智能的安全与隐私问题)
  - [12.1.1 深度神经网络内在的安全问题](#1211-深度神经网络内在的安全问题)
  - [12.1.2 深度神经网络内在的隐私问题](#1212-深度神经网络内在的隐私问题)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

随着深度学习的流行，深度神经网络内在的安全与隐私问题逐渐被发现。所谓“内在”，是指这些安全与隐私问题是深度神经网络内生的，而非人工智能系统导致的。adversarial machine learning，

## 12.1.1 深度神经网络内在的安全问题

新的技术往往伴随着新的问题，现代的人工智能模型，特别是深度学习模型，虽然具备越来越好的效果，但是也存在一些安全问题。2014 年，Christian Szegedy 等人发现了在深度神经网络中的一个“有趣”的性质：如果给一张正常的图片输入加入微小的扰动，那么原本能够将正常图片正确分类的深度神经网络会输出完全错误的预测结果！这样的扰动之小，甚至可以让人的肉眼都无法感知，却会让深度神经网络的行为完全错误。如图 12.1.1 所示，Szegedy 等人成功构造出了这样的扰动，使深度神经网络将右图——人看起来仍然是车的图片——错误识别为鸵鸟。

<center><img src="./img/12-1-1-ae.png"/></center>
<center>图 12.1.1 原始图片（左）在增加微小扰动（中）后，深度神经网络会将被加扰动的图片（右）错误识别为“鸵鸟”（<a href="https://arxiv.org/abs/1312.6199">Szegedy et al.</a>）</center>

这种被加上扰动、会导致深度神经网络输出错误结果的输入被命名为对抗样本（adversarial example）。对抗样本的数学定义如下：用 $x$ 表示模型的输入，$f(x)$ 表示模型的输出结果，如分类标签。对于给定的正实数 $\epsilon$ 以及模型输入空间中的一个点 $x_0$，如果存在 $x_1$ 同时满足 $||x_1-x_0||<\epsilon$ 且 $f(x_1)\neq f(x_0)$，那么就称 $x_1$ 为 $x_0$ 的一个对抗样本。其中 $||x_1-x_0||$ 表示  $x_1$ 与 $x_0$ 之间的距离。

所谓对抗样本攻击（或称为对抗样本生成），其目的就是在给定模型、$x_0$ 以及 $\epsilon$ 的情况下找到 $x_1$。这里介绍一种简单而高效地生成对抗样本的方法：快速梯度符号法（fast gradient sign method，FGSM）。这是 Ian Goodfellow 等人在 2015 年提出的。如果将寻找对抗样本视为一个关于损失函数的优化问题，那一种近似方法就是将扰动设置为一个与损失函数 $J_{f,y}(x)$ 在 $x_0$ 处的梯度方向相似的向量，即
$$
x_1 = x_0 + \epsilon \times \textrm{sign}(\nabla J(x_0))
$$
在以 $||\cdot||_\infty$ 衡量距离的意义下，一个大小为恰好为 $\epsilon$ 的扰动就被构造出来了。Goodfellow 等人的实验结果表明这样构造对抗样本有相当高的概率导致深度神经网络误分类。

对抗样本的存在说明深度神经网络的健壮性（robustness）并不好。这种性质如果被攻击者利用，那么有意生成一些微小的扰动就可以让现有的人工智能失效。在重要决策领域，例如无人驾驶、人脸识别、恶意软件检测等，这将带来极大的安全隐患。事实上，已经有研究人员构造出了这些场景中的真实对抗样本攻击。以人脸识别为例，Mahmood Sharif 等人在 2016 年通过制作一个精心设计的眼镜框，可以使得人脸识别算法无法识别出人脸的存在，或者将一个人识别成另一个人。这样的针对检测系统的攻击也被称为“逃逸攻击”（evasion attack）。这是一种经典的安全问题，在深度学习流行之前，安全领域的研究者就已经注意到了这种攻击了。但深度神经网络的复杂性和新颖性对人们设计人工智能检测系统带来新的要求和挑战。

理想情况下，人工智能应该是抗干扰的。如何提升深度神经网络的健壮性呢？这里介绍一种被认为比较有效的方法：对抗训练（adversarial training），即将对抗样本（以其正确的标注）加入到训练过程中。例如，可以利用 FGSM，将训练过程中的损失函数重新定义为
$$
\tilde J(x) = \alpha J(x) + (1-\alpha) J(x + \epsilon \times \textrm{sign}(\nabla J(x)))
$$
其中 $\alpha$ 是一个可以设在 0 到 1 之间的参数。以 $\tilde J(x)$ 作为损失函数，训练出来的模型可以显著地降低 FGSM 对抗样本生成的成功率，并且几乎不损失模型在测试集上的效果。另一种效果更好

给定一个通过对抗训练得到的模型，还可以通过形式化方法来确定该模型的健壮性有多好。即给定模型、输入 $x_0$ 的集合、$\epsilon$ 以及距离函数 $||\cdot||$，通过求解器（solver）来确定是否有（或有多少）对抗样本 $x_1$ 满足 $||x_1-x_0||<\epsilon$ 且 $f(x_1)\neq f(x_0)$。

但是，对抗训练也有其局限性。首先，如何一种对抗训练方法往往只针对一种距离函数意义下的对抗样本，而对其他距离函数意义下的对抗样本难以起到很好的防范效果。例如，上述的基于 FGSM 的对抗训练对 $||\cdot||_\infty$ 之外的距离函数意义下的对抗样本效果并不好。还需要注意的是，在真实世界中的对抗样本往往不是用 p-范数来衡量距离，而是用与人类感知相关的方法来描述“距离很近的样本”，这对于对抗训练提出了更大的挑战。第二，能够训练出健壮性非常好的模型的对抗训练往往需要大量的时间开销，因此如何设计即高效又效果好的对抗训练方法是一个备受关注的问题。第三，Liwei Song 等人在 2019 年还发现经过对抗训练的模型反而具有更大的隐私风险，表明模型的健壮性和隐私性之间存在一种权衡关系。

对抗样本是目前深度学习非常热门的一个研究领域。除了上文介绍的攻击、防御的方法之外，还有各种各样新颖的攻击方法和防御手段，且目前呈现出“攻防竞赛”的状态。关于对抗样本存在的原因也众说纷纭，但随着相关研究的继续深入，人们将从安全的角度对增加对深度神经网络的理解。

## 12.1.2 深度神经网络内在的隐私问题

人工智能模型的训练和使用都涉及到大量的隐私数据，甚至模型本身往往也被视作商业机密。因此需要我们特别关注整个人工智能模型生命周期中的隐私保护问题。本节我们侧重介绍针对模型训练的数据隐私保护，因为此过程中涉及到的数据量巨大，且相关攻击方法较多。

<center> <img src="./img/2/12-2-1-inv.png" width="1000" height="500" /></center>
<center>图12-2-1. 模型反转攻击的直观示意</center>

针对模型训练，有一种叫做模型反转（Model Inversion）的经典攻击，这种攻击的目标就是从训练好的模型恢复出训练数据的相关信息，因此其会对训练数据隐私产生严重的影响：

- [Fredrikson等人](https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf)在2014年展示了如何从医疗推断模型中恢复出病人的一些隐私特征。医疗推断模型可以根据基因类型以及临床变量推断出相应的医疗指导。如果攻击者知道病人的基本信息和预测结果（例如药物剂量），且可以无限次调用模型，那么攻击者就能推断出病人的基因标记。
- [Fredrikson等人](https://dl.acm.org/doi/pdf/10.1145/2810103.2813677)在2015年进一步展示了在人脸识别任务中，攻击者可以从人脸识别模型中恢复出人脸。我们假定人脸识别模型的输入时人脸图片，而输出就是相应的标签（例如人名）。那么攻击者只要知道了受害者的标签，并且可以访问人脸识别模型，就可以重构出受害者的人脸。

模型反转攻击的成功率与模型过拟合程度、批尺寸（Batch Size）等因素有关，其在小规模模型以及大批次训练的情况难度较大。另外，模型反转攻击要求攻击者拥有对模型的白盒访问权限，对应的现实场景较少。因此，一种更加高效的攻击，成员推断攻击（Membership Inference Attack），被工业界和学术界广泛研究。

<center> <img src="./img/2/12-2-2-mem.png" width="1000" height="500" /></center>
<center>图12-2-2. 成员推断攻击的直观示意</center>

在成员推断攻击中。假定攻击者拥有对模型的黑盒访问权限，其目的是对于一个给定的样本X，判断X在不在训练数据集中。我们以[Shokri等人](https://arxiv.org/pdf/1610.05820)在2017年提出的成员推断攻击为例。该攻击的核心思想就是训练一个攻击模型，该模型能根据目标模型的推理结果判断一个样本是否属于目标模型的训练数据集。具体来说有三个步骤：

- 第一步，基于目标模型，生成若干个影子模型（Shadow Model），以及对应的训练数据。影子模型数据可以基于模型搜索数据、数据统计信息以及有噪音的真实数据来生成。
- 第二步，用影子模型的数据训练一个攻击模型，用来判断样本是否在训练数据中。
- 第三步，对于要判断的样本，将其在目标模型的推理输出以及该样本作为攻击模型的输入，攻击模型的输出即为判定结果。

针对模型反转以及成员推断攻击，目前有主流的防御手段是差分隐私（Differential Privacy）。差分隐私是隐私保护的一种手段，是由微软研究院的Dwork在2006年提出。其目的是通过引入随机性保证数据库查询操作的隐私安全。一个典型的例子就是社会学调查，例如调查人员希望获得调查群体中具备属性A的大致比例。对于每个受访者，其可采取下面的策略来保护自己的隐私：（1）扔一枚硬币 （2）如果正面朝上，则如实回答 （3）如果反面朝上，则随机回答是或否。因为每个人回答具备随机性，所以隐私得到了一定程度的保护。同时可以知道当调查群体足够大的时候，可以得到调查出的比例（$p'$）和真实属性比例（$p$）的关系，即$p'=\frac{p}{2} + \frac{1}{4}$，因此真实比例$p=2p'-\frac{1}{2}$。

目前有多种达成差分隐私机制，例如适用于数值型输出的Laplace机制，其对返回结果添加Laplace分布的噪声；还有适用于非数值输出的指数机制，其以一定的随机概率从输出范围中选择输出；还有所谓的组合机制，可以将多个随机算法组合后达到相应的隐私效果。下面我们以随机梯度下降（Stochastic Gradient Descent)算法为例，论述如何保证模型参数不包含个体隐私信息，该算法由[Abadi等人](https://dl.acm.org/doi/pdf/10.1145/2976749.2978318)在2016年提出：

- 第一步，计算出梯度值，并根据梯度大小决定噪声的大小。
- 第二步，通过对梯度进行削减，使噪声规模不影响精度。
- 第三步，利用差分隐私组合规则计算隐私成本ε
- 第四步，通过调整超参数平衡隐私、准确率和性能

<center> <img src="./img/2/12-2-3-dif.png" width="1000" height="500" /></center>
<center>图12-2-3. 带差分隐私的SGD算法（改编自"Deep learning with differential privacy"）</center>


## 小结与讨论

本小节主要围绕深度神经网络内在的安全与隐私问题，讨论了与对抗样本、模型反转、成员推断相关的攻击和防御。

最后大家可以进而思考以下问题，巩固之前的内容：
对抗样本的数学定义是什么，为什么对抗样本现象很难消除？
模型反转攻击和成员推断攻击的异同是什么？

## 参考文献

- [Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. "Explaining and harnessing adversarial examples." arXiv preprint arXiv:1412.6572 (2014).](https://arxiv.org/abs/1412.6572)
- [Moosavi-Dezfooli, Seyed-Mohsen, et al. "Universal adversarial perturbations." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.](https://openaccess.thecvf.com/content_cvpr_2017/papers/Moosavi-Dezfooli_Universal_Adversarial_Perturbations_CVPR_2017_paper.pdf)
- [Baluja, Shumeet, and Ian Fischer. "Adversarial transformation networks: Learning to generate adversarial examples." arXiv preprint arXiv:1703.09387 (2017).](https://arxiv.org/pdf/1703.09387.pdf)
- [Papernot, Nicolas, et al. "Distillation as a defense to adversarial perturbations against deep neural networks." 2016 IEEE symposium on security and privacy (SP). IEEE, 2016.](https://arxiv.org/pdf/1511.04508.pdf)
- [Wang, Shiqi, et al. "Formal security analysis of neural networks using symbolic intervals." 27th USENIX Security Symposium (USENIX Security 18). 2018.](https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-wang_0.pdf)
- [Fredrikson, Matthew, et al. "Privacy in Pharmacogenetics: An {End-to-End} Case Study of Personalized Warfarin Dosing." 23rd USENIX Security Symposium (USENIX Security 14). 2014.](https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf)
- [Fredrikson, Matt, Somesh Jha, and Thomas Ristenpart. "Model inversion attacks that exploit confidence information and basic countermeasures." Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. 2015.](https://dl.acm.org/doi/pdf/10.1145/2810103.2813677)
- [Shokri, Reza, et al. "Membership inference attacks against machine learning models." 2017 IEEE symposium on security and privacy (SP). IEEE, 2017.](https://arxiv.org/pdf/1610.05820)
- [Abadi, Martin, et al. "Deep learning with differential privacy." Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016.](https://dl.acm.org/doi/pdf/10.1145/2976749.2978318)
