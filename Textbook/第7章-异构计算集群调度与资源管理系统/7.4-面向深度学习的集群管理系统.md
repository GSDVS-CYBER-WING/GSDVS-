<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/YanjieGao/AI-System/blob/main/LICENSE)版权许可-->

# 7.4 面向深度学习的集群管理系统

- [7.4 面向深度学习的集群管理系统](#74-面向深度学习的集群管理系统)
  - [7.4.1 深度学习工作负载的需求](#741-深度学习工作负载的需求)
  - [7.4.2 异构硬件的多样性](#742-异构硬件的多样性)
  - [7.4.3 异构硬件下的调度算法设计](#743-异构硬件下的调度算法设计)
  - [7.4.4 代表性异构集群管理系统](#744-代表性异构集群管理系统)
  - [参考文献](#参考文献)

## 7.4.1 深度学习工作负载的需求

- 深度学习作业特点：
  - 切分为小时间窗口的任务
  - 不同的时间点做检查点有不同的数据量
  - 资源消耗可预测性，可以通过运行时监控获取

启发优化策略：
- 时分复用(Time slicing)与超额订阅 (Oversubscription)
- 装箱 (Packing)
- 迁移 (Migration)
- …


## 7.4.2 异构硬件的多样性

- 多GPU集群运行深度学习问题与挑战：
  - 受到GPU拓扑结构影响
  - 受到服务器上同时运行作业的干扰

启发优化策略：考虑拓扑结构的亲和性(Affinity)调度。


## 7.4.3 异构硬件下的调度算法设计

我们接下来以[Gandiva]()的调度策略为例介绍针对深度学习负载的调度器会考虑哪些影响因素进而设计
相应的策略。

Reactive Mode
- 亲和性(affinity)
- 对事件作出响应 (Reactive)的调度
  - 作业到达(arrivals), 离开(departures), 失效(failures)
- 考虑亲和性的调度策略
  - Nodes with same “affinity”
  - Nodes with “no affinity”
  - Nodes with “different affinity”
  - Oversubscription: suspend-resume on same “affinity” nodes
  - Job queued

Introspective Mode
- 监控并定期优化当前作业的放置 (placement)
- 早反馈(Early feedback): 
  - 装箱(Packing)
  - 迁移(Migration)
  - 增长收缩(Grow-Shrink) 
  - 时间切片(Time slicing)

其他经典针对深度学习的调度器[HiveD](https://dl.acm.org/doi/10.5555/3488766.3488795)等大家可以参考其文献，或使用和测试其已[开源的HiveD调度器](https://github.com/microsoft/hivedscheduler)。

## 7.4.4 代表性异构集群管理系统

本小节我们将介绍代表性的开源和企业内部的大规模异构集群管理系统。基于代表性开源系统，企业可以部署和构建自己的平台，提升资源管理能力，资源利用率和开发效率。基于内部大规模异构集群管理系统所发布的参考文献，企业可以较早的规划和采取最佳实践策略，将未来可预见的问题较早规避，并持续扩容。

- [OpenPAI](https://github.com/microsoft/pai)

OpenPAI是由微软亚洲研究院和微软（亚洲）互联网工程院联合研发的，支持多种深度学习、机器学习及大数据任务，可提供大规模GPU集群调度、集群监控、任务监控、分布式存储等功能，且用户界面友好，易于操作。OpenPAI正在转向更健壮、更强大和更轻量级的架构。OpenPAI 也变得越来越模块化，以便平台可以轻松定制和扩展以满足新的需求。OpenPAI 还提供了许多 AI 用户友好的功能，使最终用户和管理员更容易完成日常的 AI 任务。 

- [Kubeflow](https://www.kubeflow.org/)

Kubeflow是由Google开源的平台项目，该项目致力于使机器学习工作流在 Kubernetes上的部署变得简单、便携和可扩展。Kubeflow的设计目标不是重新创建其他服务，而是提供一种直接的方法，将用于机器学习和深度学习的同类最佳开源系统部署到不同的基础设施。无论用户在何处运行Kubernetes，都可以运行Kubeflow。 

- [Philly](https://dl.acm.org/doi/10.5555/3358807.3358888)
Philly是微软内部使用的大规模AI训练平台，Philly 旨在支持执行有监督式机器学习的训练工作负载。 这包括培训来自开发产品的生产团队的工作，这些产品使用用于图像分类、语音识别等的模型。有相关研究工作对Philly上的[资源分配，深度学习作业性能特点](https://dl.acm.org/doi/10.5555/3358807.3358888)和[深度学习程序Bug](https://dl.acm.org/doi/10.1145/3377811.3380362)进行研究，从中我们可以观察和了解大规模生产环境中作业资源争用，资源利用率，调度策略设计和程序Bug等问题及启发相关新的研究工作。  

## 参考文献
- https://github.com/microsoft/pai
- https://www.kubeflow.org/
- [Myeongjae Jeon, Shivaram Venkataraman, Amar Phanishayee, unjie Qian, Wencong Xiao, and Fan Yang. 2019. Analysis of large-scale multi-tenant GPU clusters for DNN training workloads. In Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference (USENIX ATC '19). USENIX Association, USA, 947–960.](https://dl.acm.org/doi/10.5555/3358807.3358888)
- [Ru Zhang, Wencong Xiao, Hongyu Zhang, Yu Liu, Haoxiang Lin, and Mao Yang. 2020. An empirical study on program failures of deep learning jobs. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (ICSE '20). Association for Computing Machinery, New York, NY, USA, 1159–1170. DOI:https://doi.org/10.1145/3377811.3380362](https://dl.acm.org/doi/10.1145/3377811.3380362)
- [Wencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang, Fan Yang, and Lidong Zhou. 2018. Gandiva: introspective cluster scheduling for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 595–610.](https://dl.acm.org/doi/10.5555/3291168.3291212)
- [Hanyu Zhao, Zhenhua Han, Zhi Yang, Quanlu Zhang, Fan Yang, Lidong Zhou, Mao Yang, Francis C.M. Lau, Yuqi Wang, Yifan Xiong, and Bin Wang. 2020. HiveD: sharing a GPU cluster for deep learning with guarantees. Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation. Article 29, 515–532.](https://dl.acm.org/doi/10.5555/3488766.3488795)
- https://www.msra.cn/zh-cn/news/features/openpai
