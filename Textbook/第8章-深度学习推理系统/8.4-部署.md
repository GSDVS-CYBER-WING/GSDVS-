<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->


# 8.4 部署

- [8.4 部署](#84-部署)
  - [8.4.1 部署扩展性](#841-部署扩展性)
  - [8.4.2 部署灵活性](#842-部署灵活性)
  - [8.4.3 模型转换与开放协议](#843-模型转换与开放协议)
  - [8.4.4 模型版本管理，线上发布，回滚策略](#844-模型版本管理线上发布回滚策略)
  - [8.4.5 移动端部署](#845-移动端部署)
  - [8.4.6 推理芯片](#846-推理芯片)
  - [参考文献](#参考文献)
  
## 8.4.1 部署扩展性

- 随着请求负载增加而自动和动态的部署更多的实例
- 系统需要有扩展性的原因：
  - 应对用户与请求的增长
  - 提升吞吐量

## 8.4.2 部署灵活性

- 服务系统需要灵活性的原因：
  - 支持加载不同框架的模型
  - 框架不断的更新，大多数是为训练优化，有些框架甚至不支持在线推理
  - 与不同语言接口和不同逻辑的应用结合
- 解决方法：
  - 接口抽象:
    - 提供构建不同应用逻辑的灵活性
    - 提供不同框架的通用抽象
- RPC：
  - 跨语言，跨进程通信
- 深度学习模型开放协议：
  - 跨框架模型转换
- 容器：
  - 运行时环境依赖与资源隔离

## 8.4.3 模型转换与开放协议

- MMdnn
  - 模型通过中间表达(IR)跨框架转换
- ONNX 
  - 模型中间表达标准
  - 模型优化与部署(ONNX Runtime)

## 8.4.4 模型版本管理，线上发布，回滚策略

- 需要模型版本管理的原因 
  - 每隔一段时间训练出的新版本模型替换线上模型，但是可能存在缺陷
  - 如果新版本模型发现缺陷需要回滚
- 模型生命周期管理
  - 模型生命周期
  - 金丝雀(Canary)策略
  - 回滚(Rollback)策略

金丝雀(Canary)和回滚(Rollback)策略

- 金丝雀策略
  - 当获得一个新训练的模型版本时，当前服务的模型成为第二新版本(second-newest)时，用户可以选择同时保持这两个版本
  - 将所有推理请求流量发送到当前两个版本，比较它们的效果
  - 一旦对最新版本达标，用户就可以切换到仅该版本
  - 方法需要更多的高峰资源，避免将用户暴露于缺陷模型
- 回滚策略
  - 如果在当前的主要服务版本上检测到缺陷，则用户可以请求切换到特定的较旧版本
  - 卸载和装载的顺序应该是可配置的
  - 当问题解决并且获取到新的安全版本模型时，从而结束回滚

## 8.4.5 移动端部署

## 8.4.6 推理芯片


## 参考文献

- Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications
- Clipper: A Low-Latency Online Prediction Serving System
- TFX: A TensorFlow-Based Production-Scale Machine Learning Platform
- TensorFlow-Serving: Flexible, High-Performance ML Serving
- Optimal Aggregation Policy for Reducing Tail Latency of Web Search
- A Survey of Model Compression and Acceleration for Deep Neural Networks
- CSE 599W: System for ML - Model Serving
https://developer.nvidia.com/deep-learning-performance-training-inference 
- DEEP COMPRESSION:   COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING
Learning both Weights and Connections for Efficient Neural Networks
- DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT
- Halide: A Language and Compiler for Optimizing Parallelism,Locality, and Recomputation in Image Processing Pipelines
- TVM: An Automated End-to-End Optimizing Compiler for Deep Learning
- 8-bit Inference with TensorRT